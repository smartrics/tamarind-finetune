{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies\n",
    "# These are the core libraries: Transformers, Datasets, PEFT (for LoRA), TRL (Trainer), BitsAndBytes (4-bit quant)\n",
    "!pip install -q datasets peft transformers accelerate trl bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load JSONL Dataset\n",
    "# This loads your data into train/val/test splits using Hugging Face's `datasets` library\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"app/data/training_data.jsonl\",\n",
    "    \"validation\": \"app/data/validation_data.jsonl\",\n",
    "    \"test\": \"app/data/test_data.jsonl\"\n",
    "})\n",
    "\n",
    "# âœ… Shuffle data (important for generalization, especially if your data is grouped)\n",
    "data[\"train\"] = data[\"train\"].shuffle(seed=42)\n",
    "data[\"validation\"] = data[\"validation\"].shuffle(seed=42)\n",
    "data[\"test\"] = data[\"test\"].shuffle(seed=42)\n",
    "\n",
    "# Quick peek\n",
    "data[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert messages into Mistral-style prompt/response format\n",
    "# Your data is ChatML-style, so we turn it into <s>[INST] ... [/INST] response </s>\n",
    "\n",
    "def format_chat_prompt(example):\n",
    "    messages = example[\"messages\"]\n",
    "    prompt = \"\"\n",
    "    for i, msg in enumerate(messages):\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"].strip()\n",
    "\n",
    "        if role == \"system\":\n",
    "            system_prompt = content\n",
    "        elif role == \"user\":\n",
    "            if i == 1 and messages[0][\"role\"] == \"system\":\n",
    "                # System + first user message inside one [INST] block\n",
    "                prompt += f\"<s>[INST] {system_prompt}\\n\\n{content} [/INST]\"\n",
    "            else:\n",
    "                prompt += f\"<s>[INST] {content} [/INST]\"\n",
    "        elif role == \"assistant\":\n",
    "            # Append assistant reply and close sequence\n",
    "            prompt += f\" {content} </s>\"\n",
    "\n",
    "    return { \"prompt\": prompt }\n",
    "\n",
    "# Apply formatting to all splits\n",
    "data = data.map(format_chat_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load Mistral 7B Instruct model in 4-bit for memory efficiency\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Mistral does not have a PAD token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Apply LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Enable gradient checkpointing & cast layer norms\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define which layers to apply LoRA to\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Should show a small number of LoRA params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Tokenize the formatted prompt + response text\n",
    "# The entire [INST] ... [/INST] response is tokenized as a single sequence\n",
    "def tokenize(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"prompt\"] + tokenizer.eos_token,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = data.map(tokenize, remove_columns=data[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Train using Hugging Face's SFTTrainer from `trl`\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral-lora-output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=None\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Save LoRA adapter weights and tokenizer (not full model yet)\n",
    "trainer.model.save_pretrained(\"./mistral-lora-adapter\")\n",
    "tokenizer.save_pretrained(\"./mistral-lora-adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Merge adapter into base model to get a full model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./mistral-lora-adapter\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(\"./mistral-merged\")\n",
    "tokenizer.save_pretrained(\"./mistral-merged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Convert to GGUF format using llama.cpp\n",
    "\n",
    "# Clone llama.cpp repo\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "%cd llama.cpp\n",
    "\n",
    "# Build llama.cpp tools (optional but good practice)\n",
    "!cmake -B build\n",
    "!cmake --build build --config Release\n",
    "\n",
    "# Install Python requirements for conversion\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# Return to root\n",
    "%cd ..\n",
    "\n",
    "# Run the conversion script from llama.cpp\n",
    "!python llama.cpp/convert-hf-to-gguf.py mistral-merged --outfile mistral_model.gguf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Run inference using the merged model\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"./mistral-merged\", tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "prompt = \"<s>[INST] What is the purpose of the 'filterRows' action in TWL? [/INST]\"\n",
    "output = pipe(prompt, max_new_tokens=256, do_sample=True)\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Upload both merged HF model and GGUF model to Hugging Face\n",
    "from huggingface_hub import login, HfApi\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Authenticate (you'll be prompted)\n",
    "notebooklogin()\n",
    "\n",
    "# Update with your repo names\n",
    "repo_hf_model = \"your-username/mistral-7b-tamarind-lora\"\n",
    "repo_gguf = \"your-username/mistral-7b-tamarind-gguf\"\n",
    "\n",
    "# Push HF model + tokenizer\n",
    "AutoModelForCausalLM.from_pretrained(\"./mistral-merged\").push_to_hub(repo_hf_model)\n",
    "tokenizer.push_to_hub(repo_hf_model)\n",
    "\n",
    "# Upload GGUF model\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=repo_gguf, repo_type=\"model\", exist_ok=True)\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"../mistral_model.gguf\",\n",
    "    path_in_repo=\"mistral_model.gguf\",\n",
    "    repo_id=repo_gguf,\n",
    "    repo_type=\"model\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
