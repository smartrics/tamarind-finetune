{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'tamarind-finetune' does not exist. Cloning repository...\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Clone the repo and setup environment\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo_dir = \"./tamarind-finetune\"\n",
    "repo_url = \"https://github.com/smartrics/tamarind-finetune.git\"\n",
    "\n",
    "if os.path.isdir(repo_dir):\n",
    "    print(\"Directory 'tamarind-finetune' exists. Pulling latest changes...\")\n",
    "    subprocess.run([\"git\", \"-C\", repo_dir, \"pull\"], check=True)\n",
    "else:\n",
    "    print(\"Directory 'tamarind-finetune' does not exist. Cloning repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", repo_url, repo_dir], check=True)\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./tamarind-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: transformers==4.50.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (4.50.0)\n",
      "Requirement already satisfied: datasets==3.5.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: peft==0.14.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: trl==0.11.4 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (0.11.4)\n",
      "Requirement already satisfied: huggingface_hub>=0.20.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.30.0)\n",
      "Requirement already satisfied: bitsandbytes==0.45.4 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (0.45.4)\n",
      "Requirement already satisfied: torchsummary==1.5.1 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (1.5.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (2024.12.0)\n",
      "Collecting gcsfs (from -r requirements.txt (line 11))\n",
      "  Downloading gcsfs-2025.3.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (2.6.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (0.21.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from accelerate->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from accelerate->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from accelerate->-r requirements.txt (line 1)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from accelerate->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from accelerate->-r requirements.txt (line 1)) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from transformers==4.50.0->-r requirements.txt (line 2)) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from transformers==4.50.0->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from transformers==4.50.0->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from transformers==4.50.0->-r requirements.txt (line 2)) (0.21.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from transformers==4.50.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from datasets==3.5.0->-r requirements.txt (line 3)) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from datasets==3.5.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from datasets==3.5.0->-r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from datasets==3.5.0->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from datasets==3.5.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from datasets==3.5.0->-r requirements.txt (line 3)) (3.11.14)\n",
      "Requirement already satisfied: tyro>=0.5.11 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from trl==0.11.4->-r requirements.txt (line 5)) (0.9.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from huggingface_hub>=0.20.0->-r requirements.txt (line 6)) (4.13.0)\n",
      "Requirement already satisfied: decorator>4.1.2 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from gcsfs->-r requirements.txt (line 11)) (5.2.1)\n",
      "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading gcsfs-2025.3.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading gcsfs-2025.3.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading gcsfs-2025.2.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading gcsfs-2024.12.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting google-auth>=1.2 (from gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-auth-oauthlib (from gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-storage (from gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading google_cloud_storage-3.1.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 12)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 12)) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 12)) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from torch->-r requirements.txt (line 12)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from torchvision->-r requirements.txt (line 14)) (11.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 3)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 3)) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 3)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 3)) (1.18.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.2->gcsfs->-r requirements.txt (line 11))\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.2->gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.2->gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from requests->transformers==4.50.0->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from requests->transformers==4.50.0->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from requests->transformers==4.50.0->-r requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from requests->transformers==4.50.0->-r requirements.txt (line 2)) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.50.0->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 5)) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 5)) (14.0.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 5)) (4.4.2)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->gcsfs->-r requirements.txt (line 11))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage->gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.4.2 (from google-cloud-storage->gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.7.2 (from google-cloud-storage->gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage->gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading google_crc32c-1.7.1-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 12)) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from pandas->datasets==3.5.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from pandas->datasets==3.5.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from pandas->datasets==3.5.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading protobuf-6.30.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->-r requirements.txt (line 11))\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->-r requirements.txt (line 11))\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.5.0->-r requirements.txt (line 3)) (1.17.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->-r requirements.txt (line 11))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 5)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 5)) (0.1.2)\n",
      "Downloading gcsfs-2024.12.0-py2.py3-none-any.whl (35 kB)\n",
      "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
      "Downloading google_cloud_storage-3.1.0-py2.py3-none-any.whl (174 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.7.1-cp312-cp312-win_amd64.whl (33 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-6.30.2-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pyasn1, protobuf, oauthlib, google-crc32c, cachetools, rsa, requests-oauthlib, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, google-auth, google-auth-oauthlib, google-api-core, google-cloud-core, google-cloud-storage, gcsfs\n",
      "Successfully installed cachetools-5.5.2 gcsfs-2024.12.0 google-api-core-2.24.2 google-auth-2.39.0 google-auth-oauthlib-1.2.2 google-cloud-core-2.4.3 google-cloud-storage-3.1.0 google-crc32c-1.7.1 google-resumable-media-2.7.2 googleapis-common-protos-1.70.0 oauthlib-3.2.2 proto-plus-1.26.1 protobuf-6.30.2 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-oauthlib-2.0.0 rsa-4.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# These are the core libraries: Transformers, Datasets, PEFT (for LoRA), TRL (Trainer), BitsAndBytes (4-bit quant)\n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data points: #602\n",
      "validation data points: #100\n",
      "test data points: #74\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Prepare the Data ---\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        json_obj = json.loads(line)\n",
    "                        messages = json_obj.get(\"messages\", [])\n",
    "                        system_content = None\n",
    "                        user_content = None\n",
    "                        assistant_content = None\n",
    "\n",
    "                        for message in messages:\n",
    "                            role = message.get(\"role\")\n",
    "                            content = message.get(\"content\")\n",
    "                            if role == \"system\" and content:\n",
    "                                system_content = content\n",
    "                            elif role == \"user\" and content:\n",
    "                                user_content = content\n",
    "                            elif role == \"assistant\" and content:\n",
    "                                assistant_content = content\n",
    "\n",
    "                        if user_content and assistant_content:\n",
    "                            input_text = (system_content + \" \" if system_content else \"\") + user_content\n",
    "                            data.append({\"input\": input_text.strip(), \"output\": assistant_content})\n",
    "                        else:\n",
    "                            print(f\"Warning: Skipping malformed line in {file_path}: {line.strip()}\")\n",
    "\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Warning: Skipping invalid JSON line in {file_path}: {line.strip()}\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: File not found: {e}\")\n",
    "            return None\n",
    "    return Dataset.from_dict({\"input\": [item[\"input\"] for item in data], \"output\": [item[\"output\"] for item in data]})\n",
    "\n",
    "# Load data for each split and type\n",
    "spec_train_files = [\"data/spec_training_data.jsonl\"]\n",
    "spec_eval_files = [\"data/spec_validation_data.jsonl\"]\n",
    "spec_test_files = [\"data/spec_test_data.jsonl\"]\n",
    "\n",
    "wf_train_files = [\"data/wf_training_data.jsonl\"]\n",
    "wf_eval_files = [\"data/wf_validation_data.jsonl\"]\n",
    "wf_test_files = [\"data/wf_test_data.jsonl\"]\n",
    "\n",
    "spec_train_dataset = load_data(spec_train_files)\n",
    "spec_eval_dataset = load_data(spec_eval_files)\n",
    "spec_test_dataset = load_data(spec_test_files)\n",
    "\n",
    "wf_train_dataset = load_data(wf_train_files)\n",
    "wf_eval_dataset = load_data(wf_eval_files)\n",
    "wf_test_dataset = load_data(wf_test_files)\n",
    "\n",
    "# Merge the datasets for each split\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input\": spec_train_dataset[\"input\"] + wf_train_dataset[\"input\"],\n",
    "    \"output\": spec_train_dataset[\"output\"] + wf_train_dataset[\"output\"]\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"input\": spec_eval_dataset[\"input\"] + wf_eval_dataset[\"input\"],\n",
    "    \"output\": spec_eval_dataset[\"output\"] + wf_eval_dataset[\"output\"]\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input\": spec_test_dataset[\"input\"] + wf_test_dataset[\"input\"],\n",
    "    \"output\": spec_test_dataset[\"output\"] + wf_test_dataset[\"output\"]\n",
    "})\n",
    "\n",
    "# Create a single DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": eval_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "if raw_datasets[\"train\"] is None or raw_datasets[\"validation\"] is None or raw_datasets[\"test\"] is None:\n",
    "    print(\"Error loading datasets. Please check file paths and contents.\")\n",
    "else:\n",
    "    print(f\"training data points: #{len(raw_datasets['train'])}\")\n",
    "    print(f\"validation data points: #{len(raw_datasets['validation'])}\")\n",
    "    print(f\"test data points: #{len(raw_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# --- 2. Login to Hugging Face Hub ---\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/602 [00:00<?, ? examples/s]c:\\Users\\fab_c\\work\\github\\smartrics\\tamarind-finetune\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 602/602 [00:16<00:00, 36.05 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:02<00:00, 45.34 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:02<00:00, 34.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Load Tokenizer and Model ---\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"Salesforce/codet5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "max_input_length = 4096  # Define your desired max input length\n",
    "max_output_length = 4096 # Define your desired max output length\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"input\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"output\"], max_length=max_output_length, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1. Configure Training Arguments ---\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import os\n",
    "\n",
    "# Set the WANDB_MODE environment variable to 'disabled'\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "output_dir = \"./codet5-tamarind\"  # Adjust output directory\n",
    "learning_rate = 1e-5  # Adjusted for small dataset\n",
    "batch_size = 8      # Adjusted for small dataset\n",
    "num_epochs = 20     # Set a higher number of epochs as early stopping will handle it\n",
    "gradient_accumulation_steps = 2\n",
    "weight_decay = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"smartrics/codet5-tamarind\", \n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"eval_loss\", \n",
    "    greater_is_better=False, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "# --- 4.2. Define the Trainer with Early Stopping Callback ---\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Train the Model ---\n",
    "print(\"Starting training with early stopping...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Push the Model to Hugging Face Hub ---\n",
    "print(\"Pushing model to Hugging Face Hub...\")\n",
    "trainer.push_to_hub()\n",
    "print(f\"Model pushed to https://huggingface.co/{training_args.hub_model_id}\")\n",
    "\n",
    "print(\"Fine-tuning complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
