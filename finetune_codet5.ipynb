{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clone the repo and setup environment\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo_dir = \"./tamarind-finetune\"\n",
    "repo_url = \"https://github.com/smartrics/tamarind-finetune.git\"\n",
    "\n",
    "if os.path.isdir(repo_dir):\n",
    "    print(\"Directory 'tamarind-finetune' exists. Pulling latest changes...\")\n",
    "    subprocess.run([\"git\", \"-C\", repo_dir, \"pull\"], check=True)\n",
    "else:\n",
    "    print(\"Directory 'tamarind-finetune' does not exist. Cloning repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", repo_url, repo_dir], check=True)\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./tamarind-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the core libraries: Transformers, Datasets, PEFT (for LoRA), TRL (Trainer), BitsAndBytes (4-bit quant)\n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Authenticate (you'll be prompted)\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Prepare the Data ---\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "\n",
    "def load_and_combine_data(spec_file, wf_file):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(spec_file, 'r') as f_spec, open(wf_file, 'r') as f_wf:\n",
    "            for line_spec, line_wf in zip(f_spec, f_wf):\n",
    "                try:\n",
    "                    spec_obj = json.loads(line_spec)\n",
    "                    wf_obj = json.loads(line_wf)\n",
    "\n",
    "                    spec_messages = spec_obj.get(\"messages\", [])\n",
    "                    wf_messages = wf_obj.get(\"messages\", [])\n",
    "\n",
    "                    spec_system_content = None\n",
    "                    spec_user_content = None\n",
    "                    spec_assistant_content = None\n",
    "\n",
    "                    wf_system_content = None\n",
    "                    wf_user_content = None\n",
    "                    wf_assistant_content = None\n",
    "\n",
    "                    for message in spec_messages:\n",
    "                        role = message.get(\"role\")\n",
    "                        content = message.get(\"content\")\n",
    "                        if role == \"system\" and content:\n",
    "                            spec_system_content = content\n",
    "                        elif role == \"user\" and content:\n",
    "                            spec_user_content = content\n",
    "                        elif role == \"assistant\" and content:\n",
    "                            spec_assistant_content = content\n",
    "\n",
    "                    for message in wf_messages:\n",
    "                        role = message.get(\"role\")\n",
    "                        content = message.get(\"content\")\n",
    "                        if role == \"system\" and content:\n",
    "                            wf_system_content = content\n",
    "                        elif role == \"user\" and content:\n",
    "                            wf_user_content = content\n",
    "                        elif role == \"assistant\" and content:\n",
    "                            wf_assistant_content = content\n",
    "\n",
    "                    # Combine data from both files (you might need a more specific way to align them based on 'id' or other criteria)\n",
    "                    if spec_user_content and spec_assistant_content and wf_user_content and wf_assistant_content:\n",
    "                        # Example: Concatenate the user inputs and assistant outputs\n",
    "                        combined_input = (spec_system_content + \" \" if spec_system_content else \"\") + spec_user_content + \\\n",
    "                                         \" [SEP] \" + \\\n",
    "                                         (wf_system_content + \" \" if wf_system_content else \"\") + wf_user_content\n",
    "                        combined_output = spec_assistant_content + \" [SEP] \" + wf_assistant_content\n",
    "                        data.append({\"input\": combined_input, \"output\": combined_output})\n",
    "                    elif spec_user_content and spec_assistant_content:\n",
    "                        data.append({\"input\": (spec_system_content + \" \" if spec_system_content else \"\") + spec_user_content,\n",
    "                                     \"output\": spec_assistant_content})\n",
    "                    elif wf_user_content and wf_assistant_content:\n",
    "                        data.append({\"input\": (wf_system_content + \" \" if wf_system_content else \"\") + wf_user_content,\n",
    "                                     \"output\": wf_assistant_content})\n",
    "                    else:\n",
    "                        print(f\"Warning: Skipping misaligned or incomplete data pair: {line_spec.strip()} - {line_wf.strip()}\")\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Warning: Skipping invalid JSON line: {e}\")\n",
    "                except ValueError:\n",
    "                    print(\"Warning: Files have different number of lines. Processing will stop at the shorter file.\")\n",
    "                    break\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found: {e}\")\n",
    "        return None\n",
    "    return Dataset.from_dict({\"input\": [item[\"input\"] for item in data], \"output\": [item[\"output\"] for item in data]})\n",
    "\n",
    "# Load and combine data for each split\n",
    "train_dataset = load_and_combine_data(\"spec_training_data.jsonl\", \"wf_training_data.jsonl\")\n",
    "eval_dataset = load_and_combine_data(\"spec_validation_data.jsonl\", \"wf_validation_data.jsonl\")\n",
    "test_dataset = load_and_combine_data(\"spec_test_data.jsonl\", \"wf_test_data.jsonl\")\n",
    "\n",
    "# Create a DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": eval_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "if raw_datasets[\"train\"] is None or raw_datasets[\"validation\"] is None or raw_datasets[\"test\"] is None:\n",
    "    print(\"Error loading datasets. Please check file paths and contents.\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load Tokenizer and Model ---\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/codet5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "prefix = \"\" # Adjust prefix if needed\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"input\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"output\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Configure Training Arguments ---\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "\n",
    "output_dir = \"./codet5-combined-tuned\"  # Adjust output directory\n",
    "learning_rate = 1e-5  # Adjusted for small dataset\n",
    "batch_size = 8      # Adjusted for small dataset\n",
    "num_epochs = 20     # Set a higher number of epochs as early stopping will handle it\n",
    "gradient_accumulation_steps = 2\n",
    "weight_decay = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"your_huggingface_username/codet5-combined-tuned\", # Update hub name\n",
    "    load_best_model_at_end=True, # Optional: Load the best model based on the monitored metric\n",
    "    metric_for_best_model=\"eval_loss\", # Specify the metric to track for the best model\n",
    "    greater_is_better=False, # Set to False if monitoring loss (lower is better)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. Define the Trainer with Early Stopping Callback ---\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(patience=3, monitor=\"eval_loss\", min_delta=0.001)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Login to Hugging Face Hub ---\n",
    "notebook_login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Train the Model ---\n",
    "print(\"Starting training with early stopping...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Push the Model to Hugging Face Hub ---\n",
    "print(\"Pushing model to Hugging Face Hub...\")\n",
    "trainer.push_to_hub()\n",
    "print(f\"Model pushed to https://huggingface.co/{training_args.hub_model_id}\")\n",
    "\n",
    "print(\"Fine-tuning complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
