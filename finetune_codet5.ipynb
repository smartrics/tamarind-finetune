{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clone the repo and setup environment\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo_dir = \"./tamarind-finetune\"\n",
    "repo_url = \"https://github.com/smartrics/tamarind-finetune.git\"\n",
    "\n",
    "if os.path.isdir(repo_dir):\n",
    "    print(\"Directory 'tamarind-finetune' exists. Pulling latest changes...\")\n",
    "    subprocess.run([\"git\", \"-C\", repo_dir, \"pull\"], check=True)\n",
    "else:\n",
    "    print(\"Directory 'tamarind-finetune' does not exist. Cloning repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", repo_url, repo_dir], check=True)\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./tamarind-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the core libraries: Transformers, Datasets, PEFT (for LoRA), TRL (Trainer), BitsAndBytes (4-bit quant)\n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Prepare the Data ---\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        json_obj = json.loads(line)\n",
    "                        messages = json_obj.get(\"messages\", [])\n",
    "                        system_content = None\n",
    "                        user_content = None\n",
    "                        assistant_content = None\n",
    "\n",
    "                        for message in messages:\n",
    "                            role = message.get(\"role\")\n",
    "                            content = message.get(\"content\")\n",
    "                            if role == \"system\" and content:\n",
    "                                system_content = content\n",
    "                            elif role == \"user\" and content:\n",
    "                                user_content = content\n",
    "                            elif role == \"assistant\" and content:\n",
    "                                assistant_content = content\n",
    "\n",
    "                        if user_content and assistant_content:\n",
    "                            input_text = (system_content + \" \" if system_content else \"\") + user_content\n",
    "                            data.append({\"input\": input_text.strip(), \"output\": assistant_content})\n",
    "                        else:\n",
    "                            print(f\"Warning: Skipping malformed line in {file_path}: {line.strip()}\")\n",
    "\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Warning: Skipping invalid JSON line in {file_path}: {line.strip()}\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: File not found: {e}\")\n",
    "            return None\n",
    "    return Dataset.from_dict({\"input\": [item[\"input\"] for item in data], \"output\": [item[\"output\"] for item in data]})\n",
    "\n",
    "# Load data for each split and type\n",
    "spec_train_files = [\"data/spec_training_data.jsonl\"]\n",
    "spec_eval_files = [\"data/spec_validation_data.jsonl\"]\n",
    "spec_test_files = [\"data/spec_test_data.jsonl\"]\n",
    "\n",
    "wf_train_files = [\"data/wf_training_data.jsonl\"]\n",
    "wf_eval_files = [\"data/wf_validation_data.jsonl\"]\n",
    "wf_test_files = [\"data/wf_test_data.jsonl\"]\n",
    "\n",
    "spec_train_dataset = load_data(spec_train_files)\n",
    "spec_eval_dataset = load_data(spec_eval_files)\n",
    "spec_test_dataset = load_data(spec_test_files)\n",
    "\n",
    "wf_train_dataset = load_data(wf_train_files)\n",
    "wf_eval_dataset = load_data(wf_eval_files)\n",
    "wf_test_dataset = load_data(wf_test_files)\n",
    "\n",
    "# Merge the datasets for each split\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input\": spec_train_dataset[\"input\"] + wf_train_dataset[\"input\"],\n",
    "    \"output\": spec_train_dataset[\"output\"] + wf_train_dataset[\"output\"]\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    \"input\": spec_eval_dataset[\"input\"] + wf_eval_dataset[\"input\"],\n",
    "    \"output\": spec_eval_dataset[\"output\"] + wf_eval_dataset[\"output\"]\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input\": spec_test_dataset[\"input\"] + wf_test_dataset[\"input\"],\n",
    "    \"output\": spec_test_dataset[\"output\"] + wf_test_dataset[\"output\"]\n",
    "})\n",
    "\n",
    "# Create a single DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": eval_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "if raw_datasets[\"train\"] is None or raw_datasets[\"validation\"] is None or raw_datasets[\"test\"] is None:\n",
    "    print(\"Error loading datasets. Please check file paths and contents.\")\n",
    "else:\n",
    "    print(f\"training data points: #{len(raw_datasets['train'])}\")\n",
    "    print(f\"validation data points: #{len(raw_datasets['validation'])}\")\n",
    "    print(f\"test data points: #{len(raw_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# --- 2. Login to Hugging Face Hub ---\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Load Tokenizer and Model ---\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"Salesforce/codet5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "max_input_length = 4096  # Define your desired max input length\n",
    "max_output_length = 4096 # Define your desired max output length\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"input\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"output\"], max_length=max_output_length, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1. Configure Training Arguments ---\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import os\n",
    "\n",
    "# Set the WANDB_MODE environment variable to 'disabled'\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "output_dir = \"./codet5-tamarind\"  # Adjust output directory\n",
    "learning_rate = 1e-5  # Adjusted for small dataset\n",
    "batch_size = 8      # Adjusted for small dataset\n",
    "num_epochs = 20     # Set a higher number of epochs as early stopping will handle it\n",
    "gradient_accumulation_steps = 2\n",
    "weight_decay = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"smartrics/codet5-tamarind\", \n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"eval_loss\", \n",
    "    greater_is_better=False, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "# --- 4.2. Define the Trainer with Early Stopping Callback ---\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Train the Model ---\n",
    "print(\"Starting training with early stopping...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Push the Model to Hugging Face Hub ---\n",
    "print(\"Pushing model to Hugging Face Hub...\")\n",
    "trainer.push_to_hub()\n",
    "print(f\"Model pushed to https://huggingface.co/{training_args.hub_model_id}\")\n",
    "\n",
    "print(\"Fine-tuning complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
