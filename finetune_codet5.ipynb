{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqZeZlz6KGgB"
      },
      "outputs": [],
      "source": [
        "# Step 1: Clone the repo and setup environment\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "repo_dir = \"./tamarind-finetune\"\n",
        "repo_url = \"https://github.com/smartrics/tamarind-finetune.git\"\n",
        "\n",
        "if os.path.isdir(repo_dir):\n",
        "    print(\"Directory 'tamarind-finetune' exists. Pulling latest changes...\")\n",
        "    subprocess.run([\"git\", \"-C\", repo_dir, \"pull\"], check=True)\n",
        "else:\n",
        "    print(\"Directory 'tamarind-finetune' does not exist. Cloning repository...\")\n",
        "    subprocess.run([\"git\", \"clone\", repo_url, repo_dir], check=True)\n",
        "print(\"finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC1HVjTjKGgC"
      },
      "outputs": [],
      "source": [
        "%cd ./tamarind-finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIrUJnP5KGgC"
      },
      "outputs": [],
      "source": [
        "# These are the core libraries: Transformers, Datasets, PEFT (for LoRA), TRL (Trainer), BitsAndBytes (4-bit quant)\n",
        "%pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol8XN8CGKGgD"
      },
      "outputs": [],
      "source": [
        "# --- 1. Prepare the Data ---\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load each split from JSONL files\n",
        "train_dataset = load_dataset(\"json\", data_files=\"data_codet5/training_data.jsonl\", split=\"train\")\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"data_codet5/validation_data.jsonl\", split=\"train\")\n",
        "test_dataset = load_dataset(\"json\", data_files=\"data_codet5/test_data.jsonl\", split=\"train\")\n",
        "\n",
        "\n",
        "# Create a single DatasetDict\n",
        "raw_datasets = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": eval_dataset,\n",
        "    \"test\": test_dataset\n",
        "})\n",
        "\n",
        "if raw_datasets[\"train\"] is None or raw_datasets[\"validation\"] is None or raw_datasets[\"test\"] is None:\n",
        "    print(\"Error loading datasets. Please check file paths and contents.\")\n",
        "else:\n",
        "    print(f\"training data points: #{len(raw_datasets['train'])}\")\n",
        "    print(f\"validation data points: #{len(raw_datasets['validation'])}\")\n",
        "    print(f\"test data points: #{len(raw_datasets['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMuw3jmLKGgD"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# --- 2. Login to Hugging Face Hub ---\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Load Tokenizer and Model ---\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # ensure no pad_token error\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "max_length = 2048  # StarCoder's context window\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    prompts = examples[\"input\"]\n",
        "    completions = examples[\"output\"]\n",
        "\n",
        "    full_texts = [p + tokenizer.eos_token + c for p, c in zip(prompts, completions)]\n",
        "    tokenized = tokenizer(\n",
        "        full_texts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"longest\"\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for prompt, input_ids in zip(prompts, tokenized[\"input_ids\"]):\n",
        "        prompt_len = len(tokenizer(prompt + tokenizer.eos_token).input_ids)\n",
        "        label = input_ids.copy()\n",
        "        label[:prompt_len] = [-100] * prompt_len  # mask prompt in labels\n",
        "        labels.append(label)\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "    return tokenized\n",
        "\n",
        "def count_tokens(example):\n",
        "    input_ids = tokenizer(example[\"input\"], truncation=False).input_ids\n",
        "    output_ids = tokenizer(example[\"output\"], truncation=False).input_ids\n",
        "    return {\n",
        "        \"input_token_count\": len(input_ids),\n",
        "        \"output_token_count\": len(output_ids),\n",
        "        \"total_token_count\": len(input_ids) + len(output_ids),\n",
        "    }\n",
        "\n",
        "# Apply it to your dataset\n",
        "token_counts = raw_datasets[\"train\"].map(count_tokens)\n",
        "df = token_counts.to_pandas()\n",
        "print(df[[\"input_token_count\", \"output_token_count\", \"total_token_count\"]].describe())\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets[\"validation\"]\n",
        "\n",
        "print(\"OK\")\n"
      ],
      "metadata": {
        "id": "CJKYhqYDcYiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM_RNa_HKGgE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 4.1. Configure Training Arguments ---\n",
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import os\n",
        "\n",
        "# Set the WANDB_MODE environment variable to 'disabled'\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "output_dir = \"./starcoderbase-1b-tamarind\"  # Adjust output directory\n",
        "learning_rate = 1e-5  # Adjusted for small dataset\n",
        "batch_size = 1      # Adjusted for small dataset\n",
        "num_epochs = 20     # Set a higher number of epochs as early stopping will handle it\n",
        "gradient_accumulation_steps = 4\n",
        "weight_decay = 0.01\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    auto_find_batch_size=True,\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=weight_decay,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"smartrics/starcoderbase-1b-tamarind\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "# --- 4.2. Define the Trainer with Early Stopping Callback ---\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        ")\n",
        "\n",
        "print(\"ok\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJZ6VBE4KGgE"
      },
      "outputs": [],
      "source": [
        "# --- 5. Train the Model ---\n",
        "print(\"Starting training with early stopping...\")\n",
        "trainer.train()\n",
        "print(\"Training finished!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLtUpDsAKGgE"
      },
      "outputs": [],
      "source": [
        "# --- 6. Push the Model to Hugging Face Hub ---\n",
        "print(\"Pushing model to Hugging Face Hub...\")\n",
        "trainer.push_to_hub()\n",
        "print(f\"Model pushed to https://huggingface.co/{training_args.hub_model_id}\")\n",
        "\n",
        "print(\"Fine-tuning complete.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./tamarind-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the core libraries: Transformers, Datasets, PEFT (for LoRA), TRL (Trainer), BitsAndBytes (4-bit quant)\n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Prepare the Data ---\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load each split from JSONL files\n",
    "train_dataset = load_dataset(\"json\", data_files=\"data_codet5/training_data.jsonl\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"data_codet5/validation_data.jsonl\", split=\"train\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"data_codet5/test_data.jsonl\", split=\"train\")\n",
    "\n",
    "\n",
    "# Create a single DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": eval_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "if raw_datasets[\"train\"] is None or raw_datasets[\"validation\"] is None or raw_datasets[\"test\"] is None:\n",
    "    print(\"Error loading datasets. Please check file paths and contents.\")\n",
    "else:\n",
    "    print(f\"training data points: #{len(raw_datasets['train'])}\")\n",
    "    print(f\"validation data points: #{len(raw_datasets['validation'])}\")\n",
    "    print(f\"test data points: #{len(raw_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# --- 2. Login to Hugging Face Hub ---\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Load Tokenizer and Model ---\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # ensure no pad_token error\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "max_length = 2048  # StarCoder's context window\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    prompts = examples[\"input\"]\n",
    "    completions = examples[\"output\"]\n",
    "\n",
    "    full_texts = [p + tokenizer.eos_token + c for p, c in zip(prompts, completions)]\n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for prompt, input_ids in zip(prompts, tokenized[\"input_ids\"]):\n",
    "        prompt_len = len(tokenizer(prompt + tokenizer.eos_token).input_ids)\n",
    "        label = input_ids.copy()\n",
    "        label[:prompt_len] = [-100] * prompt_len  # mask prompt in labels\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "def count_tokens(example):\n",
    "    input_ids = tokenizer(example[\"input\"], truncation=False).input_ids\n",
    "    output_ids = tokenizer(example[\"output\"], truncation=False).input_ids\n",
    "    return {\n",
    "        \"input_token_count\": len(input_ids),\n",
    "        \"output_token_count\": len(output_ids),\n",
    "        \"total_token_count\": len(input_ids) + len(output_ids),\n",
    "    }\n",
    "\n",
    "# Apply it to your dataset\n",
    "token_counts = raw_datasets[\"train\"].map(count_tokens)\n",
    "df = token_counts.to_pandas()\n",
    "print(df[[\"input_token_count\", \"output_token_count\", \"total_token_count\"]].describe())\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "print(\"OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4.1. Configure Training Arguments ---\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import os\n",
    "\n",
    "# Set the WANDB_MODE environment variable to 'disabled'\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "output_dir = \"./deepseek-coder-1.3b-tamarind\"  # Adjust output directory\n",
    "learning_rate = 1e-5  # Adjusted for small dataset\n",
    "batch_size = 1      # Adjusted for small dataset\n",
    "num_epochs = 20     # Set a higher number of epochs as early stopping will handle it\n",
    "gradient_accumulation_steps = 4\n",
    "weight_decay = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"smartrics/deepseek-coder-1.3b-tamarind\", \n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"eval_loss\", \n",
    "    greater_is_better=False, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "# --- 4.2. Define the Trainer with Early Stopping Callback ---\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Train the Model ---\n",
    "print(\"Starting training with early stopping...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Push the Model to Hugging Face Hub ---\n",
    "print(\"Pushing model to Hugging Face Hub...\")\n",
    "trainer.push_to_hub()\n",
    "print(f\"Model pushed to https://huggingface.co/{training_args.hub_model_id}\")\n",
    "\n",
    "print(\"Fine-tuning complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
