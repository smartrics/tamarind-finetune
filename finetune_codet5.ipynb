{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clone the repo and setup environment\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo_dir = \"./tamarind-finetune\"\n",
    "repo_url = \"https://github.com/smartrics/tamarind-finetune.git\"\n",
    "\n",
    "if os.path.isdir(repo_dir):\n",
    "    print(\"Directory 'tamarind-finetune' exists. Pulling latest changes...\")\n",
    "    subprocess.run([\"git\", \"-C\", repo_dir, \"pull\"], check=True)\n",
    "else:\n",
    "    print(\"Directory 'tamarind-finetune' does not exist. Cloning repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", repo_url, repo_dir], check=True)\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./tamarind-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the core libraries: Transformers, Datasets, PEFT (for LoRA), TRL (Trainer), BitsAndBytes (4-bit quant)\n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Prepare the Data ---\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load each split from JSONL files\n",
    "train_dataset = load_dataset(\"json\", data_files=\"data_codet5/training_data.jsonl\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"data_codet5/validation_data.jsonl\", split=\"train\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"data_codet5/test_data.jsonl\", split=\"train\")\n",
    "\n",
    "\n",
    "# Create a single DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": eval_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "if raw_datasets[\"train\"] is None or raw_datasets[\"validation\"] is None or raw_datasets[\"test\"] is None:\n",
    "    print(\"Error loading datasets. Please check file paths and contents.\")\n",
    "else:\n",
    "    print(f\"training data points: #{len(raw_datasets['train'])}\")\n",
    "    print(f\"validation data points: #{len(raw_datasets['validation'])}\")\n",
    "    print(f\"test data points: #{len(raw_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# --- 2. Login to Hugging Face Hub ---\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Load Tokenizer and Model ---\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"Salesforce/codet5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "max_input_length = 4096  # Define your desired max input length\n",
    "max_output_length = 4096 # Define your desired max output length\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"input\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"output\"], max_length=max_output_length, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1. Configure Training Arguments ---\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import os\n",
    "\n",
    "# Set the WANDB_MODE environment variable to 'disabled'\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "output_dir = \"./codet5-tamarind\"  # Adjust output directory\n",
    "learning_rate = 1e-5  # Adjusted for small dataset\n",
    "batch_size = 8      # Adjusted for small dataset\n",
    "num_epochs = 20     # Set a higher number of epochs as early stopping will handle it\n",
    "gradient_accumulation_steps = 2\n",
    "weight_decay = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"smartrics/codet5-tamarind\", \n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"eval_loss\", \n",
    "    greater_is_better=False, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "# --- 4.2. Define the Trainer with Early Stopping Callback ---\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Train the Model ---\n",
    "print(\"Starting training with early stopping...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Push the Model to Hugging Face Hub ---\n",
    "print(\"Pushing model to Hugging Face Hub...\")\n",
    "trainer.push_to_hub()\n",
    "print(f\"Model pushed to https://huggingface.co/{training_args.hub_model_id}\")\n",
    "\n",
    "print(\"Fine-tuning complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
