{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Following https://github.com/bigcode-project/starcoder"
      ],
      "metadata": {
        "id": "FENgUg9ixK5y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqZeZlz6KGgB"
      },
      "outputs": [],
      "source": [
        "# Step 1: Clone the repo and setup environment\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "repo_dir = \"/content/tamarind-finetune\"\n",
        "repo_url = \"https://github.com/smartrics/tamarind-finetune.git\"\n",
        "\n",
        "if os.path.isdir(repo_dir):\n",
        "    print(\"Directory 'tamarind-finetune' exists. Pulling latest changes...\")\n",
        "    subprocess.run([\"git\", \"-C\", repo_dir, \"pull\"], check=True)\n",
        "else:\n",
        "    print(\"Directory 'tamarind-finetune' does not exist. Cloning repository...\")\n",
        "    subprocess.run([\"git\", \"clone\", repo_url, repo_dir], check=True)\n",
        "print(\"finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC1HVjTjKGgC"
      },
      "outputs": [],
      "source": [
        "%cd /content/tamarind-finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weâ€™ll finetune `bigcode/starcoderbase-1b`, which is a 1B parameter model trained on 80+ programming languages. This is a gated model, so if you plan to run this notebook with this exact model, youâ€™ll need to gain access to it on the modelâ€™s page. Log in to your Hugging Face account to do so:"
      ],
      "metadata": {
        "id": "MpJEMYoMxcxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIrUJnP5KGgC"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install datasets accelerate huggingface_hub bitsandbytes wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# --- 2. Login to Hugging Face Hub ---\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "YY9BwThwxU7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "6Ig-HkRGmSlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_starcoderbase/finetune.py \\\n",
        "  --model_path=\"bigcode/starcoderbase-1b\"\\\n",
        "  --dataset_path=\"./data_starcoderbase/tamarind_data.csv\" \\\n",
        "  --subset=\"data/finetune\"\\\n",
        "  --split=\"train\"\\\n",
        "  --size_valid_set 10000\\\n",
        "  --seq_length 1700 \\\n",
        "  --max_steps 1000\\\n",
        "  --batch_size 4\\\n",
        "  --input_column_name=\"question\"\\\n",
        "  --output_column_name=\"response\"\\\n",
        "  --gradient_accumulation_steps 16\\\n",
        "  --learning_rate 1e-4\\\n",
        "  --lr_scheduler_type=\"cosine\"\\\n",
        "  --num_warmup_steps 100\\\n",
        "  --weight_decay 0.05\\\n",
        "  --output_dir=\"./checkpoints\""
      ],
      "metadata": {
        "id": "PEGTVU9ymetJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4d8108-ec70-43e2-a73c-c2c0ce7b2ab7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-01 08:30:31.997278: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-01 08:30:32.016029: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746088232.037892    2155 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746088232.044622    2155 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-01 08:30:32.067023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 677/677 [00:00<00:00, 4.25MB/s]\n",
            "vocab.json: 100% 777k/777k [00:00<00:00, 14.0MB/s]\n",
            "merges.txt: 100% 442k/442k [00:00<00:00, 13.1MB/s]\n",
            "tokenizer.json: 100% 2.06M/2.06M [00:00<00:00, 26.8MB/s]\n",
            "special_tokens_map.json: 100% 532/532 [00:00<00:00, 3.81MB/s]\n",
            "Generating train split: 776 examples [00:00, 20515.86 examples/s]\n",
            "Size of the train set: 698. Size of the validation set: 78\n",
            "100% 400/400 [00:00<00:00, 602.71it/s]\n",
            "The character to token ratio of the dataset is: 4.27\n",
            "Loading the model\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "config.json: 100% 1.05k/1.05k [00:00<00:00, 8.11MB/s]\n",
            "model.safetensors: 100% 4.55G/4.55G [00:19<00:00, 231MB/s]\n",
            "generation_config.json: 100% 111/111 [00:00<00:00, 916kB/s]\n",
            "trainable params: 7176192 || all params: 1144383488 || trainable%: 0.6270793029827428\n",
            "Starting main loop\n",
            "Training...\n",
            "No checkpoint found. Starting training from scratch.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msmartrics\u001b[0m (\u001b[33msmartrics-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/tamarind-finetune/wandb/run-20250501_083104-yd7toe51\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mStarCoder-finetuned\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/smartrics-personal/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/smartrics-personal/huggingface/runs/yd7toe51\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "[ETA] 10.0% complete â€” Elapsed: 3h 58m 28s, Remaining: 35h 46m 18s\n",
            "{'loss': 0.4574, 'grad_norm': 0.06452707201242447, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.1}\n",
            "[ETA] 10.0% complete â€” Elapsed: 3h 58m 30s, Remaining: 35h 46m 37s\n",
            "{'eval_loss': 0.23148095607757568, 'eval_runtime': 2.0262, 'eval_samples_per_second': 2.468, 'eval_steps_per_second': 0.987, 'epoch': 0.1}\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/tamarind-finetune/data_starcoderbase/finetune.py\", line 425, in <module>\n",
            "    main(args)\n",
            "  File \"/content/tamarind-finetune/data_starcoderbase/finetune.py\", line 414, in main\n",
            "    run_training(args, train_dataset, eval_dataset)\n",
            "  File \"/content/tamarind-finetune/data_starcoderbase/finetune.py\", line 405, in run_training\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
            "    return inner_training_loop(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3782, in training_step\n",
            "    self.accelerator.backward(loss, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\", line 2454, in backward\n",
            "    loss.backward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the SE dataset is better manageable when using streaming. We also have to precise the split of the dataset that is used. For more details, check the dataset's page on ðŸ¤—. Similarly we can modify the command to account for the availability of GPUs"
      ],
      "metadata": {
        "id": "XGOo3Ftf4LBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_starcoderbase/merge_peft_adapters.py \\\n",
        "   --base_model_name_or_path \"bigcode/starcoderbase-1b\" \\\n",
        "   --peft_model_path \"./checkpoints/checkpoint-100\" \\\n",
        "   --merged_model_name_or_path \"smartrics/starcoderbase-1b-tamarind\" \\\n",
        "   --push_to_hub"
      ],
      "metadata": {
        "id": "diAEUNB043nZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}