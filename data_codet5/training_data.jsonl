{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid arithmetic expression using addition.","output":"'col1 + col2'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications for each product, including product ID, name, category, and price.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"},{\"column\":\"column_4_id\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]},\"table_id2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including estimated sales volume and product ID.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"estimated_sales_volume\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Estimated sales volume for the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"research_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the market research entry.\"}]},\"table_id3\":{\"name\":\"historical_sales\",\"location\":\"C:/data/historical_sales.csv\",\"sheet_name\":null,\"label\":\"Historical Sales Data\",\"description\":\"Contains historical sales data for various products, not relevant for this workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"sales_volume\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Actual sales volume for the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"sales_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the sales entry.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel file.\",\"Filter the market research data to include only entries from the last year.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Calculate a new column for potential revenue by multiplying the estimated sales volume by the product price.\",\"Aggregate the data to find the total potential revenue by product category.\",\"Sort the aggregated data by total potential revenue in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading market research data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"`research_date` >= '2022-01-01'\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only entries from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_revenue\",\"formula\":\"`estimated_sales_volume` * `price`\"},\"output\":\"data_with_revenue\",\"comment\":\"Calculating potential revenue by multiplying estimated sales volume by product price.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_revenue\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"potential_revenue\",\"name\":\"total_potential_revenue\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find total potential revenue by product category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_potential_revenue\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total potential revenue in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'tan_transform' operation do?","output":"The 'tan_transform' operation applies a tangent transformation to cyclical data."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of products including demand score, cost, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"demand_score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"A score representing the demand level for the product.\"},{\"column\":\"col_3\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The cost of the product.\"},{\"column\":\"col_4\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"The category to which the product belongs.\"}]},\"table_id2\":{\"name\":\"supplier_information\",\"location\":\"C:/data/supplier_information.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains information about suppliers including supplier ID and product ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product supplied.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only items with a high demand score.\",\"Join the filtered product specifications with the supplier information table.\",\"Aggregate the joined data to calculate the total cost for each product category.\",\"Sort the aggregated data by total cost in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"demand_score > 80\"},\"output\":\"high_demand_products\",\"comment\":\"Filtering products to include only those with a demand score greater than 80.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_information.csv\",\"type\":\"csv\"},\"output\":\"supplier_information\",\"comment\":\"Loading supplier information from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_demand_products\",\"supplier_information\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"product_supplier_data\",\"comment\":\"Joining high demand products with supplier information on product_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"product_supplier_data\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"cost\",\"name\":\"total_cost\",\"function\":\"sum\"}]},\"output\":\"category_costs\",\"comment\":\"Aggregating data to calculate total cost for each product category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"category_costs\",\"sortBy\":\"total_cost\",\"order\":\"desc\"},\"output\":\"sorted_category_costs\",\"comment\":\"Sorting the aggregated data by total cost in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/legal_data/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases Data\",\"description\":\"This table contains information about court cases, including case ID, lawyer ID, case status, and year.\",\"columns\":[{\"column\":\"case_id\",\"column_name\":\"CASE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court case.\"},{\"column\":\"lawyer_id\",\"column_name\":\"LAWYER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"case_status\",\"column_name\":\"CASE_STATUS\",\"column_type\":\"xsd:string\",\"column_description\":\"Status of the case, e.g., 'successful', 'unsuccessful'.\"},{\"column\":\"year\",\"column_name\":\"YEAR\",\"column_type\":\"xsd:integer\",\"column_description\":\"Year the case was handled.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/legal_data/lawyers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Lawyers Information\",\"description\":\"This table contains information about lawyers, including lawyer ID and name.\",\"columns\":[{\"column\":\"lawyer_id\",\"column_name\":\"LAWYER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"lawyer_name\",\"column_name\":\"LAWYER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"legal_firms\",\"location\":\"C:/legal_data/legal_firms.xlsx\",\"sheet_name\":\"Firms\",\"label\":\"Legal Firms Data\",\"description\":\"This table contains information about legal firms, including firm ID and name.\",\"columns\":[{\"column\":\"firm_id\",\"column_name\":\"FIRM_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each legal firm.\"},{\"column\":\"firm_name\",\"column_name\":\"FIRM_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the legal firm.\"}]}},\"instructions\":[\"Load the 'court_cases' table from the CSV file located at C:/legal_data/court_cases.csv.\",\"Load the 'lawyers' table from the Excel file located at C:/legal_data/lawyers.xlsx.\",\"Filter the 'court_cases' table to include only cases from the year 2022.\",\"Join the filtered 'court_cases' table with the 'lawyers' table on the 'lawyer_id' column.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\",\"Sort the aggregated data by the total number of cases in descending order.\",\"Calculate a new column 'case_success_rate' as the ratio of 'successful_cases' to 'total_cases'.\",\"Filter the data to include only lawyers with a 'case_success_rate' greater than 0.75.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyers.xlsx\",\"type\":\"xlsx\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"YEAR == 2022\"},\"output\":\"court_cases_2022\",\"comment\":\"Filtering court cases to include only those from the year 2022.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"court_cases_2022\",\"lawyers\"],\"joinOn\":[\"LAWYER_ID\"],\"joinType\":\"inner\"},\"output\":\"cases_with_lawyers\",\"comment\":\"Joining filtered court cases with lawyers data on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"cases_with_lawyers\",\"groupBy\":\"LAWYER_ID\",\"aggregations\":[{\"column\":\"CASE_ID\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating data to find the total number of cases handled by each lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"lawyer_case_counts\",\"sortBy\":\"total_cases\",\"order\":\"desc\"},\"output\":\"sorted_lawyer_case_counts\",\"comment\":\"Sorting the aggregated data by total number of cases in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_lawyer_case_counts\",\"columnName\":\"case_success_rate\",\"formula\":\"successful_cases / total_cases\"},\"output\":\"lawyer_case_success_rate\",\"comment\":\"Calculating a new column 'case_success_rate' as the ratio of 'successful_cases' to 'total_cases'.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"lawyer_case_success_rate\",\"query\":\"case_success_rate > 0.75\"},\"output\":\"high_success_lawyers\",\"comment\":\"Filtering the data to include only lawyers with a 'case_success_rate' greater than 0.75.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"transactions\",\"location\":\"C:/data/transactions.csv\",\"sheet_name\":null,\"label\":\"Transactions Table\",\"description\":\"This table contains financial transaction records including transaction ID, customer ID, transaction amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer associated with the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"customers\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Information Table\",\"description\":\"This table contains customer information including customer ID, name, and contact details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_number\",\"column_type\":\"xsd:string\",\"column_description\":\"The contact number of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data Table\",\"description\":\"This table contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the transactions table from the CSV file located at C:/data/transactions.csv.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Load the customer information table from the Excel file located at C:/data/customers.xlsx.\",\"Join the filtered transactions with the customer information on the customer_id column.\",\"Calculate the total amount spent by each customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/transactions.csv\",\"type\":\"csv\"},\"output\":\"transactions\",\"comment\":\"Loading the transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customers\",\"comment\":\"Loading the customer information dataset from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customers\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer information on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_spent\",\"function\":\"sum\"}]},\"output\":\"customer_spending\",\"comment\":\"Calculating the total amount spent by each customer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_data\",\"location\":\"C:/data/healthcare/patient_data.csv\",\"sheet_name\":null,\"label\":\"Patient Data\",\"description\":\"Contains patient information including ID, age, BMI, and other health metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"bmi\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Body Mass Index of the patient.\"}]},\"table_id2\":{\"name\":\"medication_records\",\"location\":\"C:/data/healthcare/medication_records.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Records of medications prescribed to patients, including dosage and patient ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"},{\"column\":\"col_3\",\"column_name\":\"dosage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Dosage of the medication prescribed.\"}]},\"table_id3\":{\"name\":\"hospital_visits\",\"location\":\"C:/data/healthcare/hospital_visits.csv\",\"sheet_name\":null,\"label\":\"Hospital Visits\",\"description\":\"Details of hospital visits by patients, including visit date and reason.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"visit_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital visit.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_3\",\"column_name\":\"visit_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the hospital visit.\"}]}},\"instructions\":[\"Load the patient data table from the CSV file.\",\"Filter the patient data to include only those with a BMI greater than 25.\",\"Join the filtered patient data with the medication records based on patient ID.\",\"Aggregate the joined data to calculate the average medication dosage by age group.\",\"Sort the aggregated data by average dosage in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/healthcare/patient_data.csv\",\"type\":\"csv\"},\"output\":\"patient_data\",\"comment\":\"Loading the patient data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_data\",\"query\":\"`bmi` > 25\"},\"output\":\"filtered_patient_data\",\"comment\":\"Filtering patient data to include only those with a BMI greater than 25.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/healthcare/medication_records.csv\",\"type\":\"csv\"},\"output\":\"medication_records\",\"comment\":\"Loading the medication records from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_patient_data\",\"medication_records\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient data with medication records based on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"age\",\"aggregations\":[{\"column\":\"dosage\",\"name\":\"average_dosage\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined data to calculate the average medication dosage by age group.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_dosage\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by average dosage in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with details such as transaction ID, amount, currency code, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code of the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the transaction.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/exchange_rates.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Exchange Rates\",\"description\":\"A table containing exchange rates for various currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"A table with data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at 'C:/data/financial_transactions.csv'.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Load the 'exchange_rates' table from the Excel file located at 'C:/data/exchange_rates.xlsx'.\",\"Join the filtered 'financial_transactions' table with the 'exchange_rates' table on the 'currency_code' column using an inner join.\",\"Calculate a new column 'amount_in_usd' in the joined table by multiplying 'amount' with 'exchange_rate'.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates dataset from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with exchange rates on currency code using an inner join.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"amount_in_usd\",\"formula\":\"amount * exchange_rate\"},\"output\":\"transactions_in_usd\",\"comment\":\"Calculating the amount in USD by multiplying the transaction amount with the exchange rate.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat parameters are required for the 'identifyTable' action?","output":"'identifyTable' requires a 'location' parameter (URL or file path) and a 'type' parameter (either 'csv' or 'xlsx')."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains information about shipments including status, dispatch time, and expected delivery time.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"dispatch_time\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"Time when the shipment was dispatched.\"},{\"column\":\"col_4\",\"column_name\":\"expected_delivery_time\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"Expected time for the shipment to be delivered.\"},{\"column\":\"col_5\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse associated with the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_data\",\"location\":\"C:/data/logistics/warehouse_data.xlsx\",\"sheet_name\":null,\"label\":\"Warehouse Data\",\"description\":\"Contains information about warehouses including their locations.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"warehouse_location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/logistics/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"Contains inventory levels for various products in different warehouses.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse where the product is stored.\"},{\"column\":\"col_3\",\"column_name\":\"inventory_level\",\"column_type\":\"xsd:integer\",\"column_description\":\"Current inventory level of the product.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at C:/data/logistics/shipment_data.csv.\",\"Load the 'warehouse_data' table from the Excel file located at C:/data/logistics/warehouse_data.xlsx.\",\"Filter the 'shipment_data' table to include only shipments with a status of 'In Transit'.\",\"Join the filtered 'shipment_data' table with the 'warehouse_data' table on the 'warehouse_id' column using an inner join.\",\"Calculate a new column 'delivery_time' in the joined table by subtracting 'dispatch_time' from 'expected_delivery_time'.\",\"Aggregate the joined table by 'warehouse_location' to calculate the total number of shipments and average delivery time.\",\"Sort the aggregated data by 'total_shipments' in descending order.\",\"Apply a rolling mean transformation on the 'average_delivery_time' column with a window size of 3.\",\"Forecast the 'total_shipments' for the next 7 days using the 'holt_winters' algorithm with a daily frequency.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouse_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"warehouse_data\",\"comment\":\"Loading the warehouse data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"status == 'In Transit'\"},\"output\":\"in_transit_shipments\",\"comment\":\"Filtering shipments to include only those that are currently in transit.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"in_transit_shipments\",\"warehouse_data\"],\"joinOn\":[\"warehouse_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining in-transit shipments with warehouse data on warehouse_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"delivery_time\",\"formula\":\"`expected_delivery_time` - `dispatch_time`\"},\"output\":\"joined_data_with_delivery_time\",\"comment\":\"Calculating delivery time by subtracting dispatch time from expected delivery time.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_delivery_time\",\"groupBy\":\"warehouse_location\",\"aggregations\":[{\"column\":\"shipment_id\",\"name\":\"total_shipments\",\"function\":\"count\"},{\"column\":\"delivery_time\",\"name\":\"average_delivery_time\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by warehouse location to calculate total shipments and average delivery time.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_shipments\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total shipments in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"transformations\":[{\"columns\":[\"average_delivery_time\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":3}}]},\"output\":\"transformed_data\",\"comment\":\"Applying a rolling mean transformation on the average delivery time with a window size of 3.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_data\",\"columns\":[\"total_shipments\"],\"forecastParameters\":{\"forecastPeriod\":7,\"frequency\":\"D\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"dateColumn\":\"warehouse_location\"}},\"output\":\"forecast_results\",\"comment\":\"Forecasting total shipments for the next 7 days using the Holt-Winters algorithm with a daily frequency.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'forecastData' action with an incorrect 'nu' value for 'one_class_svm'.","output":"{ \"name\": \"forecastData\", \"parameters\": { \"table\": \"sensor_readings\", \"columns\": [\"temperature\"], \"forecastParameters\": { \"algorithm\": \"one_class_svm\", \"nu\": 1.5 }, \"output\": \"anomaly_detection\" } } // Error: 'nu' must be between 0 and 1."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipments\",\"location\":\"C:/data/logistics/shipments.csv\",\"sheet_name\":null,\"label\":\"Shipments Data\",\"description\":\"A table containing details of all shipments including status, weight, and warehouse_id.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_4\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse associated with the shipment.\"}]},\"table_id2\":{\"name\":\"warehouses\",\"location\":\"C:/data/logistics/warehouses.csv\",\"sheet_name\":null,\"label\":\"Warehouses Data\",\"description\":\"A table with information about warehouse locations and their identifiers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse.\"}]},\"table_id3\":{\"name\":\"drivers\",\"location\":\"C:/data/logistics/drivers.csv\",\"sheet_name\":null,\"label\":\"Drivers Data\",\"description\":\"A table containing information about drivers, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"driver_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each driver.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the driver.\"}]}},\"instructions\":[\"Load the 'shipments' table containing details of all shipments.\",\"Load the 'warehouses' table with information about warehouse locations.\",\"Filter the 'shipments' table to include only shipments with a status of 'In Transit'.\",\"Join the filtered shipments with the 'warehouses' table on the warehouse_id column.\",\"Calculate the total weight of shipments for each warehouse.\",\"Sort the resulting table by total weight in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipments.csv\",\"type\":\"csv\"},\"output\":\"shipments\",\"comment\":\"Loading the shipments dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouses.csv\",\"type\":\"csv\"},\"output\":\"warehouses\",\"comment\":\"Loading the warehouses dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipments\",\"query\":\"status == 'In Transit'\"},\"output\":\"in_transit_shipments\",\"comment\":\"Filtering shipments to include only those with a status of 'In Transit'.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"in_transit_shipments\",\"warehouses\"],\"joinOn\":[\"warehouse_id\"],\"joinType\":\"inner\"},\"output\":\"joined_shipments_warehouses\",\"comment\":\"Joining filtered shipments with warehouses on warehouse_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_shipments_warehouses\",\"groupBy\":\"warehouse_id\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"warehouse_weights\",\"comment\":\"Calculating total weight of shipments for each warehouse.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"warehouse_weights\",\"sortBy\":\"total_weight\",\"order\":\"desc\"},\"output\":\"sorted_warehouse_shipments\",\"comment\":\"Sorting the table by total weight in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of shipments including shipment ID, customer ID, weight, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer associated with the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_4\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the shipment.\"}]},\"table_id2\":{\"name\":\"customer_info\",\"location\":\"C:/data/logistics/customer_info.csv\",\"sheet_name\":null,\"label\":\"Customer Information\",\"description\":\"Contains customer details including customer ID, name, and contact information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_info\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact information for the customer.\"}]},\"table_id3\":{\"name\":\"vehicle_data\",\"location\":\"C:/data/logistics/vehicle_data.csv\",\"sheet_name\":null,\"label\":\"Vehicle Data\",\"description\":\"Contains details of vehicles used for shipments, including vehicle ID and capacity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"vehicle_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each vehicle.\"},{\"column\":\"col_2\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Capacity of the vehicle in kilograms.\"}]}},\"instructions\":[\"Load the table 'shipment_data' containing shipment details.\",\"Filter the shipment data to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered shipment data with 'customer_info' table on customer_id.\",\"Aggregate the joined data to calculate the total weight of shipments for each customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data containing details of each shipment.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"heavy_shipments\",\"comment\":\"Filtering shipments to include only those with a weight greater than 1000 kg.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/customer_info.csv\",\"type\":\"csv\"},\"output\":\"customer_info\",\"comment\":\"Loading the customer information table.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"heavy_shipments\",\"customer_info\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with customer information on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"customer_shipment_summary\",\"comment\":\"Aggregating data to calculate the total weight of shipments for each customer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with details such as transaction amount, currency code, and transaction type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"transaction_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code of the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of transaction, e.g., sale, refund.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/exchange_rates.xlsx\",\"sheet_name\":\"Rates\",\"label\":\"Exchange Rates\",\"description\":\"A table containing exchange rates for various currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_financial_data\",\"location\":\"C:/data/irrelevant_financial_data.xlsx\",\"sheet_name\":\"Irrelevant\",\"label\":\"Irrelevant Financial Data\",\"description\":\"A table containing financial data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_metric\",\"column_type\":\"xsd:decimal\",\"column_description\":\"An irrelevant financial metric.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from a CSV file.\",\"Load the 'exchange_rates' table from an Excel file.\",\"Filter the 'financial_transactions' table to include only transactions above $1000.\",\"Join the filtered transactions with the 'exchange_rates' table on the 'currency_code' column.\",\"Calculate a new column 'amount_in_usd' by multiplying 'transaction_amount' with 'exchange_rate'.\",\"Aggregate the data by 'transaction_type' to find the total amount in USD for each type.\",\"Sort the aggregated data by 'total_amount_usd' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"`transaction_amount` > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those above $1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"transactions_with_rates\",\"comment\":\"Joining filtered transactions with exchange rates on currency code.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"transactions_with_rates\",\"columnName\":\"amount_in_usd\",\"formula\":\"`transaction_amount` * `exchange_rate`\"},\"output\":\"transactions_with_usd\",\"comment\":\"Calculating amount in USD by multiplying transaction amount with exchange rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_usd\",\"groupBy\":\"transaction_type\",\"aggregations\":[{\"column\":\"amount_in_usd\",\"name\":\"total_amount_usd\",\"function\":\"sum\"}]},\"output\":\"aggregated_transactions\",\"comment\":\"Aggregating data by transaction type to find total amount in USD.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_transactions\",\"sortBy\":\"total_amount_usd\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_transactions\",\"comment\":\"Sorting aggregated data by total amount in USD in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the purpose of the 'calculateDerivedColumn' action?","output":"The 'calculateDerivedColumn' action creates a new column based on a formula applied to existing columns."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of products including their development status.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"development_status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current development status of the product.\"}]},\"table_id2\":{\"name\":\"team_assignments\",\"location\":\"C:/data/team_assignments.csv\",\"sheet_name\":null,\"label\":\"Team Assignments\",\"description\":\"Lists team members assigned to each product by product ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Product ID to which the team member is assigned.\"},{\"column\":\"col_2\",\"column_name\":\"team_member_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each team member.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"Contains sales data for each product, not relevant for this workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only products with a development status of 'in progress'.\",\"Join the filtered product specifications with the team assignments table on the product ID.\",\"Aggregate the joined table to calculate the total number of team members assigned to each product.\",\"Sort the aggregated data by the total number of team members in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"development_status == 'in progress'\"},\"output\":\"in_progress_products\",\"comment\":\"Filtering products to include only those with a development status of 'in progress'.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/team_assignments.csv\",\"type\":\"csv\"},\"output\":\"team_assignments\",\"comment\":\"Loading the team assignments from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"in_progress_products\",\"team_assignments\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"product_team_data\",\"comment\":\"Joining filtered products with team assignments on product ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"product_team_data\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"team_member_id\",\"name\":\"total_team_members\",\"function\":\"count\"}]},\"output\":\"product_team_count\",\"comment\":\"Aggregating data to calculate the total number of team members assigned to each product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"product_team_count\",\"sortBy\":\"total_team_members\",\"order\":\"desc\"},\"output\":\"sorted_product_team_count\",\"comment\":\"Sorting the aggregated data by the total number of team members in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Inventory\",\"description\":\"This table contains information about the raw materials used in the manufacturing process, including material ID, type, and stock levels.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"column_2_id\",\"column_name\":\"material_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of material, such as metal, plastic, etc.\"},{\"column\":\"column_3_id\",\"column_name\":\"stock\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"This table contains the production schedule, detailing the processes and materials required for manufacturing.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"col_2\",\"column_name\":\"daily_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Daily usage of the material in the production process.\"},{\"column\":\"col_3\",\"column_name\":\"days_scheduled\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of days the material is scheduled for use.\"}]},\"table_id3\":{\"name\":\"maintenance_logs\",\"location\":\"C:/data/manufacturing/maintenance_logs.csv\",\"sheet_name\":null,\"label\":\"Maintenance Logs\",\"description\":\"Logs of maintenance activities performed on manufacturing equipment.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"equipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the equipment.\"},{\"column\":\"col_B\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when maintenance was performed.\"},{\"column\":\"col_C\",\"column_name\":\"maintenance_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of maintenance performed.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains information about the materials used in manufacturing.\",\"Load the table 'production_schedule' which contains the schedule for manufacturing processes.\",\"Filter the 'raw_materials' table to include only materials with a stock greater than 100 units.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Calculate a new column 'required_stock' in the joined table by multiplying 'daily_usage' by 'days_scheduled'.\",\"Aggregate the joined table by 'material_type' to find the total 'required_stock' for each type.\",\"Sort the aggregated data by 'total_required_stock' in descending order to prioritize materials with the highest demand.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials inventory from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"stock > 100\"},\"output\":\"filtered_materials\",\"comment\":\"Filtering raw materials to include only those with stock greater than 100 units.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_materials\",\"production_schedule\"],\"joinOn\":[\"material_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered materials with production schedule on material_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"required_stock\",\"formula\":\"daily_usage * days_scheduled\"},\"output\":\"joined_data_with_required_stock\",\"comment\":\"Calculating required stock by multiplying daily usage by days scheduled.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_required_stock\",\"groupBy\":\"material_type\",\"aggregations\":[{\"column\":\"required_stock\",\"name\":\"total_required_stock\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by material type to find total required stock.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_required_stock\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total required stock in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"Contains records of energy consumption for the building, including date, energy usage, and number of occupants.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"occupants\",\"column_type\":\"xsd:integer\",\"column_description\":\"The number of occupants in the building.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"2023_weather\",\"label\":\"Weather Data\",\"description\":\"Contains weather information including date, temperature, and humidity levels.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity level on the given date.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Maintenance Schedule\",\"description\":\"Contains the maintenance schedule for building equipment.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"equipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The identifier for the equipment.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"},{\"column\":\"col_3\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"The status of the maintenance task.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from 2023.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing energy consumption by the number of occupants.\",\"Aggregate the joined data by month to calculate the total energy consumption and average temperature.\",\"Sort the aggregated data by total energy consumption in descending order.\",\"Forecast future energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records from 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered energy consumption data with the weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`energy_usage` / `occupants`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating a new column for energy efficiency by dividing energy consumption by the number of occupants.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating the joined data by month to calculate the total energy consumption and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total energy consumption in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"columns\":[\"total_energy_consumption\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting future energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'log_transform' operation be used?","output":"Use 'log_transform' when you need to stabilize variance and reduce skewness in data. This is often used in financial and economic datasets where values span multiple orders of magnitude."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'interpolate' operation do?","output":"The 'interpolate' operation fills missing values using interpolation. It requires 'parameters.method' such as 'linear' or 'spline'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'identifyTable' action due to an unsupported file type.","output":"{ \"name\": \"identifyTable\", \"parameters\": { \"location\": \"https://example.com/data.xml\", \"type\": \"xml\" }, \"output\": \"sales_data\" }"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid expression using general functions.","output":"'abs(col3) > 10'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'rolling_quantile' operation be used?","output":"Use 'rolling_quantile' when you need to compute quantiles over a rolling window in time series data, such as calculating the moving median of stock prices."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/legal_data/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases\",\"description\":\"A table containing details of various legal cases including case ID, lawyer ID, and case date.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"column_3_id\",\"column_name\":\"case_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/legal_data/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers\",\"description\":\"A table containing details of lawyers including lawyer ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_judges\",\"location\":\"C:/legal_data/court_judges.csv\",\"sheet_name\":null,\"label\":\"Court Judges\",\"description\":\"A table containing details of judges including judge ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"judge_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each judge.\"},{\"column\":\"column_2_id\",\"column_name\":\"judge_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the judge.\"}]}},\"instructions\":[\"Load the table 'court_cases' which contains details of various legal cases.\",\"Filter the 'court_cases' table to include only cases from the year 2022.\",\"Join the filtered 'court_cases' table with the 'lawyers' table on the 'lawyer_id' column.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"`case_date`.year == 2022\"},\"output\":\"filtered_court_cases\",\"comment\":\"Filtering court cases to include only those from the year 2022.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyers.csv\",\"type\":\"csv\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_court_cases\",\"lawyers\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_cases_lawyers\",\"comment\":\"Joining filtered court cases with lawyers data on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_cases_lawyers\",\"groupBy\":\"lawyer_name\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating the joined table to find the total number of cases handled by each lawyer.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'identifyTable' action.","output":"{ \"name\": \"identifyTable\", \"parameters\": { \"location\": \"https://example.com/data.csv\", \"type\": \"csv\" }, \"output\": \"sales_data\" }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/hr/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees, including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/hr/department_data.csv\",\"sheet_name\":null,\"label\":\"Department Data\",\"description\":\"Contains information about departments, including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the department.\"}]},\"table_id3\":{\"name\":\"project_data\",\"location\":\"C:/data/hr/project_data.csv\",\"sheet_name\":null,\"label\":\"Project Data\",\"description\":\"Contains information about projects, including project IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"project_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the project.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/hr/employee_data.csv.\",\"Filter the employee data to include only those with a salary greater than 50,000.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Aggregate the joined data to calculate the average salary per department.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading the employee data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employee data to include only those with a salary greater than 50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/department_data.csv\",\"type\":\"csv\"},\"output\":\"department_data\",\"comment\":\"Loading the department data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employee_data\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"average_salary_per_department\",\"comment\":\"Aggregating data to calculate the average salary per department.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid column name usage with special characters.","output":"'`a column name` + col2'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_details\",\"location\":\"C:/legal_data/case_details.csv\",\"sheet_name\":null,\"label\":\"Case Details\",\"description\":\"A table containing information about various legal cases, including case ID, lawyer ID, and case year.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"case_year\",\"column_type\":\"xsd:integer\",\"column_description\":\"Year in which the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyer_info\",\"location\":\"C:/legal_data/lawyer_info.csv\",\"sheet_name\":null,\"label\":\"Lawyer Information\",\"description\":\"A table containing details about lawyers, including their ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedule\",\"location\":\"C:/legal_data/court_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Court Schedule\",\"description\":\"A table containing the schedule of court hearings, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"hearing_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hearing.\"},{\"column\":\"col_2\",\"column_name\":\"hearing_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court hearing.\"}]}},\"instructions\":[\"Load the 'case_details' table containing information about legal cases.\",\"Load the 'lawyer_info' table containing details about lawyers.\",\"Filter the 'case_details' table to include only cases from the year 2023.\",\"Join the filtered 'case_details' table with the 'lawyer_info' table on the lawyer_id column.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\",\"Sort the aggregated data by the total number of cases in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_details.csv\",\"type\":\"csv\"},\"output\":\"case_details\",\"comment\":\"Loading the table containing information about legal cases.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyer_info.csv\",\"type\":\"csv\"},\"output\":\"lawyer_info\",\"comment\":\"Loading the table containing details about lawyers.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_details\",\"query\":\"case_year == 2023\"},\"output\":\"filtered_cases_2023\",\"comment\":\"Filtering the case details to include only cases from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_cases_2023\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"cases_with_lawyers\",\"comment\":\"Joining filtered case details with lawyer information on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"cases_with_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating to find the total number of cases handled by each lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"lawyer_case_counts\",\"sortBy\":\"total_cases\",\"order\":\"desc\"},\"output\":\"sorted_lawyer_case_counts\",\"comment\":\"Sorting the aggregated data by the total number of cases in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat parameters are required for the 'applyTimeSeriesTransformations' action?","output":"'applyTimeSeriesTransformations' requires a 'table' (name of the table) and 'transformations' (a list of transformations, each specifying 'columns', 'operation', and optional 'parameters')."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"This table contains customer feedback including ratings and comments for various products.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"product_category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product being reviewed.\"},{\"column\":\"col_3\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating for the product, ranging from 1 to 5.\"},{\"column\":\"col_4\",\"column_name\":\"feedback\",\"column_type\":\"xsd:string\",\"column_description\":\"Textual feedback provided by the customer.\"}]},\"table_id2\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.xlsx\",\"sheet_name\":\"Products\",\"label\":\"Product Catalog\",\"description\":\"A catalog of products with details like product ID, name, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"product_category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback to include only those with a rating of 4 or 5.\",\"Aggregate the filtered feedback by product category to calculate the average rating.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading the customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback to include only those with a rating of 4 or 5.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"positive_feedback\",\"groupBy\":\"product_category\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_by_category\",\"comment\":\"Aggregating feedback by product category to calculate the average rating.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information of customers including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"marketing_campaign_results\",\"location\":\"C:/data/marketing_campaign_results.xlsx\",\"sheet_name\":\"Results\",\"label\":\"Marketing Campaign Results\",\"description\":\"Contains results of various marketing campaigns including success status and customer IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"success\",\"column_type\":\"xsd:boolean\",\"column_description\":\"Indicates whether the campaign was successful.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"data_point\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant data point.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the marketing campaign results from the Excel file.\",\"Filter the marketing campaign results to include only successful campaigns.\",\"Join the filtered campaign results with customer demographics on customer ID.\",\"Aggregate the joined data to find the average age of customers for each campaign.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing_campaign_results.xlsx\",\"type\":\"xlsx\"},\"output\":\"marketing_campaign_results\",\"comment\":\"Loading marketing campaign results from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"marketing_campaign_results\",\"query\":\"success == True\"},\"output\":\"successful_campaigns\",\"comment\":\"Filtering marketing campaign results to include only successful campaigns.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"successful_campaigns\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining successful campaign results with customer demographics on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"campaign_id\",\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"campaign_age_analysis\",\"comment\":\"Aggregating joined data to find the average age of customers for each campaign.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/marketing/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information about customers, including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"marketing_campaign_results\",\"location\":\"C:/data/marketing/campaign_results.xlsx\",\"sheet_name\":\"Results\",\"label\":\"Marketing Campaign Results\",\"description\":\"Contains results of various marketing campaigns, including success metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"success\",\"column_type\":\"xsd:boolean\",\"column_description\":\"Indicates whether the campaign was successful.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/marketing/irrelevant_data.xlsx\",\"sheet_name\":\"Irrelevant\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column for this workflow.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the marketing campaign results table from the Excel file.\",\"Filter the marketing campaign results to include only successful campaigns.\",\"Join the filtered campaign results with customer demographics on customer_id.\",\"Calculate the average age of customers for each successful campaign.\",\"Sort the campaigns by average customer age in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/campaign_results.xlsx\",\"type\":\"xlsx\"},\"output\":\"marketing_campaign_results\",\"comment\":\"Loading marketing campaign results from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"marketing_campaign_results\",\"query\":\"success == True\"},\"output\":\"successful_campaigns\",\"comment\":\"Filtering campaign results to include only successful campaigns.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"successful_campaigns\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining successful campaign results with customer demographics on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"campaign_id\",\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"campaign_age_data\",\"comment\":\"Calculating the average age of customers for each successful campaign.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"campaign_age_data\",\"sortBy\":\"average_age\",\"order\":\"desc\"},\"output\":\"sorted_campaigns\",\"comment\":\"Sorting campaigns by average customer age in descending order.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/sorted_campaigns.csv\",\"type\":\"csv\"},\"output\":\"sorted_campaigns\",\"comment\":\"Saving the final sorted table to a new CSV file.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid comparison expression using less than.","output":"'col1 < col2'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/data/legal/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases\",\"description\":\"A table containing details of various court cases, including case ID, verdict, and associated lawyer ID.\",\"columns\":[{\"column\":\"case_id\",\"column_name\":\"CASE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court case.\"},{\"column\":\"verdict\",\"column_name\":\"VERDICT\",\"column_type\":\"xsd:string\",\"column_description\":\"The verdict of the court case, e.g., 'Guilty' or 'Not Guilty'.\"},{\"column\":\"lawyer_id\",\"column_name\":\"LAWYER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer associated with the case.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/data/legal/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers\",\"description\":\"A table containing information about lawyers, including their ID and name.\",\"columns\":[{\"column\":\"lawyer_id\",\"column_name\":\"LAWYER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"lawyer_name\",\"column_name\":\"LAWYER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the lawyer.\"}]},\"table_id3\":{\"name\":\"legal_firms\",\"location\":\"C:/data/legal/legal_firms.csv\",\"sheet_name\":null,\"label\":\"Legal Firms\",\"description\":\"A table containing information about legal firms, including firm ID and name.\",\"columns\":[{\"column\":\"firm_id\",\"column_name\":\"FIRM_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each legal firm.\"},{\"column\":\"firm_name\",\"column_name\":\"FIRM_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the legal firm.\"}]}},\"instructions\":[\"Load the 'court_cases' table from the CSV file located at C:/data/legal/court_cases.csv.\",\"Filter the 'court_cases' table to include only cases where the verdict is 'Guilty'.\",\"Join the filtered 'court_cases' table with the 'lawyers' table on the 'lawyer_id' column.\",\"Aggregate the joined table to find the total number of 'Guilty' verdicts per lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"VERDICT == 'Guilty'\"},\"output\":\"guilty_cases\",\"comment\":\"Filtering court cases to include only those with a 'Guilty' verdict.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/lawyers.csv\",\"type\":\"csv\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"guilty_cases\",\"lawyers\"],\"joinOn\":[\"LAWYER_ID\"],\"joinType\":\"inner\"},\"output\":\"guilty_cases_with_lawyers\",\"comment\":\"Joining the filtered court cases with the lawyers table on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"guilty_cases_with_lawyers\",\"groupBy\":\"LAWYER_NAME\",\"aggregations\":[{\"column\":\"CASE_ID\",\"name\":\"total_guilty_verdicts\",\"function\":\"count\"}]},\"output\":\"guilty_verdicts_per_lawyer\",\"comment\":\"Aggregating the joined table to find the total number of 'Guilty' verdicts per lawyer.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid math function using logarithm and square root.","output":"'log(col1) + sqrt(col2)'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Inventory\",\"description\":\"Contains information about the raw materials available in the inventory, including material ID, name, and quantity.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"Material ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"material_name\",\"column_name\":\"Material Name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the raw material.\"},{\"column\":\"quantity\",\"column_name\":\"Quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current quantity of the raw material in stock.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"Details the production schedule, including material requirements and timelines.\",\"columns\":[{\"column\":\"schedule_id\",\"column_name\":\"Schedule ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each production schedule entry.\"},{\"column\":\"material_id\",\"column_name\":\"Material ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the raw material required.\"},{\"column\":\"required_quantity\",\"column_name\":\"Required Quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material required for production.\"}]},\"table_id3\":{\"name\":\"supplier_info\",\"location\":\"C:/data/supplier_info.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains information about suppliers, including supplier ID and contact details.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"Supplier ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"supplier_name\",\"column_name\":\"Supplier Name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"},{\"column\":\"contact_info\",\"column_name\":\"Contact Info\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact details for the supplier.\"}]}},\"instructions\":[\"Load the raw materials inventory table from the CSV file.\",\"Load the production schedule from the Excel file.\",\"Filter the raw materials to include only those with a quantity less than 100.\",\"Join the filtered raw materials with the production schedule on the material ID.\",\"Aggregate the joined table to calculate the total quantity needed for each material.\",\"Sort the aggregated data by total quantity in descending order.\",\"Calculate a new column for reorder level by multiplying total quantity by 1.2.\",\"Filter the data to include only materials with reorder level greater than 150.\",\"Join the filtered data with supplier information on supplier ID.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials inventory from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"quantity < 100\"},\"output\":\"low_stock_materials\",\"comment\":\"Filtering raw materials to include only those with a quantity less than 100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_stock_materials\",\"production_schedule\"],\"joinOn\":[\"material_id\"],\"joinType\":\"inner\"},\"output\":\"materials_with_schedule\",\"comment\":\"Joining filtered raw materials with the production schedule on material ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"materials_with_schedule\",\"groupBy\":\"material_id\",\"aggregations\":[{\"column\":\"required_quantity\",\"name\":\"total_quantity\",\"function\":\"sum\"}]},\"output\":\"aggregated_materials\",\"comment\":\"Aggregating the joined table to calculate the total quantity needed for each material.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_materials\",\"sortBy\":\"total_quantity\",\"order\":\"desc\"},\"output\":\"sorted_materials\",\"comment\":\"Sorting the aggregated data by total quantity in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_materials\",\"columnName\":\"reorder_level\",\"formula\":\"total_quantity * 1.2\"},\"output\":\"materials_with_reorder_level\",\"comment\":\"Calculating a new column for reorder level by multiplying total quantity by 1.2.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"materials_with_reorder_level\",\"query\":\"reorder_level > 150\"},\"output\":\"high_reorder_materials\",\"comment\":\"Filtering the data to include only materials with reorder level greater than 150.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_reorder_materials\",\"supplier_info\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"materials_with_suppliers\",\"comment\":\"Joining the filtered data with supplier information on supplier ID.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/hospital_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains detailed records of patients including their diagnoses.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"The diagnosis given to the patient.\"},{\"column\":\"col_3\",\"column_name\":\"admission_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date the patient was admitted to the hospital.\"}]},\"table_id2\":{\"name\":\"medication\",\"location\":\"C:/hospital_data/medication.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Details of medications prescribed to patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"},{\"column\":\"col_3\",\"column_name\":\"dosage\",\"column_type\":\"xsd:string\",\"column_description\":\"Dosage of the medication prescribed.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/hospital_data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Information about hospital staff members.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the staff member works.\"}]}},\"instructions\":[\"Load the patient records table from the hospital database.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Join the filtered patient records with the medication table to get the prescribed medications.\",\"Aggregate the data to find the total number of diabetes patients per medication.\",\"Sort the aggregated data by the total number of patients in descending order.\",\"Calculate the percentage of patients for each medication relative to the total number of diabetes patients.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records table from the hospital database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetes_patients\",\"comment\":\"Filtering the patient records to include only those with a diagnosis of diabetes.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/medication.csv\",\"type\":\"csv\"},\"output\":\"medication\",\"comment\":\"Loading the medication table to get prescribed medications.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"diabetes_patients\",\"medication\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"diabetes_medications\",\"comment\":\"Joining the filtered patient records with the medication table to get the prescribed medications.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"diabetes_medications\",\"groupBy\":\"medication_name\",\"aggregations\":[{\"column\":\"patient_id\",\"name\":\"total_patients\",\"function\":\"count\"}]},\"output\":\"medication_aggregation\",\"comment\":\"Aggregating the data to find the total number of diabetes patients per medication.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"medication_aggregation\",\"sortBy\":\"total_patients\",\"order\":\"desc\"},\"output\":\"sorted_medication_aggregation\",\"comment\":\"Sorting the aggregated data by the total number of patients in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_medication_aggregation\",\"columnName\":\"patient_percentage\",\"formula\":\"total_patients / sum(total_patients) * 100\"},\"output\":\"medication_percentage\",\"comment\":\"Calculating the percentage of patients for each medication relative to the total number of diabetes patients.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"legal_cases\",\"location\":\"C:/data/legal_cases.csv\",\"sheet_name\":null,\"label\":\"Legal Cases\",\"description\":\"A table containing details of legal cases including case ID, lawyer ID, filing date, and duration.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each legal case.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"column_3_id\",\"column_name\":\"filing_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"},{\"column\":\"column_4_id\",\"column_name\":\"duration\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Duration of the case in days.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/data/lawyers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Lawyers Information\",\"description\":\"A table containing information about lawyers including lawyer ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedules\",\"location\":\"C:/data/court_schedules.xlsx\",\"sheet_name\":\"Schedules\",\"label\":\"Court Schedules\",\"description\":\"A table containing court schedules, not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"schedule_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each schedule.\"},{\"column\":\"column_2_id\",\"column_name\":\"court_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court schedule.\"}]}},\"instructions\":[\"Load the 'legal_cases' table from the CSV file located at C:/data/legal_cases.csv.\",\"Load the 'lawyers' table from the Excel file located at C:/data/lawyers.xlsx.\",\"Filter the 'legal_cases' table to include only cases filed after January 1, 2020.\",\"Join the filtered 'legal_cases' table with the 'lawyers' table on the lawyer_id column.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\",\"Sort the aggregated data by the total number of cases in descending order.\",\"Calculate a new column 'average_case_duration' by dividing 'total_duration' by 'total_cases'.\",\"Filter the sorted data to include only lawyers with more than 10 cases.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal_cases.csv\",\"type\":\"csv\"},\"output\":\"legal_cases\",\"comment\":\"Loading the legal cases dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/lawyers.xlsx\",\"type\":\"xlsx\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"legal_cases\",\"query\":\"`filing_date` > '2020-01-01'\"},\"output\":\"recent_cases\",\"comment\":\"Filtering legal cases to include only those filed after January 1, 2020.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"recent_cases\",\"lawyers\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"cases_with_lawyers\",\"comment\":\"Joining recent cases with lawyer information based on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"cases_with_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"},{\"column\":\"duration\",\"name\":\"total_duration\",\"function\":\"sum\"}]},\"output\":\"lawyer_case_summary\",\"comment\":\"Aggregating data to find the total number of cases and total duration handled by each lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"lawyer_case_summary\",\"sortBy\":[\"total_cases\"],\"order\":[\"desc\"]},\"output\":\"sorted_lawyer_summary\",\"comment\":\"Sorting the aggregated data by total number of cases in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_lawyer_summary\",\"columnName\":\"average_case_duration\",\"formula\":\"`total_duration` / `total_cases`\"},\"output\":\"lawyer_summary_with_avg_duration\",\"comment\":\"Calculating the average case duration for each lawyer.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"lawyer_summary_with_avg_duration\",\"query\":\"`total_cases` > 10\"},\"output\":\"top_lawyers\",\"comment\":\"Filtering to include only lawyers with more than 10 cases.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/data/legal/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases Data\",\"description\":\"A dataset containing details of various court cases, including case ID, date, category, and outcome.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"CASE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court case.\"},{\"column\":\"col_2\",\"column_name\":\"DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the court case was filed.\"},{\"column\":\"col_3\",\"column_name\":\"CATEGORY\",\"column_type\":\"xsd:string\",\"column_description\":\"The legal category of the court case.\"},{\"column\":\"col_4\",\"column_name\":\"OUTCOME\",\"column_type\":\"xsd:string\",\"column_description\":\"The outcome of the court case.\"}]},\"table_id2\":{\"name\":\"legal_references\",\"location\":\"C:/data/legal/legal_references.xlsx\",\"sheet_name\":\"References\",\"label\":\"Legal References\",\"description\":\"A dataset containing legal references and their descriptions.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"REFERENCE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each legal reference.\"},{\"column\":\"col_B\",\"column_name\":\"DESCRIPTION\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the legal reference.\"}]}},\"instructions\":[\"Load the table 'court_cases' which contains information about various legal cases.\",\"Filter the 'court_cases' table to include only cases from the year 2022.\",\"Aggregate the filtered data to count the number of cases per legal category.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"`DATE`.dt.year == 2022\"},\"output\":\"court_cases_2022\",\"comment\":\"Filtering court cases to include only those from the year 2022.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"court_cases_2022\",\"groupBy\":\"CATEGORY\",\"aggregations\":[{\"column\":\"CASE_ID\",\"name\":\"case_count\",\"function\":\"count\"}]},\"output\":\"case_count_by_category\",\"comment\":\"Aggregating data to count the number of cases per legal category.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains daily production targets and actual output for the manufacturing plant.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the production record.\"},{\"column\":\"col_2\",\"column_name\":\"target_output\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The target output for the day.\"},{\"column\":\"col_3\",\"column_name\":\"actual_output\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The actual output achieved for the day.\"}]},\"table_id2\":{\"name\":\"machine_maintenance\",\"location\":\"C:/data/manufacturing/machine_maintenance.xlsx\",\"sheet_name\":\"Maintenance\",\"label\":\"Machine Maintenance Records\",\"description\":\"This table contains records of maintenance activities for machines in the plant.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"machine_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for each machine.\"},{\"column\":\"col_B\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when maintenance was performed.\"},{\"column\":\"col_C\",\"column_name\":\"maintenance_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of maintenance performed.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the daily production targets and actual output.\",\"Filter the 'production_schedule' table to include only records where the actual output is less than the target.\",\"Calculate the difference between the target and actual output for each record and store it in a new column called 'output_gap'.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule table containing daily targets and actual output.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"`actual_output` < `target_output`\"},\"output\":\"underperforming_days\",\"comment\":\"Filtering the production schedule to find days where actual output was less than the target.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"underperforming_days\",\"columnName\":\"output_gap\",\"formula\":\"`target_output` - `actual_output`\"},\"output\":\"underperforming_days_with_gap\",\"comment\":\"Calculating the output gap by subtracting actual output from target output for each underperforming day.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and energy usage values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.xlsx\",\"sheet_name\":\"Info\",\"label\":\"Building Information\",\"description\":\"Contains information about buildings such as address, size, and type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"address\",\"column_type\":\"xsd:string\",\"column_description\":\"The address of the building.\"},{\"column\":\"col_3\",\"column_name\":\"size\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The size of the building in square meters.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each building.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data for buildings from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp`.year == 2023\"},\"output\":\"energy_2023\",\"comment\":\"Filtering the energy consumption data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_2023\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy\",\"function\":\"sum\"}]},\"output\":\"total_energy_per_building\",\"comment\":\"Calculating the total energy consumption for each building.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of shipments including shipment ID, weight, and destination.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"Destination of the shipment.\"}]},\"table_id2\":{\"name\":\"transport_costs\",\"location\":\"C:/data/logistics/transport_costs.xlsx\",\"sheet_name\":null,\"label\":\"Transport Costs\",\"description\":\"Contains transport cost details including shipment ID and cost.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Transport cost for the shipment.\"}]},\"table_id3\":{\"name\":\"vehicle_data\",\"location\":\"C:/data/logistics/vehicle_data.csv\",\"sheet_name\":null,\"label\":\"Vehicle Data\",\"description\":\"Contains information about vehicles used in logistics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"vehicle_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each vehicle.\"},{\"column\":\"col_2\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Capacity of the vehicle in kilograms.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at C:/data/logistics/shipment_data.csv.\",\"Filter the 'shipment_data' table to include only shipments with a weight greater than 1000 kg.\",\"Load the 'transport_costs' table from the Excel file located at C:/data/logistics/transport_costs.xlsx.\",\"Join the filtered 'shipment_data' with 'transport_costs' on the 'shipment_id' column using an inner join.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"filtered_shipment_data\",\"comment\":\"Filtering shipments to include only those with weight greater than 1000 kg.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/transport_costs.xlsx\",\"type\":\"xlsx\"},\"output\":\"transport_costs\",\"comment\":\"Loading transport costs from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipment_data\",\"transport_costs\"],\"joinOn\":[\"shipment_id\"],\"joinType\":\"inner\"},\"output\":\"shipment_with_costs\",\"comment\":\"Joining filtered shipment data with transport costs on shipment_id.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid multi-line expression without assignment.","output":"'a = b + 1\\na + 2' // Error: Multi-line expressions without assignment are not allowed."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'applyTransformations' action with an unsupported transformation.","output":"{ \"name\": \"applyTransformations\", \"parameters\": { \"table\": \"sales_data\", \"transformations\": [ { \"columns\": [\"revenue\"], \"operation\": \"unsupported_transformation\" } ] }, \"output\": \"transformed_sales\" } // Error: 'unsupported_transformation' is not a valid transformation operation."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid Boolean expression using 'not'.","output":"'not col1 < 5'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid dictionary comprehension.","output":"'{x: x**2 for x in col1}' // Error: Dictionary comprehensions are not allowed in Tamarind."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"A CSV file containing detailed records of employees including their salaries, department IDs, and other personal information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_details\",\"location\":\"C:/data/department_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Department Details\",\"description\":\"An Excel sheet containing details about each department including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data\",\"description\":\"A dataset that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the employee records from the CSV file located at C:/data/employee_records.csv.\",\"Load the department details from the Excel file located at C:/data/department_details.xlsx.\",\"Filter the employee records to include only those with a salary greater than 50,000.\",\"Join the filtered employee records with the department details on the department_id column.\",\"Calculate a new column 'annual_bonus' as 10% of the salary for each employee.\",\"Aggregate the data to find the total salary and average bonus for each department.\",\"Sort the aggregated data by total salary in descending order.\",\"Apply a standardization transformation to the salary column in the joined table.\",\"Forecast the headcount for each department for the next 12 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading employee records from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_details\",\"comment\":\"Loading department details from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employees\",\"comment\":\"Filtering employees with a salary greater than 50,000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employees\",\"department_details\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee records with department details on department_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"annual_bonus\",\"formula\":\"salary * 0.10\"},\"output\":\"data_with_bonus\",\"comment\":\"Calculating annual bonus as 10% of the salary.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_bonus\",\"groupBy\":\"department_id\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"total_salary\",\"function\":\"sum\"},{\"column\":\"annual_bonus\",\"name\":\"average_bonus\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find total salary and average bonus for each department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_salary\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total salary in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"joined_data\",\"transformations\":[{\"columns\":[\"salary\"],\"operation\":\"standardize\"}]},\"output\":\"standardized_data\",\"comment\":\"Applying standardization transformation to the salary column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"joined_data\",\"columns\":[\"employee_id\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"headcount_forecast\",\"comment\":\"Forecasting headcount for each department for the next 12 months using Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains information about shipments including weight, destination, and warehouse ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse associated with the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_data\",\"location\":\"C:/data/logistics/warehouse_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Warehouse Data\",\"description\":\"Contains information about warehouses including their location and capacity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse.\"},{\"column\":\"col_3\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Maximum capacity of the warehouse in kilograms.\"}]},\"table_id3\":{\"name\":\"driver_data\",\"location\":\"C:/data/logistics/driver_data.csv\",\"sheet_name\":null,\"label\":\"Driver Data\",\"description\":\"Contains information about drivers including their ID and assigned routes.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"driver_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each driver.\"},{\"column\":\"col_2\",\"column_name\":\"route\",\"column_type\":\"xsd:string\",\"column_description\":\"Assigned route for the driver.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file.\",\"Load the 'warehouse_data' table from the Excel file.\",\"Filter the 'shipment_data' to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered 'shipment_data' with 'warehouse_data' on the 'warehouse_id' column.\",\"Aggregate the joined data to calculate the total shipment weight per warehouse.\",\"Sort the aggregated data by total shipment weight in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouse_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"warehouse_data\",\"comment\":\"Loading warehouse data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"filtered_shipment_data\",\"comment\":\"Filtering shipments to include only those with a weight greater than 1000 kg.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipment_data\",\"warehouse_data\"],\"joinOn\":[\"warehouse_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with warehouse data on warehouse_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"warehouse_id\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_shipment_weight\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total shipment weight per warehouse.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_shipment_weight\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total shipment weight in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with details such as account ID, transaction amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"account_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for the account.\"},{\"column\":\"col_2\",\"column_name\":\"transaction_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the transaction.\"}]},\"table_id2\":{\"name\":\"account_details\",\"location\":\"C:/data/account_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Account Details\",\"description\":\"A table containing details about accounts, including account ID and account holder information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"account_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for the account.\"},{\"column\":\"col_2\",\"column_name\":\"account_holder\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the account holder.\"}]}},\"instructions\":[\"Load the table 'financial_transactions' from the CSV file located at C:/data/financial_transactions.csv.\",\"Filter the 'financial_transactions' table to include only transactions where the amount is greater than 1000.\",\"Aggregate the filtered transactions by 'account_id' to calculate the total amount for each account.\",\"Sort the aggregated data by 'total_amount' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"transaction_amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_transactions\",\"groupBy\":\"account_id\",\"aggregations\":[{\"column\":\"transaction_amount\",\"name\":\"total_amount\",\"function\":\"sum\"}]},\"output\":\"aggregated_transactions\",\"comment\":\"Aggregating transactions by account ID to calculate the total amount for each account.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_transactions\",\"sortBy\":\"total_amount\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_transactions\",\"comment\":\"Sorting the aggregated data by total amount in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of products including status, cost, and quantity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Approval status of the product.\"},{\"column\":\"col_3\",\"column_name\":\"total_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total cost of the product.\"},{\"column\":\"col_4\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the product.\"},{\"column\":\"col_5\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]},\"table_id2\":{\"name\":\"supplier_information\",\"location\":\"C:/data/supplier_information.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains information about suppliers including their IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a status of 'approved'.\",\"Join the filtered product specifications with the supplier information table on the supplier ID.\",\"Calculate the cost per unit by dividing the total cost by the quantity for each product.\",\"Aggregate the data to find the total cost per supplier.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"status == 'approved'\"},\"output\":\"approved_products\",\"comment\":\"Filtering the product specifications to include only approved products.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_information.csv\",\"type\":\"csv\"},\"output\":\"supplier_information\",\"comment\":\"Loading the supplier information from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"approved_products\",\"supplier_information\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"products_with_suppliers\",\"comment\":\"Joining approved products with supplier information on supplier ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"products_with_suppliers\",\"columnName\":\"cost_per_unit\",\"formula\":\"total_cost / quantity\"},\"output\":\"products_with_cost_per_unit\",\"comment\":\"Calculating cost per unit by dividing total cost by quantity for each product.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"products_with_cost_per_unit\",\"groupBy\":\"supplier_id\",\"aggregations\":[{\"column\":\"total_cost\",\"name\":\"total_cost_per_supplier\",\"function\":\"sum\"}]},\"output\":\"total_cost_per_supplier\",\"comment\":\"Aggregating data to find the total cost per supplier.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains records of energy consumption with columns for date, time, and energy usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"time\",\"column_type\":\"xsd:time\",\"column_description\":\"The time of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains weather information with columns for date, temperature, and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature recorded on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The humidity level recorded on the given date.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Filter the data to include only records where the energy usage is greater than 100 kWh.\",\"Aggregate the filtered data by date to calculate the total energy usage per day.\",\"Sort the aggregated data by total energy usage in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"energy_usage > 100\"},\"output\":\"filtered_energy\",\"comment\":\"Filtering the data to include only records where energy usage is greater than 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy_usage\",\"function\":\"sum\"}]},\"output\":\"daily_energy_usage\",\"comment\":\"Aggregating the filtered data by date to calculate the total energy usage per day.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"daily_energy_usage\",\"sortBy\":\"total_energy_usage\",\"order\":\"desc\"},\"output\":\"sorted_energy_usage\",\"comment\":\"Sorting the aggregated data by total energy usage in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transaction data including transaction amounts, fees, and other related information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"fees\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The fees associated with the transaction.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customer_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Data\",\"description\":\"A table containing customer information such as names, addresses, and contact details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at 'C:/data/financial_transactions.csv'.\",\"Filter the 'financial_transactions' table to include only transactions where the 'amount' is greater than 1000.\",\"Calculate a new column 'net_amount' in the filtered table by subtracting 'fees' from 'amount'.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"filtered_transactions\",\"columnName\":\"net_amount\",\"formula\":\"amount - fees\"},\"output\":\"transactions_with_net_amount\",\"comment\":\"Calculating net amount by subtracting fees from the transaction amount.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'forecastData' action using 'xgboost'.","output":"{ \"name\": \"forecastData\", \"parameters\": { \"table\": \"sales_data\", \"columns\": [\"revenue\"], \"forecastParameters\": { \"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"date\", \"algorithm\": \"xgboost\", \"confidenceInterval\": 95 }, \"output\": \"forecast_results\" } }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/hr/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees including their ID, name, department, salary, and years with the company.\",\"columns\":[{\"column\":\"emp_id\",\"column_name\":\"EMPLOYEE_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"name\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"department_id\",\"column_name\":\"DEPARTMENT_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Identifier for the department the employee belongs to.\"},{\"column\":\"salary\",\"column_name\":\"SALARY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual salary of the employee.\"},{\"column\":\"years_with_company\",\"column_name\":\"YEARS_WITH_COMPANY\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of years the employee has been with the company.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/hr/department_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Department Data\",\"description\":\"Contains information about departments including department ID and name.\",\"columns\":[{\"column\":\"dept_id\",\"column_name\":\"DEPARTMENT_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"dept_name\",\"column_name\":\"DEPARTMENT_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"not_relevant_table\",\"location\":\"C:/data/hr/irrelevant_data.xlsx\",\"sheet_name\":\"Irrelevant\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_x\",\"column_name\":\"COLUMN_X\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant column.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/hr/employee_data.csv.\",\"Load the department data from the Excel file located at C:/data/hr/department_data.xlsx.\",\"Filter the employee data to include only those with a salary greater than 50,000.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Calculate a new column 'annual_bonus' as 10% of the salary for each employee.\",\"Aggregate the joined data to find the total salary and average bonus by department.\",\"Sort the aggregated data by total salary in descending order.\",\"Identify employees who have been with the company for more than 5 years.\",\"Apply a transformation to standardize the salary column.\",\"Forecast the headcount for each department for the next 12 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading employee data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/department_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_data\",\"comment\":\"Loading department data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"`SALARY` > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employees with a salary greater than 50,000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_salary_employees\",\"department_data\"],\"joinOn\":[\"DEPARTMENT_ID\"],\"joinType\":\"inner\"},\"output\":\"employee_department_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"employee_department_data\",\"columnName\":\"annual_bonus\",\"formula\":\"`SALARY` * 0.10\"},\"output\":\"employee_data_with_bonus\",\"comment\":\"Calculating annual bonus as 10% of the salary.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"employee_data_with_bonus\",\"groupBy\":\"DEPARTMENT_ID\",\"aggregations\":[{\"column\":\"SALARY\",\"name\":\"total_salary\",\"function\":\"sum\"},{\"column\":\"annual_bonus\",\"name\":\"average_bonus\",\"function\":\"mean\"}]},\"output\":\"aggregated_department_data\",\"comment\":\"Aggregating data to find total salary and average bonus by department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_department_data\",\"sortBy\":\"total_salary\",\"order\":\"desc\"},\"output\":\"sorted_department_data\",\"comment\":\"Sorting departments by total salary in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"`YEARS_WITH_COMPANY` > 5\"},\"output\":\"long_term_employees\",\"comment\":\"Identifying employees who have been with the company for more than 5 years.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"employee_data\",\"transformations\":[{\"columns\":[\"SALARY\"],\"operation\":\"standardize\"}]},\"output\":\"standardized_employee_data\",\"comment\":\"Standardizing the salary column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"employee_data\",\"columns\":[\"EMPLOYEE_ID\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"EMPLOYEE_ID\"}},\"output\":\"headcount_forecast\",\"comment\":\"Forecasting headcount for each department for the next 12 months using Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/hospital_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains patient information including ID, name, age, and diagnosis.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_4\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"medication\",\"location\":\"C:/hospital_data/medication.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Contains medication information including patient ID, medication name, and dosage.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"},{\"column\":\"col_3\",\"column_name\":\"dosage\",\"column_type\":\"xsd:string\",\"column_description\":\"Dosage of the medication prescribed.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/hospital_data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Contains information about hospital staff including ID, name, and role.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"role\",\"column_type\":\"xsd:string\",\"column_description\":\"Role of the staff member in the hospital.\"}]}},\"instructions\":[\"Load the patient records table from the hospital database.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Join the filtered patient records with the medication table on patient ID.\",\"Aggregate the joined data to find the total number of medications prescribed per patient.\",\"Sort the aggregated data by the total number of medications in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetes_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with diabetes.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/medication.csv\",\"type\":\"csv\"},\"output\":\"medication\",\"comment\":\"Loading the medication records dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"diabetes_patients\",\"medication\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient records with medication data on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"medication_name\",\"name\":\"total_medications\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find the total number of medications prescribed per patient.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_medications\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by the total number of medications in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/finance/transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"This table contains all financial transactions including date, amount, and account type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"account_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of account associated with the transaction.\"}]},\"table_id2\":{\"name\":\"customer_info\",\"location\":\"C:/data/finance/customers.csv\",\"sheet_name\":null,\"label\":\"Customer Information\",\"description\":\"Contains customer details such as name, address, and account number.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"address\",\"column_type\":\"xsd:string\",\"column_description\":\"The address of the customer.\"}]}},\"instructions\":[\"Load the financial transactions table from the CSV file.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Aggregate the filtered transactions by account type to calculate the total amount.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"`amount` > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_transactions\",\"groupBy\":\"account_type\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_amount\",\"function\":\"sum\"}]},\"output\":\"aggregated_transactions\",\"comment\":\"Aggregating filtered transactions by account type to calculate the total amount.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/legal_data/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases\",\"description\":\"A table containing information about various legal cases, including case ID, lawyer ID, and case date.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"column_3_id\",\"column_name\":\"case_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/legal_data/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers\",\"description\":\"A table containing details about lawyers, including lawyer ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_judges\",\"location\":\"C:/legal_data/court_judges.csv\",\"sheet_name\":null,\"label\":\"Court Judges\",\"description\":\"A table containing information about judges, which is not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"judge_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each judge.\"},{\"column\":\"column_2_id\",\"column_name\":\"judge_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the judge.\"}]}},\"instructions\":[\"Load the table 'court_cases' which contains information about various legal cases.\",\"Load the table 'lawyers' which contains details about lawyers involved in the cases.\",\"Filter the 'court_cases' table to include only cases from the year 2023.\",\"Join the filtered 'court_cases' table with the 'lawyers' table on the lawyer_id.\",\"Aggregate the joined table to count the number of cases each lawyer handled in 2023.\",\"Sort the aggregated data by the number of cases in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyers.csv\",\"type\":\"csv\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"`case_date`.year == 2023\"},\"output\":\"court_cases_2023\",\"comment\":\"Filtering the court cases to include only those from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"court_cases_2023\",\"lawyers\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"cases_with_lawyers\",\"comment\":\"Joining the filtered court cases with the lawyers table on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"cases_with_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"case_count\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating the data to count the number of cases each lawyer handled in 2023.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"lawyer_case_counts\",\"sortBy\":[\"case_count\"],\"order\":[\"desc\"]},\"output\":\"sorted_lawyer_case_counts\",\"comment\":\"Sorting the aggregated data by the number of cases in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat operations are supported in 'applyTimeSeriesTransformations'?","output":"Supported operations include: 'set_index', 'reset_index', 'resample', 'rolling_mean', 'rolling_median', 'rolling_std', 'rolling_min', 'rolling_max', 'expanding_sum', 'expanding_mean', 'expanding_std', 'expanding_min', 'expanding_max', 'lag', 'diff', 'pct_change', 'cumsum', 'cumprod', 'ewm_mean', 'ewm_std', 'ewm_var', 'decompose', 'detect_outliers', 'convert_timezone', 'extract_datetime_features', 'fourier_transform', 'inverse_fourier_transform', 'detrend', and 'seasonal_adjustment'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of shipments including weight, destination, and warehouse ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse associated with the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_data\",\"location\":\"C:/data/logistics/warehouse_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Warehouse Data\",\"description\":\"Contains information about warehouses including their ID and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse.\"}]},\"table_id3\":{\"name\":\"transport_data\",\"location\":\"C:/data/logistics/transport_data.csv\",\"sheet_name\":null,\"label\":\"Transport Data\",\"description\":\"Contains information about transportation modes and schedules.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transport_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transport mode.\"},{\"column\":\"col_2\",\"column_name\":\"schedule\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Scheduled time for the transport.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at 'C:/data/logistics/shipment_data.csv'.\",\"Load the 'warehouse_data' table from the Excel file located at 'C:/data/logistics/warehouse_data.xlsx'.\",\"Filter the 'shipment_data' table to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered 'shipment_data' with 'warehouse_data' on the 'warehouse_id' column using an inner join.\",\"Aggregate the joined table to calculate the total shipment weight per warehouse.\",\"Sort the aggregated data by total shipment weight in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouse_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"warehouse_data\",\"comment\":\"Loading warehouse data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"filtered_shipment_data\",\"comment\":\"Filtering shipments to include only those with weight greater than 1000 kg.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipment_data\",\"warehouse_data\"],\"joinOn\":[\"warehouse_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with warehouse data on warehouse_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"warehouse_id\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_shipment_weight\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total shipment weight per warehouse.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_shipment_weight\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total shipment weight in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains daily energy consumption data with timestamps and consumption values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains daily weather data including temperature and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature of the day in Celsius.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity of the day in percentage.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains unrelated data not used in the workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"unrelated_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An unrelated column.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Aggregate the data to calculate the total energy consumption per month.\",\"Join the aggregated data with the weather data to analyze the impact of weather on energy consumption.\",\"Calculate a new column for the average daily energy consumption per month.\",\"Sort the data by total energy consumption in descending order.\",\"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"filtered_energy_2023\",\"comment\":\"Filtering the energy consumption data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_2023\",\"groupBy\":\"timestamp.dt.month\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"total_monthly_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Aggregating the data to calculate the total energy consumption per month.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.csv\",\"type\":\"csv\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"monthly_energy_consumption\",\"weather_data\"],\"joinOn\":[\"timestamp\",\"date\"],\"joinType\":\"inner\"},\"output\":\"energy_weather_analysis\",\"comment\":\"Joining the aggregated energy data with weather data to analyze the impact of weather on energy consumption.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"energy_weather_analysis\",\"columnName\":\"average_daily_consumption\",\"formula\":\"total_monthly_consumption / 30\"},\"output\":\"energy_with_avg_daily\",\"comment\":\"Calculating a new column for the average daily energy consumption per month.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"energy_with_avg_daily\",\"sortBy\":\"total_monthly_consumption\",\"order\":\"desc\"},\"output\":\"sorted_energy_data\",\"comment\":\"Sorting the data by total energy consumption in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sorted_energy_data\",\"columns\":[\"total_monthly_consumption\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"timestamp\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"confidenceInterval\":95}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/finance/transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"This table contains detailed records of financial transactions including transaction ID, customer ID, amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The monetary amount of the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"customer_info\",\"location\":\"C:/data/finance/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Information\",\"description\":\"This table contains customer details such as customer ID, name, and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"customer_region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region where the customer is located.\"}]},\"table_id3\":{\"name\":\"irrelevant_financial_data\",\"location\":\"C:/data/finance/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Financial Data\",\"description\":\"This table contains financial data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_id\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant identifier.\"},{\"column\":\"col_B\",\"column_name\":\"irrelevant_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"An irrelevant financial value.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at 'C:/data/finance/transactions.csv'.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Load the 'customer_info' table from the Excel file located at 'C:/data/finance/customers.xlsx'.\",\"Join the filtered transactions table with the 'customer_info' table on the 'customer_id' column using an inner join.\",\"Aggregate the joined table by 'customer_region' to calculate the total transaction amount for each region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_info\",\"comment\":\"Loading the customer information dataset from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_info\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer information on customer_id using an inner join.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_region\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"}]},\"output\":\"regional_transaction_totals\",\"comment\":\"Aggregating the joined data by customer region to calculate total transaction amounts.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the 'one_class_svm' algorithm used for?","output":"Use 'one_class_svm' for anomaly detection using support vector machines. It requires 'nu', which must be between 0 and 1."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_usage\",\"location\":\"C:/data/energy_usage.csv\",\"sheet_name\":null,\"label\":\"Daily Energy Usage\",\"description\":\"This table contains daily energy consumption data including total usage and solar generation.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy usage record.\"},{\"column\":\"col_2\",\"column_name\":\"total_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total energy usage in kilowatt-hours.\"},{\"column\":\"col_3\",\"column_name\":\"solar_generation\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Energy generated from solar panels in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Daily Weather Data\",\"description\":\"This table contains daily weather information including temperature and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Average daily temperature in degrees Celsius.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Average daily humidity percentage.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data not relevant to the energy management workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"random_column\",\"column_type\":\"xsd:string\",\"column_description\":\"A random string column.\"}]}},\"instructions\":[\"Load the table 'energy_usage' which contains daily energy consumption data.\",\"Load the table 'weather_data' which contains daily weather information.\",\"Filter the 'energy_usage' table to include only data from the year 2023.\",\"Join the 'energy_usage' table with the 'weather_data' table on the 'date' column.\",\"Calculate a new column 'adjusted_usage' in the joined table by subtracting 'solar_generation' from 'total_usage'.\",\"Aggregate the joined table by 'month' to calculate the total 'adjusted_usage' and average 'temperature'.\",\"Sort the aggregated data by 'total_adjusted_usage' in descending order.\",\"Apply a rolling mean transformation with a window of 7 days to the 'adjusted_usage' column.\",\"Forecast the 'adjusted_usage' for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_usage.csv\",\"type\":\"csv\"},\"output\":\"energy_usage\",\"comment\":\"Loading the daily energy usage data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.csv\",\"type\":\"csv\"},\"output\":\"weather_data\",\"comment\":\"Loading the daily weather data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_usage\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"filtered_energy_usage\",\"comment\":\"Filtering energy usage data to include only records from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_usage\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining energy usage data with weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"adjusted_usage\",\"formula\":\"`total_usage` - `solar_generation`\"},\"output\":\"data_with_adjusted_usage\",\"comment\":\"Calculating adjusted energy usage by subtracting solar generation from total usage.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_adjusted_usage\",\"groupBy\":\"date.dt.to_period('M')\",\"aggregations\":[{\"column\":\"adjusted_usage\",\"name\":\"total_adjusted_usage\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating data by month to calculate total adjusted usage and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_adjusted_usage\",\"order\":\"desc\"},\"output\":\"sorted_monthly_data\",\"comment\":\"Sorting the aggregated data by total adjusted usage in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"data_with_adjusted_usage\",\"transformations\":[{\"columns\":[\"adjusted_usage\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":7}}]},\"output\":\"smoothed_data\",\"comment\":\"Applying a rolling mean transformation with a 7-day window to the adjusted usage column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"smoothed_data\",\"columns\":[\"adjusted_usage\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"forecasted_data\",\"comment\":\"Forecasting adjusted energy usage for the next 30 days using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains demographic information and medical history of patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"medical_history\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical history of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_visits\",\"location\":\"C:/data/hospital_visits.xlsx\",\"sheet_name\":\"Visits\",\"label\":\"Hospital Visits\",\"description\":\"Records of hospital visits including patient ID, hospital ID, and length of stay.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"visit_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital visit.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking to the patient records.\"},{\"column\":\"col_3\",\"column_name\":\"hospital_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the hospital.\"},{\"column\":\"col_4\",\"column_name\":\"length_of_stay\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of days the patient stayed in the hospital.\"}]},\"table_id3\":{\"name\":\"insurance_claims\",\"location\":\"C:/data/insurance_claims.xlsx\",\"sheet_name\":\"Claims\",\"label\":\"Insurance Claims\",\"description\":\"Contains insurance claim information, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"claim_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each insurance claim.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking to the patient records.\"},{\"column\":\"col_3\",\"column_name\":\"claim_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount claimed by the patient.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file.\",\"Load the hospital visits table from the Excel file.\",\"Filter the patient records to include only those with age above 60.\",\"Join the filtered patient records with hospital visits on patient ID.\",\"Calculate the average length of stay for each patient.\",\"Aggregate the data to find the total number of visits per hospital.\",\"Sort the aggregated data by the total number of visits in descending order.\",\"Identify patients with more than 3 hospital visits.\",\"Calculate a risk score for each patient based on age and number of visits.\",\"Filter the data to include only high-risk patients with a risk score above 7.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hospital_visits.xlsx\",\"type\":\"xlsx\"},\"output\":\"hospital_visits\",\"comment\":\"Loading the hospital visits dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"age > 60\"},\"output\":\"filtered_patients\",\"comment\":\"Filtering patient records to include only those aged above 60.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_patients\",\"hospital_visits\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"patient_visits\",\"comment\":\"Joining filtered patient records with hospital visits on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"patient_visits\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"length_of_stay\",\"name\":\"average_stay\",\"function\":\"mean\"}]},\"output\":\"patient_visits_with_avg_stay\",\"comment\":\"Calculating the average length of stay for each patient.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"patient_visits_with_avg_stay\",\"groupBy\":\"hospital_id\",\"aggregations\":[{\"column\":\"visit_id\",\"name\":\"total_visits\",\"function\":\"count\"}]},\"output\":\"hospital_visit_counts\",\"comment\":\"Aggregating data to find the total number of visits per hospital.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"hospital_visit_counts\",\"sortBy\":\"total_visits\",\"order\":\"desc\"},\"output\":\"sorted_hospital_visits\",\"comment\":\"Sorting the aggregated data by the total number of visits in descending order.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"patient_visits_with_avg_stay\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"visit_id\",\"name\":\"visit_count\",\"function\":\"count\"}]},\"output\":\"patient_visit_counts\",\"comment\":\"Counting the number of visits for each patient.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_visit_counts\",\"query\":\"visit_count > 3\"},\"output\":\"frequent_patients\",\"comment\":\"Identifying patients with more than 3 hospital visits.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"frequent_patients\",\"columnName\":\"risk_score\",\"formula\":\"age * 0.1 + visit_count * 0.5\"},\"output\":\"patients_with_risk_score\",\"comment\":\"Calculating a risk score for each patient based on age and number of visits.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patients_with_risk_score\",\"query\":\"risk_score > 7\"},\"output\":\"high_risk_patients\",\"comment\":\"Filtering the data to include only high-risk patients with a risk score above 7.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_data.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"This table contains historical sales data with columns for date and sales amount.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the sales record.\"},{\"column\":\"col_2\",\"column_name\":\"sales\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The sales amount for the given date.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customer_data.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"Contains customer information including customer ID and demographics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\",\"Apply a time series transformation to set the 'date' column as the index.\",\"Forecast the 'sales' column for the next 30 days using the Holt-Winters algorithm with a seasonal period of 7 days.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_data.csv\",\"type\":\"csv\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data from a CSV file.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sales_data\",\"transformations\":[{\"columns\":[\"date\"],\"operation\":\"set_index\"}]},\"output\":\"sales_data_indexed\",\"comment\":\"Setting the 'date' column as the index for time series analysis.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sales_data_indexed\",\"columns\":[\"sales\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"sales_forecast\",\"comment\":\"Forecasting the 'sales' column for the next 30 days using Holt-Winters algorithm.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data with columns for timestamp, usage, and other relevant metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location where the energy consumption was recorded.\"}]},\"table_id2\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy_prices.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains energy pricing information with columns for date, price per kWh, and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date for the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price_per_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kWh.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region for which the energy price is applicable.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Filter the data to include only records where the energy usage is greater than 100 kWh.\",\"Aggregate the filtered data to calculate the total energy usage per day.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"usage > 100\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the data to include only records where the energy usage is greater than 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_consumption\",\"groupBy\":\"timestamp\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_daily_usage\",\"function\":\"sum\"}]},\"output\":\"daily_energy_usage\",\"comment\":\"Aggregating the filtered data to calculate the total energy usage per day.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat hyperbolic functions are supported in Tamarind expressions?","output":"Supported hyperbolic functions include 'sinh', 'cosh', 'tanh', 'arcsinh', 'arccosh', and 'arctanh'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'calculateDerivedColumn' action with a syntax error in the formula.","output":"{ \"name\": \"calculateDerivedColumn\", \"parameters\": { \"table\": \"sales\", \"columnName\": \"profit_margin\", \"formula\": \"(revenue - cost / revenue\" }, \"output\": \"sales_with_margin\" } // Error: Missing closing parenthesis in formula."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow does 'sortData' handle null values?","output":"Null values are sorted based on the underlying sorting algorithm."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/HR_data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"A table containing detailed records of all employees, including their hire dates and salaries.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"hire_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date the employee was hired.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The current salary of the employee.\"}]},\"table_id2\":{\"name\":\"department_info\",\"location\":\"C:/HR_data/department_info.csv\",\"sheet_name\":null,\"label\":\"Department Information\",\"description\":\"A table containing information about each department within the company.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the department.\"}]}},\"instructions\":[\"Load the employee records table from the HR database.\",\"Filter the employee records to include only those who have been with the company for more than 5 years.\",\"Calculate the average salary of these long-term employees.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_data/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records from the HR database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"(`hire_date` <= '2018-10-01')\"},\"output\":\"long_term_employees\",\"comment\":\"Filtering employees who have been with the company for more than 5 years.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"long_term_employees\",\"groupBy\":null,\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"average_salary_long_term\",\"comment\":\"Calculating the average salary of long-term employees.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption records with columns for date, consumption in kWh, and other related metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The energy consumption in kilowatt-hours (kWh).\"},{\"column\":\"col_3\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location where the energy consumption was recorded.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data with columns for date, temperature, humidity, and other weather-related metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature recorded on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity recorded on the given date.\"}]},\"table_id3\":{\"name\":\"historical_energy_prices\",\"location\":\"C:/data/historical_energy_prices.csv\",\"sheet_name\":null,\"label\":\"Historical Energy Prices\",\"description\":\"This table contains historical energy prices with columns for date, price per kWh, and other related metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price_per_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kilowatt-hour on the given date.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Filter the data to include only records where the consumption is greater than 100 kWh.\",\"Load the weather data from the Excel file located at C:/data/weather_data.xlsx.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Aggregate the joined data by date to calculate the total energy consumption and average temperature.\",\"Sort the aggregated data by total energy consumption in descending order.\",\"Forecast future energy consumption using the Holt-Winters method with a seasonal period of 7 days.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"consumption > 100\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records with consumption greater than 100 kWh.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered energy consumption data with the weather data on the date column.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"total_consumption\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined data by date to calculate total energy consumption and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_consumption\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total energy consumption in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sorted_data\",\"columns\":[\"total_consumption\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7}},\"output\":\"forecast_results\",\"comment\":\"Forecasting future energy consumption using the Holt-Winters method with a seasonal period of 7 days.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid loop statement.","output":"'for i in range(10): i + 1' // Error: Loops are not allowed in Tamarind expressions."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if an action references a dataset that does not exist?","output":"If an action references a dataset that does not exist, an error is raised."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"legal_cases\",\"location\":\"C:/data/legal_cases.csv\",\"sheet_name\":null,\"label\":\"Legal Cases\",\"description\":\"A table containing information about legal cases, including case ID, lawyer ID, and case year.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each legal case.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"column_3_id\",\"column_name\":\"case_year\",\"column_type\":\"xsd:integer\",\"column_description\":\"The year the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyer_info\",\"location\":\"C:/data/lawyer_info.csv\",\"sheet_name\":null,\"label\":\"Lawyer Information\",\"description\":\"A table containing information about lawyers, including lawyer ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedule\",\"location\":\"C:/data/court_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Court Schedule\",\"description\":\"A table containing the schedule of court hearings.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"hearing_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court hearing.\"},{\"column\":\"column_2_id\",\"column_name\":\"hearing_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court hearing.\"}]}},\"instructions\":[\"Load the legal cases table from the CSV file.\",\"Filter the cases to include only those from the year 2023.\",\"Join the filtered cases with the lawyer information table on lawyer_id.\",\"Aggregate the joined data to count the number of cases per lawyer.\",\"Sort the aggregated data by the number of cases in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal_cases.csv\",\"type\":\"csv\"},\"output\":\"legal_cases\",\"comment\":\"Loading the legal cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"legal_cases\",\"query\":\"case_year == 2023\"},\"output\":\"cases_2023\",\"comment\":\"Filtering cases to include only those from the year 2023.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/lawyer_info.csv\",\"type\":\"csv\"},\"output\":\"lawyer_info\",\"comment\":\"Loading the lawyer information dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"cases_2023\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_cases_lawyers\",\"comment\":\"Joining filtered cases with lawyer information on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_cases_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"case_count\",\"function\":\"count\"}]},\"output\":\"cases_per_lawyer\",\"comment\":\"Aggregating data to count the number of cases per lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"cases_per_lawyer\",\"sortBy\":\"case_count\",\"order\":\"desc\"},\"output\":\"sorted_cases_per_lawyer\",\"comment\":\"Sorting the aggregated data by the number of cases in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/legal_data/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases\",\"description\":\"A table containing details of court cases including case ID, lawyer ID, and case date.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court case.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"column_3_id\",\"column_name\":\"case_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the court case was filed.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/legal_data/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers\",\"description\":\"A table containing details of lawyers including lawyer ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"legal_fees\",\"location\":\"C:/legal_data/legal_fees.csv\",\"sheet_name\":null,\"label\":\"Legal Fees\",\"description\":\"A table containing details of legal fees charged for each case.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court case.\"},{\"column\":\"column_2_id\",\"column_name\":\"fee_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of legal fees charged for the case.\"}]}},\"instructions\":[\"Load the 'court_cases' table from the legal database.\",\"Load the 'lawyers' table from the legal database.\",\"Filter the 'court_cases' table to include only cases from the year 2023.\",\"Join the filtered 'court_cases' table with the 'lawyers' table on the lawyer_id column.\",\"Aggregate the joined table to count the number of cases handled by each lawyer.\",\"Sort the aggregated data by the number of cases in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyers.csv\",\"type\":\"csv\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"`case_date`.year == 2023\"},\"output\":\"filtered_cases_2023\",\"comment\":\"Filtering court cases to include only those from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_cases_2023\",\"lawyers\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"cases_with_lawyers\",\"comment\":\"Joining filtered court cases with lawyers data on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"cases_with_lawyers\",\"groupBy\":\"lawyer_name\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"case_count\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating data to count the number of cases handled by each lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"lawyer_case_counts\",\"sortBy\":\"case_count\",\"order\":\"desc\"},\"output\":\"sorted_lawyer_case_counts\",\"comment\":\"Sorting the aggregated data by the number of cases in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains energy consumption data for the building, including total energy used and number of occupants.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"total_energy\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total energy consumed in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"occupants\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of occupants in the building.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature and humidity for the year 2023.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Average temperature in Celsius.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Average humidity percentage.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.csv\",\"sheet_name\":null,\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for the building, including dates and tasks.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"task_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the maintenance task.\"},{\"column\":\"col_2\",\"column_name\":\"task_description\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the maintenance task.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from the year 2023.\",\"Join the energy consumption data with the weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing total energy consumption by the number of occupants.\",\"Aggregate the data by month to calculate the average energy efficiency.\",\"Sort the aggregated data by average energy efficiency in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date`.dt.year == 2023\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the energy consumption data with the weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`total_energy` / `occupants`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating a new column for energy efficiency by dividing total energy consumption by the number of occupants.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date.dt.to_period('M')\",\"aggregations\":[{\"column\":\"energy_efficiency\",\"name\":\"average_energy_efficiency\",\"function\":\"mean\"}]},\"output\":\"monthly_efficiency\",\"comment\":\"Aggregating the data by month to calculate the average energy efficiency.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_efficiency\",\"sortBy\":\"average_energy_efficiency\",\"order\":\"desc\"},\"output\":\"sorted_efficiency\",\"comment\":\"Sorting the aggregated data by average energy efficiency in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/HR_data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"A table containing detailed records of employees including their salary, hire date, and other personal information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The annual salary of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"hire_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the employee was hired.\"}]},\"table_id2\":{\"name\":\"department_info\",\"location\":\"C:/HR_data/department_info.csv\",\"sheet_name\":null,\"label\":\"Department Information\",\"description\":\"A table containing information about various departments within the company.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the department.\"}]}},\"instructions\":[\"Load the employee records table from the HR database.\",\"Filter the employee records to include only those with a salary greater than $50,000.\",\"Sort the filtered employee records by their hire date in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_data/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employee records to include only those with a salary greater than $50,000.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"high_salary_employees\",\"sortBy\":\"hire_date\",\"order\":\"asc\"},\"output\":\"sorted_high_salary_employees\",\"comment\":\"Sorting the filtered employee records by their hire date in ascending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees including their salary, department, and other personal details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Department Data\",\"description\":\"Contains information about departments including department names and IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"project_data\",\"location\":\"C:/data/project_data.xlsx\",\"sheet_name\":\"Projects\",\"label\":\"Project Data\",\"description\":\"Contains information about projects and their details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"project_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the project.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/employee_data.csv.\",\"Load the department data from the Excel file located at C:/data/department_data.xlsx.\",\"Filter the employee data to include only employees with a salary greater than 50,000.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Calculate a new column in the joined table for annual bonus as 10% of the salary.\",\"Aggregate the joined table to find the total salary and total bonus for each department.\",\"Sort the aggregated data by total salary in descending order.\",\"Apply a transformation to normalize the total bonus column.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading employee data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_data\",\"comment\":\"Loading department data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employees with a salary greater than 50,000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employee_data\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"annual_bonus\",\"formula\":\"salary * 0.10\"},\"output\":\"joined_data_with_bonus\",\"comment\":\"Calculating annual bonus as 10% of the salary.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_bonus\",\"groupBy\":\"department_id\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"total_salary\",\"function\":\"sum\"},{\"column\":\"annual_bonus\",\"name\":\"total_bonus\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find total salary and total bonus for each department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_salary\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total salary in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_data\",\"transformations\":[{\"columns\":[\"total_bonus\"],\"operation\":\"normalize\"}]},\"output\":\"normalized_data\",\"comment\":\"Normalizing the total bonus column.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"building_energy_data\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for various buildings, including usage in kWh and building type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"building_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of building, e.g., residential, commercial.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Energy consumption in kilowatt-hours.\"},{\"column\":\"col_4\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Timestamp of the energy consumption record.\"}]},\"table_id2\":{\"name\":\"building_maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Building Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for various buildings, including dates and types of maintenance activities.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_B\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Scheduled date for maintenance.\"},{\"column\":\"col_C\",\"column_name\":\"maintenance_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of maintenance activity.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only entries where energy usage exceeds 1000 kWh.\",\"Aggregate the filtered data by building type to calculate the total energy consumption.\",\"Sort the aggregated data by total energy consumption in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"building_energy_data\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"building_energy_data\",\"query\":\"`energy_usage_kwh` > 1000\"},\"output\":\"high_energy_usage\",\"comment\":\"Filtering the data to include only entries where energy usage exceeds 1000 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"high_energy_usage\",\"groupBy\":\"building_type\",\"aggregations\":[{\"column\":\"energy_usage_kwh\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"aggregated_energy_data\",\"comment\":\"Aggregating the filtered data by building type to calculate the total energy consumption.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_energy_data\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_energy_data\",\"comment\":\"Sorting the aggregated data by total energy consumption in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"This table contains details of shipments including shipment ID, origin, destination, and weight.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"origin\",\"column_type\":\"xsd:string\",\"column_description\":\"The origin location of the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"The destination location of the shipment.\"},{\"column\":\"col_4\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The weight of the shipment in kilograms.\"}]},\"table_id2\":{\"name\":\"vehicle_data\",\"location\":\"C:/data/logistics/vehicle_data.csv\",\"sheet_name\":null,\"label\":\"Vehicle Data\",\"description\":\"This table contains details of vehicles used for shipments including vehicle ID, type, and capacity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"vehicle_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each vehicle.\"},{\"column\":\"col_2\",\"column_name\":\"vehicle_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of vehicle.\"},{\"column\":\"col_3\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The capacity of the vehicle in kilograms.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of shipments including shipment ID, origin, destination, and weight.\",\"Filter the 'shipment_data' to include only shipments with a weight greater than 1000 kg.\",\"Sort the filtered shipment data by destination in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data containing details of shipments.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"heavy_shipments\",\"comment\":\"Filtering shipments to include only those with a weight greater than 1000 kg.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"heavy_shipments\",\"sortBy\":\"destination\",\"order\":\"asc\"},\"output\":\"sorted_heavy_shipments\",\"comment\":\"Sorting the filtered shipment data by destination in ascending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan parentheses be used in expressions?","output":"Yes, parentheses can be used to enforce precedence in expressions, such as '(a + b) * c'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if timestamps in the dataset are not uniformly spaced?","output":"If timestamps are not uniformly spaced, resampling is applied before forecasting to ensure a consistent time interval."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid comparison expression using greater than or equal to.","output":"'col1 >= col2'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid expression using if-else.","output":"'col1 if col2 > 10 else col3' // Error: If expressions are not allowed in Tamarind."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat statistical functions are supported in Tamarind expressions?","output":"Supported statistical functions include 'sum', 'mean', 'std', 'var', 'median', 'cumsum', 'cumprod', and 'diff'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains the energy consumption data for the building, including total energy usage and number of occupants.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"total_energy\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total energy consumed on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"occupants\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of occupants in the building on the given date.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"2023_weather\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature and humidity for the year 2023.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity on the given date.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.csv\",\"sheet_name\":null,\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for various building systems.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"system\",\"column_type\":\"xsd:string\",\"column_description\":\"The building system scheduled for maintenance.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel sheet.\",\"Filter the energy consumption data to include only records from 2023.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing total energy by the number of occupants.\",\"Aggregate the joined data to find the average energy efficiency per month.\",\"Sort the aggregated data by energy efficiency in descending order.\",\"Apply a rolling mean transformation to the energy efficiency column with a window of 3 months.\",\"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel sheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records from 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered energy consumption data with the weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`total_energy` / `occupants`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating a new column for energy efficiency by dividing total energy by the number of occupants.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"energy_efficiency\",\"name\":\"avg_energy_efficiency\",\"function\":\"mean\"}]},\"output\":\"monthly_efficiency\",\"comment\":\"Aggregating the joined data to find the average energy efficiency per month.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_efficiency\",\"sortBy\":\"avg_energy_efficiency\",\"order\":\"desc\"},\"output\":\"sorted_efficiency\",\"comment\":\"Sorting the aggregated data by energy efficiency in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_efficiency\",\"transformations\":[{\"columns\":[\"avg_energy_efficiency\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":3}}]},\"output\":\"smoothed_efficiency\",\"comment\":\"Applying a rolling mean transformation to the energy efficiency column with a window of 3 months.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"smoothed_efficiency\",\"columns\":[\"avg_energy_efficiency\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"date\"}},\"output\":\"forecast_results\",\"comment\":\"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of each product including product ID, name, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]},\"table_id2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including product ID, demand, and date of research.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"average_demand\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Average demand for the product.\"},{\"column\":\"col_3\",\"column_name\":\"research_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the market research was conducted.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel sheet.\",\"Filter the market research data to include only data from the last year.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Calculate the potential market size by multiplying the average demand by the number of products.\",\"Sort the resulting data by potential market size in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading market research data from an Excel sheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"`research_date` >= '2022-01-01'\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only data from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_market_size\",\"formula\":\"`average_demand` * 1000\"},\"output\":\"data_with_market_size\",\"comment\":\"Calculating potential market size by multiplying average demand by the number of products.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"data_with_market_size\",\"sortBy\":\"potential_market_size\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the data by potential market size in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transaction records including amounts, fees, and transaction dates.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"column_2_id\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"column_3_id\",\"column_name\":\"fees\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The fees deducted from the transaction amount.\"},{\"column\":\"column_4_id\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customer_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Data\",\"description\":\"A table containing customer information including names and contact details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"}]}},\"instructions\":[\"Load the table 'financial_transactions' from the CSV file located at C:/data/financial_transactions.csv.\",\"Filter the financial transactions to include only those with an amount greater than 1000.\",\"Calculate a new column 'net_amount' by subtracting 'fees' from 'amount' in the filtered transactions.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"filtered_transactions\",\"columnName\":\"net_amount\",\"formula\":\"amount - fees\"},\"output\":\"transactions_with_net_amount\",\"comment\":\"Calculating net amount by subtracting fees from the transaction amount.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'sortData' action.","output":"{ \"name\": \"sortData\", \"parameters\": { \"table\": \"sales\", \"sortBy\": \"revenue\", \"order\": \"desc\" }, \"output\": \"sorted_sales\" }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_data\",\"location\":\"C:/legal_data/case_data.csv\",\"sheet_name\":null,\"label\":\"Case Data\",\"description\":\"Contains information about legal cases, including case ID, lawyer ID, and verdict.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"verdict\",\"column_type\":\"xsd:string\",\"column_description\":\"The outcome of the case, e.g., 'guilty' or 'not guilty'.\"}]},\"table_id2\":{\"name\":\"lawyer_info\",\"location\":\"C:/legal_data/lawyer_info.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Lawyer Information\",\"description\":\"Contains details about lawyers, including their ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedule\",\"location\":\"C:/legal_data/court_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Court Schedule\",\"description\":\"Contains the schedule of court hearings, including date and case ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court hearing.\"},{\"column\":\"col_2\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the case scheduled for hearing.\"}]}},\"instructions\":[\"Load the 'case_data' table from the CSV file located at C:/legal_data/case_data.csv.\",\"Filter the 'case_data' table to include only cases where the verdict is 'guilty'.\",\"Load the 'lawyer_info' table from the Excel file located at C:/legal_data/lawyer_info.xlsx.\",\"Join the filtered 'case_data' with 'lawyer_info' on the 'lawyer_id' column using an inner join.\",\"Aggregate the joined data to find the total number of guilty verdicts per lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_data.csv\",\"type\":\"csv\"},\"output\":\"case_data\",\"comment\":\"Loading the case data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_data\",\"query\":\"verdict == 'guilty'\"},\"output\":\"guilty_cases\",\"comment\":\"Filtering cases to include only those with a guilty verdict.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyer_info.xlsx\",\"type\":\"xlsx\"},\"output\":\"lawyer_info\",\"comment\":\"Loading lawyer information from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"guilty_cases\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining guilty cases with lawyer information on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_guilty_verdicts\",\"function\":\"count\"}]},\"output\":\"guilty_verdicts_per_lawyer\",\"comment\":\"Aggregating data to find the total number of guilty verdicts per lawyer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains information about shipments including shipment ID, weight, and destination.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"Destination of the shipment.\"}]},\"table_id2\":{\"name\":\"delivery_schedule\",\"location\":\"C:/data/logistics/delivery_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Delivery Schedule\",\"description\":\"Contains the delivery schedule for shipments including shipment ID and delivery date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"delivery_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Scheduled delivery date for the shipment.\"}]},\"table_id3\":{\"name\":\"warehouse_inventory\",\"location\":\"C:/data/logistics/warehouse_inventory.csv\",\"sheet_name\":null,\"label\":\"Warehouse Inventory\",\"description\":\"Contains inventory details of the warehouse, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"col_2\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the item in stock.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at C:/data/logistics/shipment_data.csv.\",\"Load the 'delivery_schedule' table from the Excel file located at C:/data/logistics/delivery_schedule.xlsx.\",\"Filter the 'shipment_data' table to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered 'shipment_data' with the 'delivery_schedule' on the 'shipment_id' column using an inner join.\",\"Aggregate the joined table by 'destination' to calculate the total weight of shipments for each destination.\",\"Sort the aggregated data by 'total_weight' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/delivery_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"delivery_schedule\",\"comment\":\"Loading the delivery schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"heavy_shipments\",\"comment\":\"Filtering shipments to include only those with a weight greater than 1000 kg.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"heavy_shipments\",\"delivery_schedule\"],\"joinOn\":[\"shipment_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with delivery schedule on shipment_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"destination\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by destination to calculate total shipment weight.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_weight\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total weight in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with details such as transaction amount, currency code, and account ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"transaction_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code of the transaction amount.\"},{\"column\":\"col_4\",\"column_name\":\"account_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The account ID associated with the transaction.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/exchange_rates.xlsx\",\"sheet_name\":\"Rates\",\"label\":\"Exchange Rates\",\"description\":\"A table containing exchange rates for different currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Irrelevant Data\",\"description\":\"A table that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at C:/data/financial_transactions.csv.\",\"Load the 'exchange_rates' table from the Excel file located at C:/data/exchange_rates.xlsx.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Join the filtered transactions with the 'exchange_rates' table on the 'currency_code' column.\",\"Calculate a new column 'amount_in_usd' by multiplying 'transaction_amount' with 'exchange_rate'.\",\"Aggregate the joined table by 'account_id' to calculate the total 'amount_in_usd' for each account.\",\"Sort the aggregated data by 'total_amount_in_usd' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"transaction_amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with exchange rates on the currency code.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"amount_in_usd\",\"formula\":\"transaction_amount * exchange_rate\"},\"output\":\"transactions_with_usd\",\"comment\":\"Calculating the transaction amount in USD by multiplying with the exchange rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_usd\",\"groupBy\":\"account_id\",\"aggregations\":[{\"column\":\"amount_in_usd\",\"name\":\"total_amount_in_usd\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by account ID to calculate total amount in USD for each account.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_amount_in_usd\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total amount in USD in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'power_transform' operation do?","output":"The 'power_transform' operation applies a power transform (e.g., Box-Cox) for variance stabilization."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"This table contains detailed specifications of products including their development status and estimated completion time.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"development_status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current development status of the product.\"},{\"column\":\"col_3\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the product development started.\"},{\"column\":\"col_4\",\"column_name\":\"estimated_days_to_complete\",\"column_type\":\"xsd:integer\",\"column_description\":\"Estimated number of days to complete the product development.\"}]},\"table_id2\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"This table contains sales data for various products.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a development status of 'In Progress'.\",\"Calculate the estimated completion time by adding the estimated days to complete to the start date.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"development_status == 'In Progress'\"},\"output\":\"in_progress_products\",\"comment\":\"Filtering products to include only those with a development status of 'In Progress'.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"in_progress_products\",\"columnName\":\"estimated_completion_date\",\"formula\":\"`start_date` + `estimated_days_to_complete`\"},\"output\":\"products_with_completion_date\",\"comment\":\"Calculating the estimated completion date by adding the estimated days to complete to the start date.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/healthcare/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains patient information including ID, name, age, and diagnosis.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_4\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"medication_data\",\"location\":\"C:/data/healthcare/medication_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Medication Data\",\"description\":\"Contains medication details including patient ID, medication name, and cost.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"},{\"column\":\"col_3\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of the medication.\"}]},\"table_id3\":{\"name\":\"hospital_visits\",\"location\":\"C:/data/healthcare/hospital_visits.csv\",\"sheet_name\":null,\"label\":\"Hospital Visits\",\"description\":\"Records of hospital visits including patient ID, visit date, and reason for visit.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"visit_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the hospital visit.\"},{\"column\":\"col_3\",\"column_name\":\"reason\",\"column_type\":\"xsd:string\",\"column_description\":\"Reason for the hospital visit.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file.\",\"Filter the patient records to include only those with a diagnosis of hypertension.\",\"Load the medication data from the Excel file.\",\"Join the filtered patient records with the medication data on patient ID.\",\"Aggregate the joined data to find the average medication cost per patient.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/healthcare/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'hypertension'\"},\"output\":\"hypertension_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with hypertension.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/healthcare/medication_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"medication_data\",\"comment\":\"Loading medication data from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"hypertension_patients\",\"medication_data\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient records with medication data on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"cost\",\"name\":\"average_cost\",\"function\":\"mean\"}]},\"output\":\"average_medication_cost\",\"comment\":\"Aggregating data to find the average medication cost per patient.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_data\",\"location\":\"C:/data/patient_data.csv\",\"sheet_name\":null,\"label\":\"Patient Data\",\"description\":\"Contains demographic information of patients including age, gender, height, and weight.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"column_2_id\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"column_3_id\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the patient.\"},{\"column\":\"column_4_id\",\"column_name\":\"height\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Height of the patient in centimeters.\"},{\"column\":\"column_5_id\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the patient in kilograms.\"}]},\"table_id2\":{\"name\":\"medical_records\",\"location\":\"C:/data/medical_records.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Medical Records\",\"description\":\"Contains medical history and records of patients.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"column_2_id\",\"column_name\":\"medical_history\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical history of the patient.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Contains information about hospital staff members.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"}]}},\"instructions\":[\"Load the patient data table from the CSV file located at C:/data/patient_data.csv.\",\"Load the medical records table from the Excel file located at C:/data/medical_records.xlsx.\",\"Filter the patient data to include only patients above the age of 50.\",\"Join the filtered patient data with the medical records on the patient_id column.\",\"Calculate the Body Mass Index (BMI) for each patient using the formula: weight / (height/100)^2.\",\"Filter the joined table to include only patients with a BMI greater than 25.\",\"Aggregate the data by gender to find the average BMI for each gender.\",\"Sort the aggregated data by average BMI in descending order.\",\"Apply a log transformation to the average BMI column to stabilize variance.\",\"Export the final table to a CSV file located at C:/output/average_bmi_by_gender.csv.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_data.csv\",\"type\":\"csv\"},\"output\":\"patient_data\",\"comment\":\"Loading patient data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/medical_records.xlsx\",\"type\":\"xlsx\"},\"output\":\"medical_records\",\"comment\":\"Loading medical records from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_data\",\"query\":\"age > 50\"},\"output\":\"filtered_patient_data\",\"comment\":\"Filtering patient data to include only patients above the age of 50.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_patient_data\",\"medical_records\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient data with medical records on patient_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"BMI\",\"formula\":\"weight / (height/100)**2\"},\"output\":\"joined_data_with_bmi\",\"comment\":\"Calculating BMI for each patient.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"joined_data_with_bmi\",\"query\":\"BMI > 25\"},\"output\":\"overweight_patients\",\"comment\":\"Filtering to include only patients with a BMI greater than 25.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"overweight_patients\",\"groupBy\":\"gender\",\"aggregations\":[{\"column\":\"BMI\",\"name\":\"average_BMI\",\"function\":\"mean\"}]},\"output\":\"average_bmi_by_gender\",\"comment\":\"Aggregating data by gender to find the average BMI.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_bmi_by_gender\",\"sortBy\":\"average_BMI\",\"order\":\"desc\"},\"output\":\"sorted_average_bmi_by_gender\",\"comment\":\"Sorting the aggregated data by average BMI in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_average_bmi_by_gender\",\"transformations\":[{\"columns\":[\"average_BMI\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_average_bmi_by_gender\",\"comment\":\"Applying a log transformation to the average BMI column.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/output/average_bmi_by_gender.csv\",\"type\":\"csv\"},\"output\":\"exported_data\",\"comment\":\"Exporting the final table to a CSV file.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nAre comparison operations applied element-wise on table columns?","output":"Yes, comparison operations are applied element-wise when used with table columns."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid arithmetic expression using parentheses for precedence.","output":"'col1 * (col2 + col3) / col4'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'rolling_kurtosis' operation be used?","output":"Use 'rolling_kurtosis' when you need to measure the peakedness of data distributions over a rolling window, useful for identifying extreme values in time series analysis."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid Boolean expression using 'not'.","output":"'not col1 < 5'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid Boolean expression using 'and'.","output":"'col1 > 10 and col2 < 5'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information about customers, including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"sales_transactions\",\"location\":\"C:/data/sales_transactions.xlsx\",\"sheet_name\":\"Transactions\",\"label\":\"Sales Transactions\",\"description\":\"Contains records of sales transactions, including customer ID, transaction date, and total amount.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the transaction to a customer.\"},{\"column\":\"col_3\",\"column_name\":\"total_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total amount of the transaction.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the sales transactions table from the Excel file.\",\"Filter the sales transactions to include only those with a total amount greater than $100.\",\"Join the filtered sales transactions with customer demographics on customer ID.\",\"Aggregate the joined table to calculate the total sales per customer.\",\"Sort the aggregated data by total sales in descending order.\",\"Identify the top 10 customers based on total sales.\",\"Calculate a new column for customer lifetime value using a specific formula.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_transactions.xlsx\",\"type\":\"xlsx\"},\"output\":\"sales_transactions\",\"comment\":\"Loading sales transactions data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_transactions\",\"query\":\"`total_amount` > 100\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales transactions to include only those with a total amount greater than $100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered sales transactions with customer demographics on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"total_amount\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"aggregated_sales\",\"comment\":\"Aggregating data to calculate total sales per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_sales\",\"comment\":\"Sorting aggregated data by total sales in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_sales\",\"query\":\"index < 10\"},\"output\":\"top_customers\",\"comment\":\"Identifying the top 10 customers based on total sales.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"top_customers\",\"columnName\":\"customer_lifetime_value\",\"formula\":\"`total_sales` * 0.8\"},\"output\":\"customers_with_clv\",\"comment\":\"Calculating customer lifetime value using a specific formula.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of all shipments including shipment ID, customer ID, status, and weight.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer associated with the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment (e.g., delivered, in transit).\"},{\"column\":\"col_4\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/logistics/customer_data.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"Contains customer details including customer ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/logistics/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"Contains inventory details including item ID and stock levels.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"col_2\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the item.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of all shipments.\",\"Filter the shipment data to include only shipments with a status of 'delivered'.\",\"Join the filtered shipment data with the 'customer_data' table on the 'customer_id' column.\",\"Aggregate the joined data to calculate the total weight of shipments per customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"status == 'delivered'\"},\"output\":\"delivered_shipments\",\"comment\":\"Filtering shipment data to include only delivered shipments.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"delivered_shipments\",\"customer_data\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining delivered shipments with customer data on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"total_weight_per_customer\",\"comment\":\"Aggregating data to calculate the total weight of shipments per customer.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat forecasting algorithms are supported in 'forecastData'?","output":"Supported algorithms include 'holt_winters', 'xgboost', 'isolation_forest', 'local_outlier_factor', 'one_class_svm', 'gmm', and 'z-score'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"This table contains customer feedback including ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:integer\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comments from customers.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.csv\",\"sheet_name\":null,\"label\":\"Customer Details\",\"description\":\"This table contains detailed information about customers including their region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"col_3\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales Data\",\"description\":\"This table contains sales data for various products, not relevant to customer feedback.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount of sales for the product.\"},{\"column\":\"col_3\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the sale.\"}]}},\"instructions\":[\"Load the customer feedback data from the CSV file.\",\"Filter the feedback to include only those with a rating of 4 or 5.\",\"Join the filtered feedback with customer details using the customer ID.\",\"Aggregate the data to find the average rating per region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback to include only those with a rating of 4 or 5.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"feedback_with_details\",\"comment\":\"Joining filtered feedback with customer details using customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"feedback_with_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_per_region\",\"comment\":\"Aggregating data to find the average rating per region.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/legal_data/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases Data\",\"description\":\"This table contains information about various legal cases, including case ID, category, and date.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"CASE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"column_2_id\",\"column_name\":\"CATEGORY\",\"column_type\":\"xsd:string\",\"column_description\":\"The legal category of the case.\"},{\"column\":\"column_3_id\",\"column_name\":\"DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/legal_data/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers Data\",\"description\":\"This table contains information about lawyers, including their ID, name, and specialization.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"LAWYER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"},{\"column\":\"column_3_id\",\"column_name\":\"SPECIALIZATION\",\"column_type\":\"xsd:string\",\"column_description\":\"The area of law the lawyer specializes in.\"}]}},\"instructions\":[\"Load the table 'court_cases' which contains information about various legal cases.\",\"Filter the 'court_cases' table to include only cases from the year 2022.\",\"Aggregate the filtered data to count the number of cases per legal category.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"`DATE`.dt.year == 2022\"},\"output\":\"court_cases_2022\",\"comment\":\"Filtering court cases to include only those from the year 2022.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"court_cases_2022\",\"groupBy\":\"CATEGORY\",\"aggregations\":[{\"column\":\"CASE_ID\",\"name\":\"case_count\",\"function\":\"count\"}]},\"output\":\"cases_per_category\",\"comment\":\"Aggregating the filtered data to count the number of cases per legal category.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'exp_transform' operation be used?","output":"Use 'exp_transform' when you need to reverse a log transformation or model exponential growth, such as population growth or interest calculations."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of all shipments including product IDs, quantities, and status.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product being shipped.\"},{\"column\":\"col_3\",\"column_name\":\"required_quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the product required for the shipment.\"},{\"column\":\"col_4\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_inventory\",\"location\":\"C:/data/logistics/warehouse_inventory.csv\",\"sheet_name\":null,\"label\":\"Warehouse Inventory\",\"description\":\"Current stock levels of products in the warehouse.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product in inventory.\"},{\"column\":\"col_2\",\"column_name\":\"inventory_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the product in the warehouse.\"}]},\"table_id3\":{\"name\":\"transport_routes\",\"location\":\"C:/data/logistics/transport_routes.xlsx\",\"sheet_name\":\"Routes\",\"label\":\"Transport Routes\",\"description\":\"Details of transport routes used for shipments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"route_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transport route.\"},{\"column\":\"col_2\",\"column_name\":\"origin\",\"column_type\":\"xsd:string\",\"column_description\":\"Starting point of the transport route.\"},{\"column\":\"col_3\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"End point of the transport route.\"}]}},\"instructions\":[\"Load the table 'shipment_data' containing shipment details.\",\"Load the table 'warehouse_inventory' with current stock levels.\",\"Filter the 'shipment_data' to include only shipments with a status of 'pending'.\",\"Join the filtered shipment data with 'warehouse_inventory' on 'product_id'.\",\"Calculate a new column 'stock_deficit' by subtracting 'inventory_level' from 'required_quantity'.\",\"Filter the joined table to include only rows where 'stock_deficit' is greater than zero.\",\"Sort the resulting table by 'stock_deficit' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data containing shipment details.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouse_inventory.csv\",\"type\":\"csv\"},\"output\":\"warehouse_inventory\",\"comment\":\"Loading warehouse inventory data with current stock levels.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"`status` == 'pending'\"},\"output\":\"pending_shipments\",\"comment\":\"Filtering shipment data to include only pending shipments.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"pending_shipments\",\"warehouse_inventory\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining pending shipment data with warehouse inventory on product_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"stock_deficit\",\"formula\":\"`required_quantity` - `inventory_level`\"},\"output\":\"joined_data_with_deficit\",\"comment\":\"Calculating stock deficit by subtracting inventory level from required quantity.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"joined_data_with_deficit\",\"query\":\"`stock_deficit` > 0\"},\"output\":\"deficit_shipments\",\"comment\":\"Filtering joined data to include only rows where stock deficit is greater than zero.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"deficit_shipments\",\"sortBy\":\"stock_deficit\",\"order\":\"desc\"},\"output\":\"sorted_deficit_shipments\",\"comment\":\"Sorting the resulting table by stock deficit in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat are the main components of a Tamarind workflow action?","output":"Each action consists of a name identifying the operation, a list of parameters including input tables and transformation options, and an output table name where the result is stored."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for the building, including date, total energy consumption, and building area.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"total_energy\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total energy consumption in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"building_area\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The area of the building in square meters.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including date, temperature, and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity on the given date.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for the building, including dates and tasks.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the scheduled maintenance.\"},{\"column\":\"col_2\",\"column_name\":\"task\",\"column_type\":\"xsd:string\",\"column_description\":\"The maintenance task to be performed.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only entries from the last year.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing total energy consumption by the building area.\",\"Aggregate the data by month to calculate the average energy efficiency.\",\"Sort the aggregated data by energy efficiency in descending order.\",\"Forecast the next 12 months of energy consumption using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date` >= '2022-01-01'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only entries from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"energy_weather_data\",\"comment\":\"Joining the filtered energy consumption data with the weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"energy_weather_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`total_energy` / `building_area`\"},\"output\":\"energy_weather_efficiency\",\"comment\":\"Calculating a new column for energy efficiency by dividing total energy consumption by the building area.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_weather_efficiency\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"energy_efficiency\",\"name\":\"avg_energy_efficiency\",\"function\":\"mean\"}]},\"output\":\"monthly_energy_efficiency\",\"comment\":\"Aggregating the data by month to calculate the average energy efficiency.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_energy_efficiency\",\"sortBy\":\"avg_energy_efficiency\",\"order\":\"desc\"},\"output\":\"sorted_energy_efficiency\",\"comment\":\"Sorting the aggregated data by energy efficiency in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"energy_weather_efficiency\",\"columns\":[\"total_energy\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the next 12 months of energy consumption using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'rolling_kurtosis' operation do?","output":"The 'rolling_kurtosis' operation computes rolling kurtosis over a given window. It requires 'parameters.window'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat types of functions are allowed in 'calculateDerivedColumn'?","output":"Allowed functions include abs, round, min, max, sqrt, log, exp, sin, cos, tan, sum, mean, std, var, median, cumsum, cumprod, and diff."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'filterData' action.","output":"{ \"name\": \"filterData\", \"parameters\": { \"table\": \"sales_data\", \"query\": \"revenue > 1000\" }, \"output\": \"filtered_sales\" }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains demographic and medical information of patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_visits\",\"location\":\"C:/data/hospital_visits.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Hospital Visits\",\"description\":\"Records of hospital visits including duration and cost.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"visit_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital visit.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking to the patient records.\"},{\"column\":\"col_3\",\"column_name\":\"length_of_stay\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of days the patient stayed in the hospital.\"},{\"column\":\"col_4\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of the hospital visit.\"}]},\"table_id3\":{\"name\":\"insurance_claims\",\"location\":\"C:/data/insurance_claims.csv\",\"sheet_name\":null,\"label\":\"Insurance Claims\",\"description\":\"Contains information about insurance claims made by patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"claim_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each insurance claim.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking to the patient records.\"},{\"column\":\"col_3\",\"column_name\":\"claim_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount claimed by the patient.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file located at C:/data/patient_records.csv.\",\"Load the hospital visits table from the Excel file located at C:/data/hospital_visits.xlsx.\",\"Filter the patient records to include only patients older than 50 years.\",\"Join the filtered patient records with the hospital visits table on patient_id.\",\"Calculate the total length of stay for each patient.\",\"Aggregate the data to find the total number of visits per hospital.\",\"Sort the aggregated data by the total number of visits in descending order.\",\"Calculate a new column for each patient indicating the total cost of visits.\",\"Filter the joined table to include only patients with a total cost greater than $10,000.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hospital_visits.xlsx\",\"type\":\"xlsx\"},\"output\":\"hospital_visits\",\"comment\":\"Loading the hospital visits dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"age > 50\"},\"output\":\"filtered_patient_records\",\"comment\":\"Filtering patient records to include only patients older than 50 years.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_patient_records\",\"hospital_visits\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_patient_visits\",\"comment\":\"Joining filtered patient records with hospital visits on patient_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_patient_visits\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"length_of_stay\",\"name\":\"total_length_of_stay\",\"function\":\"sum\"}]},\"output\":\"patient_visits_with_total_stay\",\"comment\":\"Calculating the total length of stay for each patient.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_patient_visits\",\"groupBy\":\"hospital_id\",\"aggregations\":[{\"column\":\"visit_id\",\"name\":\"total_visits\",\"function\":\"count\"}]},\"output\":\"hospital_visit_counts\",\"comment\":\"Aggregating data to find the total number of visits per hospital.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"hospital_visit_counts\",\"sortBy\":\"total_visits\",\"order\":\"desc\"},\"output\":\"sorted_hospital_visits\",\"comment\":\"Sorting the aggregated data by the total number of visits in descending order.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_patient_visits\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"cost\",\"name\":\"total_visit_cost\",\"function\":\"sum\"}]},\"output\":\"patient_visits_with_cost\",\"comment\":\"Calculating a new column for each patient indicating the total cost of visits.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_visits_with_cost\",\"query\":\"total_visit_cost > 10000\"},\"output\":\"high_cost_patients\",\"comment\":\"Filtering the joined table to include only patients with a total cost greater than $10,000.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat types of comparison operations are supported in Tamarind expressions?","output":"Supported comparison operations include equality ('=='), inequality ('!='), less than ('<'), greater than ('>'), less than or equal to ('<='), and greater than or equal to ('>=')."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the purpose of the 'forecastData' action?","output":"The 'forecastData' action performs time-series forecasting by predicting future values based on past trends using various forecasting models."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'fillna' operation be used?","output":"Use 'fillna' when you need to replace missing values with a specific value or method, such as zero, forward fill, or mean replacement. This is useful when handling incomplete datasets."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Table\",\"description\":\"Contains details of all raw materials used in manufacturing, including stock levels and reorder thresholds.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"stock_level\",\"column_name\":\"STOCK_LEVEL\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the material.\"},{\"column\":\"reorder_threshold\",\"column_name\":\"REORDER_THRESHOLD\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Stock level at which the material should be reordered.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"Contains the production schedule for the next month, including daily usage of materials.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"daily_usage\",\"column_name\":\"DAILY_USAGE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Daily usage of the material in production.\"},{\"column\":\"days_until_restock\",\"column_name\":\"DAYS_UNTIL_RESTOCK\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of days until the material is restocked.\"}]},\"table_id3\":{\"name\":\"supplier_info\",\"location\":\"C:/data/manufacturing/supplier_info.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains supplier details for each material, including reliability scores.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"supplier_name\",\"column_name\":\"SUPPLIER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"},{\"column\":\"reliability_score\",\"column_name\":\"RELIABILITY_SCORE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Reliability score of the supplier.\"}]},\"table_id4\":{\"name\":\"machine_maintenance\",\"location\":\"C:/data/manufacturing/machine_maintenance.csv\",\"sheet_name\":null,\"label\":\"Machine Maintenance Records\",\"description\":\"Contains records of machine maintenance activities, not relevant for this workflow.\",\"columns\":[{\"column\":\"machine_id\",\"column_name\":\"MACHINE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each machine.\"},{\"column\":\"maintenance_date\",\"column_name\":\"MAINTENANCE_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the maintenance activity.\"},{\"column\":\"maintenance_type\",\"column_name\":\"MAINTENANCE_TYPE\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of maintenance performed.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains details of all raw materials used in manufacturing.\",\"Load the table 'production_schedule' which contains the production schedule for the next month.\",\"Filter the 'raw_materials' table to include only materials with a stock level below the reorder threshold.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Calculate a new column 'required_quantity' in the joined table by multiplying 'daily_usage' by 'days_until_restock'.\",\"Aggregate the joined table to find the total 'required_quantity' for each 'material_id'.\",\"Sort the aggregated table by 'required_quantity' in descending order to prioritize materials.\",\"Load the table 'supplier_info' which contains supplier details for each material.\",\"Join the sorted table with the 'supplier_info' table on the 'material_id' column.\",\"Filter the final table to include only suppliers with a 'reliability_score' above 80.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"STOCK_LEVEL < REORDER_THRESHOLD\"},\"output\":\"low_stock_materials\",\"comment\":\"Filtering raw materials to include only those with stock levels below the reorder threshold.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_stock_materials\",\"production_schedule\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"materials_with_schedule\",\"comment\":\"Joining low stock materials with the production schedule on material_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"materials_with_schedule\",\"columnName\":\"required_quantity\",\"formula\":\"DAILY_USAGE * DAYS_UNTIL_RESTOCK\"},\"output\":\"materials_with_required_quantity\",\"comment\":\"Calculating the required quantity by multiplying daily usage by days until restock.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"materials_with_required_quantity\",\"groupBy\":\"MATERIAL_ID\",\"aggregations\":[{\"column\":\"required_quantity\",\"name\":\"total_required_quantity\",\"function\":\"sum\"}]},\"output\":\"aggregated_materials\",\"comment\":\"Aggregating to find the total required quantity for each material.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_materials\",\"sortBy\":\"total_required_quantity\",\"order\":\"desc\"},\"output\":\"prioritized_materials\",\"comment\":\"Sorting materials by total required quantity in descending order.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/supplier_info.csv\",\"type\":\"csv\"},\"output\":\"supplier_info\",\"comment\":\"Loading supplier information from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"prioritized_materials\",\"supplier_info\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"materials_with_suppliers\",\"comment\":\"Joining prioritized materials with supplier information on material_id.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"materials_with_suppliers\",\"query\":\"RELIABILITY_SCORE > 80\"},\"output\":\"reliable_suppliers\",\"comment\":\"Filtering to include only suppliers with a reliability score above 80.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'applyTransformations' action with a non-existent column.","output":"{ \"name\": \"applyTransformations\", \"parameters\": { \"table\": \"sales_data\", \"transformations\": [ { \"columns\": [\"non_existent_column\"], \"operation\": \"log_transform\" } ] }, \"output\": \"transformed_sales\" } // Error: 'non_existent_column' does not exist in the table."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid comparison expression using an unsupported operator.","output":"'col1 === col2' // Error: '===' is not a supported comparison operator in Tamarind expressions."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/HR_Database/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"Contains detailed records of all employees including their department and job title.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the employee works.\"},{\"column\":\"col_3\",\"column_name\":\"job_title\",\"column_type\":\"xsd:string\",\"column_description\":\"Job title of the employee.\"}]},\"table_id2\":{\"name\":\"salary_details\",\"location\":\"C:/HR_Database/salary_details.csv\",\"sheet_name\":null,\"label\":\"Salary Details\",\"description\":\"Contains salary information for each employee identified by employee_id.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual salary of the employee.\"}]},\"table_id3\":{\"name\":\"benefits_details\",\"location\":\"C:/HR_Database/benefits_details.csv\",\"sheet_name\":null,\"label\":\"Benefits Details\",\"description\":\"Contains information about employee benefits packages.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"benefits_package\",\"column_type\":\"xsd:string\",\"column_description\":\"Details of the benefits package for the employee.\"}]}},\"instructions\":[\"Load the employee records table from the HR database.\",\"Filter the records to include only employees in the 'Engineering' department.\",\"Join the filtered employee records with the salary details table on the 'employee_id' column.\",\"Calculate the average salary for each job title within the Engineering department.\",\"Sort the results by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_Database/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records from the HR database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"department == 'Engineering'\"},\"output\":\"engineering_employees\",\"comment\":\"Filtering records to include only employees in the Engineering department.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_Database/salary_details.csv\",\"type\":\"csv\"},\"output\":\"salary_details\",\"comment\":\"Loading the salary details from the HR database.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"engineering_employees\",\"salary_details\"],\"joinOn\":[\"employee_id\"],\"joinType\":\"inner\"},\"output\":\"engineering_salaries\",\"comment\":\"Joining filtered employee records with salary details on employee_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"engineering_salaries\",\"groupBy\":\"job_title\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"average_salary_by_title\",\"comment\":\"Calculating the average salary for each job title within the Engineering department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_salary_by_title\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_average_salary\",\"comment\":\"Sorting the results by average salary in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat parameters are required for the 'calculateDerivedColumn' action?","output":"'calculateDerivedColumn' requires a 'table' (name of the table), 'columnName' (name of the new column), and 'formula' (expression using columns and functions)."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of products including status and supplier ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Approval status of the product.\"},{\"column\":\"col_3\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]},\"table_id2\":{\"name\":\"supplier_information\",\"location\":\"C:/data/supplier_information.xlsx\",\"sheet_name\":\"Suppliers\",\"label\":\"Supplier Information\",\"description\":\"Contains information about suppliers including their IDs and material costs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"material_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of materials provided by the supplier.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"random_column\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant column for demonstration.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a status of 'approved'.\",\"Load the supplier information from the Excel file.\",\"Join the approved product specifications with the supplier information on the supplier ID.\",\"Aggregate the joined data to calculate the total cost of materials for each product.\",\"Sort the aggregated data by total cost in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"status == 'approved'\"},\"output\":\"approved_products\",\"comment\":\"Filtering product specifications to include only approved products.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_information.xlsx\",\"type\":\"xlsx\"},\"output\":\"supplier_information\",\"comment\":\"Loading supplier information from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"approved_products\",\"supplier_information\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining approved product specifications with supplier information based on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"material_cost\",\"name\":\"total_material_cost\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total material cost for each product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_material_cost\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total material cost in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/healthcare/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"A table containing patient information including age, diagnosis, and treatment details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_staff\",\"location\":\"C:/data/healthcare/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"A table containing information about hospital staff including their roles and departments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the staff member works.\"}]}},\"instructions\":[\"Load the patient records table from the provided CSV file.\",\"Filter the patient records to include only those with a diagnosis of hypertension.\",\"Calculate the average age of patients diagnosed with hypertension.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/healthcare/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"`diagnosis` == 'hypertension'\"},\"output\":\"hypertension_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with hypertension.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"hypertension_patients\",\"groupBy\":null,\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"average_age_hypertension\",\"comment\":\"Calculating the average age of patients diagnosed with hypertension.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid comparison expression using greater than.","output":"'col1 > col2'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if the 'query' in 'filterData' references a non-existent column?","output":"If the query references a non-existent column, an error is raised."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipments\",\"location\":\"C:/data/logistics/shipments.csv\",\"sheet_name\":null,\"label\":\"Shipment Details\",\"description\":\"Contains details of all shipments including shipment_id, weight, and destination.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"column_2_id\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"column_3_id\",\"column_name\":\"destination_city\",\"column_type\":\"xsd:string\",\"column_description\":\"City where the shipment is to be delivered.\"}]},\"table_id2\":{\"name\":\"delivery_times\",\"location\":\"C:/data/logistics/delivery_times.csv\",\"sheet_name\":null,\"label\":\"Delivery Times\",\"description\":\"Records of delivery times including shipment_id, expected and actual delivery dates.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"column_2_id\",\"column_name\":\"expected_delivery_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date the shipment is expected to be delivered.\"},{\"column\":\"column_3_id\",\"column_name\":\"actual_delivery_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date the shipment was actually delivered.\"}]},\"table_id3\":{\"name\":\"warehouse_inventory\",\"location\":\"C:/data/logistics/warehouse_inventory.csv\",\"sheet_name\":null,\"label\":\"Warehouse Inventory\",\"description\":\"Details of current inventory in the warehouse, not relevant for this workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"column_2_id\",\"column_name\":\"quantity\",\"column_type\":\"xsd:integer\",\"column_description\":\"Quantity of the item in stock.\"}]}},\"instructions\":[\"Load the 'shipments' table containing shipment details.\",\"Load the 'delivery_times' table with delivery time records.\",\"Filter the 'shipments' table to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered shipments with the 'delivery_times' table on the 'shipment_id' column.\",\"Calculate a new column 'delivery_delay' by subtracting 'expected_delivery_date' from 'actual_delivery_date'.\",\"Aggregate the joined table to find the average delivery delay per 'destination_city'.\",\"Sort the aggregated data by 'average_delay' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipments.csv\",\"type\":\"csv\"},\"output\":\"shipments\",\"comment\":\"Loading the shipments dataset containing shipment details.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/delivery_times.csv\",\"type\":\"csv\"},\"output\":\"delivery_times\",\"comment\":\"Loading the delivery times dataset with delivery records.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipments\",\"query\":\"weight > 1000\"},\"output\":\"heavy_shipments\",\"comment\":\"Filtering shipments to include only those with weight greater than 1000 kg.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"heavy_shipments\",\"delivery_times\"],\"joinOn\":[\"shipment_id\"],\"joinType\":\"inner\"},\"output\":\"joined_shipments\",\"comment\":\"Joining filtered shipments with delivery times on shipment_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_shipments\",\"columnName\":\"delivery_delay\",\"formula\":\"`actual_delivery_date` - `expected_delivery_date`\"},\"output\":\"shipments_with_delay\",\"comment\":\"Calculating delivery delay by subtracting expected delivery date from actual delivery date.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"shipments_with_delay\",\"groupBy\":\"destination_city\",\"aggregations\":[{\"column\":\"delivery_delay\",\"name\":\"average_delay\",\"function\":\"mean\"}]},\"output\":\"average_delay_per_city\",\"comment\":\"Aggregating data to find average delivery delay per destination city.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_delay_per_city\",\"sortBy\":\"average_delay\",\"order\":\"desc\"},\"output\":\"sorted_delays\",\"comment\":\"Sorting the aggregated data by average delay in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"employee_data\",\"location\":\"C:/data/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Department Data\",\"description\":\"Contains information about departments including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the department.\"}]},\"table_3\":{\"name\":\"project_data\",\"location\":\"C:/data/project_data.xlsx\",\"sheet_name\":\"Projects\",\"label\":\"Project Data\",\"description\":\"Contains information about projects including project IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"project_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the project.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/employee_data.csv.\",\"Filter the employee data to include only those with a salary greater than $50,000.\",\"Load the department data from the Excel file located at C:/data/department_data.xlsx.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Aggregate the joined data to calculate the average salary per department.\",\"Sort the aggregated data by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading the employee data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employees with a salary greater than $50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_data\",\"comment\":\"Loading the department data from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_salary_employees\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"employee_department_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"employee_department_data\",\"groupBy\":\"department_id\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"average_salary_per_department\",\"comment\":\"Calculating the average salary per department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_salary_per_department\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_average_salary\",\"comment\":\"Sorting the aggregated data by average salary in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of each product including product ID, name, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]},\"table_id2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including product ID, demand, and potential customers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"average_demand\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Average demand for the product over the last year.\"},{\"column\":\"col_3\",\"column_name\":\"potential_customers\",\"column_type\":\"xsd:integer\",\"column_description\":\"Estimated number of potential customers for the product.\"},{\"column\":\"col_4\",\"column_name\":\"year\",\"column_type\":\"xsd:integer\",\"column_description\":\"Year of the market research data.\"}]},\"table_id3\":{\"name\":\"historical_sales\",\"location\":\"C:/data/historical_sales.csv\",\"sheet_name\":null,\"label\":\"Historical Sales Data\",\"description\":\"Contains historical sales data for various products.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sales figures for the product.\"},{\"column\":\"col_3\",\"column_name\":\"year\",\"column_type\":\"xsd:integer\",\"column_description\":\"Year of the sales data.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel sheet.\",\"Filter the market research data to include only entries from the last year.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Calculate the potential market size by multiplying the average demand with the number of potential customers.\",\"Aggregate the joined data by product category to find the total potential market size for each category.\",\"Sort the aggregated data by total potential market size in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading the market research data from an Excel sheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"year == 2023\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only entries from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_market_size\",\"formula\":\"average_demand * potential_customers\"},\"output\":\"joined_data_with_market_size\",\"comment\":\"Calculating potential market size by multiplying average demand with the number of potential customers.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_market_size\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"potential_market_size\",\"name\":\"total_potential_market_size\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined data by product category to find the total potential market size for each category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_potential_market_size\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total potential market size in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of shipments including shipment ID, weight, departure and arrival times.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"departure_time\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Time when the shipment departs.\"},{\"column\":\"col_4\",\"column_name\":\"arrival_time\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Time when the shipment arrives.\"}]},\"table_id2\":{\"name\":\"delivery_schedule\",\"location\":\"C:/data/logistics/delivery_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Delivery Schedule\",\"description\":\"Contains the delivery schedule including shipment ID and destination.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"Destination of the shipment.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/logistics/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"Contains inventory details which are not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"col_2\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the item in stock.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at C:/data/logistics/shipment_data.csv.\",\"Load the 'delivery_schedule' table from the Excel file located at C:/data/logistics/delivery_schedule.xlsx.\",\"Filter the 'shipment_data' table to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered 'shipment_data' with 'delivery_schedule' on the 'shipment_id' column using an inner join.\",\"Calculate a new column 'delivery_time_hours' in the joined table by subtracting 'departure_time' from 'arrival_time' and converting the result to hours.\",\"Aggregate the joined table by 'destination' to calculate the total weight of shipments and the average delivery time.\",\"Sort the aggregated data by 'total_weight' in descending order.\",\"Apply a standardization transformation to the 'average_delivery_time' column in the sorted table.\",\"Forecast the future shipment weights for the next 30 days using the 'shipment_data' table with the 'holt_winters' algorithm.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/delivery_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"delivery_schedule\",\"comment\":\"Loading the delivery schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"filtered_shipment_data\",\"comment\":\"Filtering shipments to include only those with weight greater than 1000 kg.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipment_data\",\"delivery_schedule\"],\"joinOn\":[\"shipment_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with delivery schedule on shipment_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"delivery_time_hours\",\"formula\":\"(`arrival_time` - `departure_time`).total_seconds() / 3600\"},\"output\":\"joined_data_with_delivery_time\",\"comment\":\"Calculating delivery time in hours by subtracting departure time from arrival time.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_delivery_time\",\"groupBy\":\"destination\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"},{\"column\":\"delivery_time_hours\",\"name\":\"average_delivery_time\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by destination to calculate total weight and average delivery time.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_weight\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total weight in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"transformations\":[{\"columns\":[\"average_delivery_time\"],\"operation\":\"standardize\"}]},\"output\":\"standardized_data\",\"comment\":\"Standardizing the average delivery time column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"shipment_data\",\"columns\":[\"weight\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"departure_time\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7}},\"output\":\"forecasted_weights\",\"comment\":\"Forecasting future shipment weights for the next 30 days using Holt-Winters algorithm.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'set_index' transformation.","output":"{ \"name\": \"applyTimeSeriesTransformations\", \"parameters\": { \"table\": \"stock_prices\", \"transformations\": [ { \"columns\": [\"date\"], \"operation\": \"set_index\" } ] }, \"output\": \"indexed_stock_prices\" }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"This table contains customer feedback including ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating of the service, from 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Customer comments about the service.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"This table contains detailed information about customers, including their region.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"column_3_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales Data\",\"description\":\"This table contains sales data for various products, not relevant for customer service analysis.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback data to include only entries with a rating less than 3.\",\"Load the customer details table from the Excel file.\",\"Join the filtered feedback data with customer details using the customer ID.\",\"Aggregate the joined data to count the number of complaints per region.\",\"Sort the aggregated data by the number of complaints in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating < 3\"},\"output\":\"low_rating_feedback\",\"comment\":\"Filtering feedback to include only entries with a rating less than 3.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_rating_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_feedback_details\",\"comment\":\"Joining filtered feedback with customer details using customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_feedback_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"customer_id\",\"name\":\"complaint_count\",\"function\":\"count\"}]},\"output\":\"complaints_per_region\",\"comment\":\"Aggregating data to count the number of complaints per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"complaints_per_region\",\"sortBy\":\"complaint_count\",\"order\":\"desc\"},\"output\":\"sorted_complaints\",\"comment\":\"Sorting the aggregated data by the number of complaints in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"Contains daily energy consumption data with timestamps and consumption values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Weather Data\",\"description\":\"Contains daily weather data including temperature and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The daily average temperature in Celsius.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The daily average humidity percentage.\"}]},\"table_id3\":{\"name\":\"historical_energy_prices\",\"location\":\"C:/data/historical_energy_prices.csv\",\"sheet_name\":null,\"label\":\"Historical Energy Prices\",\"description\":\"Contains historical energy prices data which is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the price record.\"},{\"column\":\"col_2\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kilowatt-hour.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from 2023.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate the average energy consumption per day.\",\"Sort the joined data by temperature in descending order.\",\"Apply a rolling mean transformation with a window of 7 days on the energy consumption column.\",\"Forecast the energy consumption for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records from 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered energy consumption data with the weather data on the date column.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"average_consumption\",\"function\":\"mean\"}]},\"output\":\"daily_average_consumption\",\"comment\":\"Calculating the average energy consumption per day.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"joined_data\",\"sortBy\":\"temperature\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the joined data by temperature in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_data\",\"transformations\":[{\"columns\":[\"consumption\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":7}}]},\"output\":\"smoothed_data\",\"comment\":\"Applying a rolling mean transformation with a window of 7 days on the energy consumption column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"smoothed_data\",\"columns\":[\"consumption\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"forecast_results\",\"comment\":\"Forecasting the energy consumption for the next 30 days using the Holt-Winters method.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/forecast_results.csv\",\"type\":\"csv\"},\"output\":\"forecast_results\",\"comment\":\"Saving the forecasted data to a new CSV file.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"Contains customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:integer\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Customer feedback comments.\"}]},\"table_id2\":{\"name\":\"purchase_history\",\"location\":\"C:/data/purchase_history.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Purchase History\",\"description\":\"Records of customer purchases including amounts and dates.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"purchase_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount spent by the customer.\"},{\"column\":\"col_3\",\"column_name\":\"purchase_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the purchase.\"}]},\"table_id3\":{\"name\":\"customer_support_tickets\",\"location\":\"C:/data/customer_support_tickets.csv\",\"sheet_name\":null,\"label\":\"Customer Support Tickets\",\"description\":\"Contains records of customer support interactions.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"ticket_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each support ticket.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"issue_description\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the customer's issue.\"}]}},\"instructions\":[\"Load the customer feedback data from the CSV file.\",\"Load the customer purchase history from the Excel file.\",\"Filter the feedback data to include only feedback with a rating of 4 or 5.\",\"Join the filtered feedback data with the purchase history on the customer ID.\",\"Aggregate the joined data to calculate the total purchase amount for each customer.\",\"Sort the aggregated data by total purchase amount in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/purchase_history.xlsx\",\"type\":\"xlsx\"},\"output\":\"purchase_history\",\"comment\":\"Loading customer purchase history from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback data to include only ratings of 4 or 5.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"purchase_history\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered feedback with purchase history on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"purchase_amount\",\"name\":\"total_purchase\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total purchase amount for each customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_purchase\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total purchase amount in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Rating given by the customer, ranging from 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"feedback\",\"column_type\":\"xsd:string\",\"column_description\":\"Textual feedback provided by the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"column_3_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"Table containing sales data for various products.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Load the customer details table from the Excel file.\",\"Filter the customer feedback to include only feedback with a rating of 4 or higher.\",\"Join the filtered feedback with customer details on the customer ID.\",\"Aggregate the joined data to calculate the average rating per region.\",\"Sort the aggregated data by average rating in descending order.\",\"Filter the sorted data to include only regions with more than 50 feedback entries.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"filtered_feedback\",\"comment\":\"Filtering feedback to include only ratings of 4 or higher.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"},{\"column\":\"customer_id\",\"name\":\"total_feedback\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating average rating and total feedback count per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting data by average rating in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_data\",\"query\":\"total_feedback > 50\"},\"output\":\"filtered_sorted_data\",\"comment\":\"Filtering to include only regions with more than 50 feedback entries.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains demographic and medical information of patients.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"column_2_id\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"column_3_id\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_visits\",\"location\":\"C:/data/hospital_visits.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Hospital Visits\",\"description\":\"Records of hospital visits including patient ID and length of stay.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"visit_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital visit.\"},{\"column\":\"column_2_id\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking to the patient records.\"},{\"column\":\"column_3_id\",\"column_name\":\"length_of_stay\",\"column_type\":\"xsd:integer\",\"column_description\":\"Duration of the hospital stay in days.\"}]},\"table_id3\":{\"name\":\"insurance_claims\",\"location\":\"C:/data/insurance_claims.csv\",\"sheet_name\":null,\"label\":\"Insurance Claims\",\"description\":\"Contains information about insurance claims made by patients.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"claim_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each insurance claim.\"},{\"column\":\"column_2_id\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking to the patient records.\"},{\"column\":\"column_3_id\",\"column_name\":\"claim_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount claimed by the patient.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file located at C:/data/patient_records.csv.\",\"Load the hospital visits table from the Excel file located at C:/data/hospital_visits.xlsx.\",\"Filter the patient records to include only patients aged 65 and above.\",\"Join the filtered patient records with the hospital visits table on the patient_id column.\",\"Calculate the average length of stay for each patient in the joined table.\",\"Aggregate the data to find the total number of visits per hospital.\",\"Sort the aggregated data by the total number of visits in descending order.\",\"Apply a log transformation to the average length of stay column in the joined table.\",\"Forecast the number of hospital visits for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hospital_visits.xlsx\",\"type\":\"xlsx\"},\"output\":\"hospital_visits\",\"comment\":\"Loading the hospital visits dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"age >= 65\"},\"output\":\"elderly_patients\",\"comment\":\"Filtering patient records to include only those aged 65 and above.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"elderly_patients\",\"hospital_visits\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_patient_visits\",\"comment\":\"Joining filtered patient records with hospital visits on patient_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_patient_visits\",\"columnName\":\"average_length_of_stay\",\"formula\":\"length_of_stay\"},\"output\":\"patient_visits_with_avg_stay\",\"comment\":\"Calculating the average length of stay for each patient.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_patient_visits\",\"groupBy\":\"hospital_id\",\"aggregations\":[{\"column\":\"visit_id\",\"name\":\"total_visits\",\"function\":\"count\"}]},\"output\":\"hospital_visit_counts\",\"comment\":\"Aggregating data to find the total number of visits per hospital.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"hospital_visit_counts\",\"sortBy\":\"total_visits\",\"order\":\"desc\"},\"output\":\"sorted_hospital_visits\",\"comment\":\"Sorting the aggregated data by total number of visits in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"patient_visits_with_avg_stay\",\"transformations\":[{\"columns\":[\"average_length_of_stay\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_patient_visits\",\"comment\":\"Applying a log transformation to the average length of stay column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"hospital_visits\",\"columns\":[\"visit_id\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"visit_date\"}},\"output\":\"visit_forecast\",\"comment\":\"Forecasting the number of hospital visits for the next 6 months using Holt-Winters.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of each product including product ID, name, and category.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"product_name\",\"column_name\":\"PRODUCT_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"category\",\"column_name\":\"CATEGORY\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]},\"table_id2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including product ID, target audience size, and purchase frequency.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"target_audience_size\",\"column_name\":\"TARGET_AUDIENCE_SIZE\",\"column_type\":\"xsd:integer\",\"column_description\":\"Size of the target audience for the product.\"},{\"column\":\"average_purchase_frequency\",\"column_name\":\"AVERAGE_PURCHASE_FREQUENCY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Average frequency at which the target audience purchases the product.\"},{\"column\":\"date\",\"column_name\":\"DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the market research entry.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col1\",\"column_name\":\"COL1\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"},{\"column\":\"col2\",\"column_name\":\"COL2\",\"column_type\":\"xsd:integer\",\"column_description\":\"Another irrelevant column.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel file.\",\"Filter the market research data to include only entries from the last year.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Calculate the potential market size by multiplying the target audience size by the average purchase frequency.\",\"Aggregate the joined data to find the total potential revenue for each product category.\",\"Sort the aggregated data by total potential revenue in descending order.\",\"Identify the top 5 product categories with the highest potential revenue.\",\"Apply a log transformation to the potential revenue to stabilize variance.\",\"Forecast the potential revenue for the next quarter using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading the market research data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"`DATE` >= '2022-01-01'\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only entries from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"PRODUCT_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_market_size\",\"formula\":\"`TARGET_AUDIENCE_SIZE` * `AVERAGE_PURCHASE_FREQUENCY`\"},\"output\":\"data_with_market_size\",\"comment\":\"Calculating potential market size by multiplying target audience size by average purchase frequency.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_market_size\",\"groupBy\":\"CATEGORY\",\"aggregations\":[{\"column\":\"potential_market_size\",\"name\":\"total_potential_revenue\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find total potential revenue for each product category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_potential_revenue\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total potential revenue in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_data\",\"query\":\"index < 5\"},\"output\":\"top_5_categories\",\"comment\":\"Identifying the top 5 product categories with the highest potential revenue.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"top_5_categories\",\"transformations\":[{\"columns\":[\"total_potential_revenue\"],\"operation\":\"log_transform\"}]},\"output\":\"log_transformed_data\",\"comment\":\"Applying a log transformation to the potential revenue to stabilize variance.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"log_transformed_data\",\"columns\":[\"total_potential_revenue\"],\"forecastParameters\":{\"forecastPeriod\":3,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"DATE\"}},\"output\":\"forecasted_revenue\",\"comment\":\"Forecasting the potential revenue for the next quarter using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy usage data with timestamps and usage values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy usage record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy used in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Weather Data\",\"description\":\"This table contains weather information including temperature and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature on the given date.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from the last year.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing energy usage by temperature.\",\"Aggregate the data by month to calculate the total energy usage and average temperature.\",\"Sort the aggregated data by total energy usage in descending order.\",\"Apply a rolling mean transformation to the energy usage column with a window of 3 months.\",\"Forecast the energy usage for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp` >= '2022-01-01'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering energy consumption data to include only records from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"timestamp\",\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered energy consumption data with weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`usage` / `temperature`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating energy efficiency by dividing energy usage by temperature.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"month\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_energy_usage\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating data by month to calculate total energy usage and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy_usage\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total energy usage in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"transformations\":[{\"columns\":[\"total_energy_usage\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":3}}]},\"output\":\"rolling_mean_data\",\"comment\":\"Applying a rolling mean transformation to the energy usage column with a window of 3 months.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"rolling_mean_data\",\"columns\":[\"total_energy_usage\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"month\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the energy usage for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'standardize' operation do?","output":"The 'standardize' operation standardizes data by subtracting the mean and dividing by the standard deviation."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_records\",\"location\":\"C:/data/legal/case_records.csv\",\"sheet_name\":null,\"label\":\"Case Records\",\"description\":\"A table containing records of legal cases, including case ID, lawyer ID, and case year.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"case_year\",\"column_type\":\"xsd:integer\",\"column_description\":\"Year when the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyer_details\",\"location\":\"C:/data/legal/lawyer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Lawyer Details\",\"description\":\"A table containing details of lawyers, including lawyer ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedule\",\"location\":\"C:/data/legal/court_schedule.csv\",\"sheet_name\":null,\"label\":\"Court Schedule\",\"description\":\"A table containing the schedule of court hearings, including date and case ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"hearing_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court hearing.\"},{\"column\":\"col_2\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the case being heard.\"}]}},\"instructions\":[\"Load the 'case_records' table from the CSV file located at C:/data/legal/case_records.csv.\",\"Load the 'lawyer_details' table from the Excel file located at C:/data/legal/lawyer_details.xlsx.\",\"Filter the 'case_records' table to include only cases from the year 2023.\",\"Join the filtered 'case_records' table with the 'lawyer_details' table using the 'lawyer_id' column.\",\"Aggregate the joined table to calculate the total number of cases handled by each lawyer.\",\"Sort the aggregated data by the total number of cases in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/case_records.csv\",\"type\":\"csv\"},\"output\":\"case_records\",\"comment\":\"Loading the case records dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/lawyer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"lawyer_details\",\"comment\":\"Loading the lawyer details dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_records\",\"query\":\"`case_year` == 2023\"},\"output\":\"filtered_case_records\",\"comment\":\"Filtering case records to include only cases from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_case_records\",\"lawyer_details\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_cases_lawyers\",\"comment\":\"Joining filtered case records with lawyer details using the lawyer_id column.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_cases_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating data to calculate the total number of cases handled by each lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"lawyer_case_counts\",\"sortBy\":\"total_cases\",\"order\":\"desc\"},\"output\":\"sorted_lawyer_case_counts\",\"comment\":\"Sorting the aggregated data by the total number of cases in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information about customers such as age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"purchase_history\",\"location\":\"C:/data/purchase_history.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Purchase History\",\"description\":\"Records of customer purchases including date, amount, and items purchased.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total amount spent in the transaction.\"}]},\"table_id3\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing_campaigns.csv\",\"sheet_name\":null,\"label\":\"Marketing Campaigns\",\"description\":\"Details of past marketing campaigns including target audience and results.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each campaign.\"},{\"column\":\"col_2\",\"column_name\":\"target_audience\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the target audience for the campaign.\"},{\"column\":\"col_3\",\"column_name\":\"results\",\"column_type\":\"xsd:string\",\"column_description\":\"Outcome of the campaign.\"}]}},\"instructions\":[\"Load the customer demographics data from the CSV file.\",\"Load the purchase history data from the Excel file.\",\"Filter the purchase history to include only transactions from the last year.\",\"Join the customer demographics data with the filtered purchase history on customer ID.\",\"Calculate the total spending for each customer.\",\"Sort the customers by their total spending in descending order.\",\"Identify the top 10% of customers based on total spending.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/purchase_history.xlsx\",\"type\":\"xlsx\"},\"output\":\"purchase_history\",\"comment\":\"Loading purchase history data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"purchase_history\",\"query\":\"`transaction_date` >= '2022-01-01'\"},\"output\":\"recent_purchases\",\"comment\":\"Filtering purchase history to include only transactions from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"customer_demographics\",\"recent_purchases\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"customer_purchase_data\",\"comment\":\"Joining customer demographics with recent purchase history on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"customer_purchase_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_spending\",\"function\":\"sum\"}]},\"output\":\"customer_spending\",\"comment\":\"Calculating total spending for each customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"customer_spending\",\"sortBy\":\"total_spending\",\"order\":\"desc\"},\"output\":\"sorted_customers\",\"comment\":\"Sorting customers by total spending in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_customers\",\"query\":\"index < 0.1 * len(sorted_customers)\"},\"output\":\"top_customers\",\"comment\":\"Identifying the top 10% of customers based on total spending.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption records with timestamps and usage values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"}]},\"table_id2\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy_prices.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains historical energy prices with timestamps and price values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy at the given timestamp.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Filter the energy consumption data to include only records where the usage is greater than 100 kWh.\",\"Aggregate the filtered data by date to calculate the total energy usage per day.\",\"Sort the aggregated data by total energy usage in descending order.\",\"Forecast the energy usage for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"usage > 100\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering energy consumption data to include only records with usage greater than 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_consumption\",\"groupBy\":\"timestamp\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_usage\",\"function\":\"sum\"}]},\"output\":\"daily_energy_usage\",\"comment\":\"Aggregating filtered data by date to calculate total energy usage per day.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"daily_energy_usage\",\"sortBy\":\"total_usage\",\"order\":\"desc\"},\"output\":\"sorted_daily_energy_usage\",\"comment\":\"Sorting aggregated data by total energy usage in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sorted_daily_energy_usage\",\"columns\":[\"total_usage\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"timestamp\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"energy_usage_forecast\",\"comment\":\"Forecasting energy usage for the next 30 days using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"This table contains detailed specifications for various products, including priority levels.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"priority\",\"column_type\":\"xsd:string\",\"column_description\":\"Priority level of the product.\"},{\"column\":\"col_3\",\"column_name\":\"specification\",\"column_type\":\"xsd:string\",\"column_description\":\"Detailed specification of the product.\"}]},\"table_id2\":{\"name\":\"supplier_information\",\"location\":\"C:/data/supplier_information.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"This table provides information about suppliers, including their IDs and associated costs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Product ID associated with the supplier.\"},{\"column\":\"col_3\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost associated with the product from this supplier.\"}]},\"table_id3\":{\"name\":\"market_analysis\",\"location\":\"C:/data/market_analysis.xlsx\",\"sheet_name\":\"Analysis\",\"label\":\"Market Analysis\",\"description\":\"Contains market analysis data which is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"market_trend\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of market trends.\"},{\"column\":\"col_B\",\"column_name\":\"analysis_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the market analysis.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a high priority.\",\"Join the filtered product specifications with the supplier information table.\",\"Aggregate the joined table to calculate the total cost per supplier.\",\"Sort the aggregated data by total cost in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"priority == 'high'\"},\"output\":\"high_priority_products\",\"comment\":\"Filtering products to include only those with high priority.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_information.csv\",\"type\":\"csv\"},\"output\":\"supplier_information\",\"comment\":\"Loading supplier information from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_priority_products\",\"supplier_information\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_product_supplier\",\"comment\":\"Joining high priority products with supplier information based on product ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_product_supplier\",\"groupBy\":\"supplier_id\",\"aggregations\":[{\"column\":\"cost\",\"name\":\"total_cost\",\"function\":\"sum\"}]},\"output\":\"total_cost_per_supplier\",\"comment\":\"Aggregating data to calculate the total cost per supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"total_cost_per_supplier\",\"sortBy\":\"total_cost\",\"order\":\"desc\"},\"output\":\"sorted_supplier_costs\",\"comment\":\"Sorting suppliers by total cost in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid expression using 'is not' operator.","output":"'col1 is not col2' // Error: 'is not' operator is not supported in Tamarind expressions."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if the table specified in 'identifyTable' does not exist?","output":"If the table does not exist, an error is raised."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid column name usage with single quotes.","output":"'a column name' + col2' // Error: Column names must be enclosed in backticks, not single quotes."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the 'isolation_forest' algorithm used for?","output":"Use 'isolation_forest' for anomaly detection in time series data by isolating outliers."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"A table containing details of all shipments including shipment ID, status, and warehouse ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse associated with the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_data\",\"location\":\"C:/logistics/warehouse_data.csv\",\"sheet_name\":null,\"label\":\"Warehouse Data\",\"description\":\"A table containing information about warehouse locations including warehouse ID and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse.\"}]},\"table_id3\":{\"name\":\"transport_data\",\"location\":\"C:/logistics/transport_data.csv\",\"sheet_name\":null,\"label\":\"Transport Data\",\"description\":\"A table containing information about transport vehicles and routes.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"vehicle_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transport vehicle.\"},{\"column\":\"col_2\",\"column_name\":\"route\",\"column_type\":\"xsd:string\",\"column_description\":\"Route taken by the transport vehicle.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of all shipments.\",\"Filter the shipment data to include only shipments with a status of 'Delivered'.\",\"Load the table 'warehouse_data' which contains information about warehouse locations.\",\"Join the filtered shipment data with the warehouse data on the 'warehouse_id' column.\",\"Aggregate the joined data to calculate the total number of shipments per warehouse.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"status == 'Delivered'\"},\"output\":\"delivered_shipments\",\"comment\":\"Filtering the shipment data to include only delivered shipments.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/logistics/warehouse_data.csv\",\"type\":\"csv\"},\"output\":\"warehouse_data\",\"comment\":\"Loading the warehouse data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"delivered_shipments\",\"warehouse_data\"],\"joinOn\":[\"warehouse_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the delivered shipments with warehouse data on warehouse_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"warehouse_id\",\"aggregations\":[{\"column\":\"shipment_id\",\"name\":\"total_shipments\",\"function\":\"count\"}]},\"output\":\"shipments_per_warehouse\",\"comment\":\"Aggregating the joined data to calculate the total number of shipments per warehouse.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'standardize' operation be used?","output":"Use 'standardize' when you need to normalize data to have zero mean and unit variance, which is useful for algorithms that assume normally distributed data, such as PCA or linear regression."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the 'holt_winters' algorithm used for?","output":"Use 'holt_winters' for forecasting time series with trend and seasonality. It requires 'frequency' and 'seasonal_periods'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data with timestamps and usage values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature recorded on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The humidity level recorded on the given date.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Aggregate the data to calculate the total energy consumption per month.\",\"Identify the peak energy consumption month.\",\"Calculate the average daily energy consumption for the peak month.\",\"Forecast the energy consumption for the next 3 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"energy_2023\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_2023\",\"groupBy\":\"timestamp.dt.month\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_monthly_usage\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_2023\",\"comment\":\"Aggregating the data to calculate the total energy consumption per month.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_energy_2023\",\"sortBy\":\"total_monthly_usage\",\"order\":\"desc\"},\"output\":\"sorted_monthly_energy\",\"comment\":\"Identifying the peak energy consumption month by sorting the data.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_monthly_energy\",\"columnName\":\"average_daily_usage\",\"formula\":\"total_monthly_usage / 30\"},\"output\":\"peak_month_daily_average\",\"comment\":\"Calculating the average daily energy consumption for the peak month.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"energy_2023\",\"columns\":[\"usage\"],\"forecastParameters\":{\"forecastPeriod\":3,\"frequency\":\"MS\",\"dateColumn\":\"timestamp\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"confidenceInterval\":95}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the energy consumption for the next 3 months using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains the energy consumption data for the building, including date, total energy consumption, and building area.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"total_energy\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total energy consumed by the building on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"building_area\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total area of the building in square meters.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including date, temperature, and humidity for the year 2023.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity on the given date.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for the building, including date and maintenance type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the scheduled maintenance.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of maintenance scheduled.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from the year 2023.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing total energy consumption by the building area.\",\"Aggregate the joined data by month to calculate the average energy efficiency.\",\"Sort the aggregated data by average energy efficiency in descending order.\",\"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered energy consumption data with the weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`total_energy` / `building_area`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating a new column for energy efficiency by dividing total energy consumption by the building area.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"energy_efficiency\",\"name\":\"avg_energy_efficiency\",\"function\":\"mean\"}]},\"output\":\"monthly_efficiency\",\"comment\":\"Aggregating the joined data by month to calculate the average energy efficiency.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_efficiency\",\"sortBy\":\"avg_energy_efficiency\",\"order\":\"desc\"},\"output\":\"sorted_efficiency\",\"comment\":\"Sorting the aggregated data by average energy efficiency in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"columns\":[\"total_energy\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/finance/transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with details such as account ID, transaction amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"account_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the account associated with the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date on which the transaction occurred.\"}]},\"table_id2\":{\"name\":\"account_details\",\"location\":\"C:/data/finance/accounts.xlsx\",\"sheet_name\":\"Details\",\"label\":\"Account Details\",\"description\":\"A table containing details about each account, including account holder name and account type.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"account_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the account.\"},{\"column\":\"col_B\",\"column_name\":\"account_holder\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the account holder.\"},{\"column\":\"col_C\",\"column_name\":\"account_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of the account, such as savings or checking.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at 'C:/data/finance/transactions.csv'.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Aggregate the filtered transactions by 'account_id' to calculate the total transaction amount for each account.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_transactions\",\"groupBy\":\"account_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_amount\",\"function\":\"sum\"}]},\"output\":\"aggregated_transactions\",\"comment\":\"Aggregating the filtered transactions by account_id to calculate the total transaction amount for each account.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the purpose of the 'parameters' field in a Tamarind workflow action?","output":"The 'parameters' field contains the input tables and transformation options required for the action to execute correctly."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'aggregateData' action.","output":"{ \"name\": \"aggregateData\", \"parameters\": { \"table\": \"sales\", \"groupBy\": \"region\", \"aggregations\": [ { \"column\": \"revenue\", \"name\": \"total_revenue\", \"function\": \"sum\" } ] }, \"output\": \"sales_summary\" }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_details\",\"location\":\"C:/legal_data/case_details.csv\",\"sheet_name\":null,\"label\":\"Case Details\",\"description\":\"This table contains details of legal cases including case ID, lawyer ID, and case year.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"case_year\",\"column_type\":\"xsd:integer\",\"column_description\":\"Year the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyer_info\",\"location\":\"C:/legal_data/lawyer_info.csv\",\"sheet_name\":null,\"label\":\"Lawyer Information\",\"description\":\"This table contains information about lawyers including lawyer ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedule\",\"location\":\"C:/legal_data/court_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Court Schedule\",\"description\":\"This table contains the schedule of court hearings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"hearing_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hearing.\"},{\"column\":\"col_2\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court hearing.\"}]}},\"instructions\":[\"Load the 'case_details' table from the CSV file.\",\"Filter the 'case_details' table to include only cases from 2023.\",\"Join the filtered 'case_details' with the 'lawyer_info' table on 'lawyer_id'.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_details.csv\",\"type\":\"csv\"},\"output\":\"case_details\",\"comment\":\"Loading the case details from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_details\",\"query\":\"case_year == 2023\"},\"output\":\"filtered_cases_2023\",\"comment\":\"Filtering cases to include only those from the year 2023.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyer_info.csv\",\"type\":\"csv\"},\"output\":\"lawyer_info\",\"comment\":\"Loading the lawyer information from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_cases_2023\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"cases_with_lawyers\",\"comment\":\"Joining filtered cases with lawyer information on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"cases_with_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating to find the total number of cases handled by each lawyer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/hr/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/hr/department_data.csv\",\"sheet_name\":null,\"label\":\"Department Data\",\"description\":\"Contains information about departments including their IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"project_data\",\"location\":\"C:/data/hr/project_data.csv\",\"sheet_name\":null,\"label\":\"Project Data\",\"description\":\"Contains information about projects including project IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"project_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the project.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/hr/employee_data.csv.\",\"Filter the employee data to include only those with a salary greater than 50,000.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Aggregate the joined data to calculate the average salary per department.\",\"Sort the aggregated data by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading the employee data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employees with a salary greater than 50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/department_data.csv\",\"type\":\"csv\"},\"output\":\"department_data\",\"comment\":\"Loading the department data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employee_data\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating the average salary per department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by average salary in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/marketing/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"This table contains information about customer demographics including age, income, and city.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"income\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual income of the customer.\"},{\"column\":\"col_4\",\"column_name\":\"city\",\"column_type\":\"xsd:string\",\"column_description\":\"City where the customer resides.\"}]},\"table_id2\":{\"name\":\"product_sales\",\"location\":\"C:/data/marketing/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"This table contains sales data for different products.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Filter the customers who are aged between 25 and 40.\",\"Aggregate the filtered data to find the average income by city.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading the customer demographics table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_demographics\",\"query\":\"`age` >= 25 and `age` <= 40\"},\"output\":\"filtered_customers\",\"comment\":\"Filtering customers who are aged between 25 and 40.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_customers\",\"groupBy\":\"city\",\"aggregations\":[{\"column\":\"income\",\"name\":\"average_income\",\"function\":\"mean\"}]},\"output\":\"average_income_by_city\",\"comment\":\"Aggregating the filtered data to find the average income by city.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"This table contains all financial transactions including date, amount, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"The category of the transaction.\"}]},\"table_id2\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains irrelevant data not related to financial transactions.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the financial transactions table from the CSV file located at C:/data/financial_transactions.csv.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Calculate the total amount for each category of transactions.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"`amount` > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_transactions\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_amount\",\"function\":\"sum\"}]},\"output\":\"category_totals\",\"comment\":\"Calculating the total amount for each category of transactions.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid column name starting with a number.","output":"'123column + col2' // Error: Column names starting with numbers must be enclosed in backticks (`123column`)."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if the query provided in 'filterData' is invalid?","output":"If the query is invalid, an error is raised."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'min_max_scale' operation do?","output":"The 'min_max_scale' operation scales data between a specified range. It requires 'parameters.range', e.g., [0,1]."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan lists and tuples be used in expressions?","output":"Yes, lists and tuples can be used, for example: 'col1 in [1, 2, 3]' or 'col1 not in (4, 5, 6)'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains records of energy consumption with columns for date, usage, and other relevant details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location where the energy consumption was recorded.\"}]},\"table_id2\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy_prices.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains historical energy prices with columns for date, price, and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kWh.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region where the energy price was recorded.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at 'C:/data/energy_consumption.csv'.\",\"Filter the energy consumption data to include only records where the usage is greater than 100 kWh.\",\"Aggregate the filtered data by date to calculate the total energy usage per day.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"usage > 100\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records where usage is greater than 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_consumption\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_daily_usage\",\"function\":\"sum\"}]},\"output\":\"daily_energy_usage\",\"comment\":\"Aggregating the filtered data by date to calculate the total energy usage per day.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'resample' operation be used?","output":"Use 'resample' when you need to aggregate data at a different time frequency, such as converting hourly data to daily data."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"Contains customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"feedback\",\"column_type\":\"xsd:string\",\"column_description\":\"Customer feedback comments.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details Data\",\"description\":\"Contains detailed information about customers.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant data column.\"}]}},\"instructions\":[\"Load the customer feedback data from the CSV file.\",\"Load the customer details data from the Excel file.\",\"Filter the feedback data to include only feedback with a rating of 4 or higher.\",\"Join the filtered feedback data with the customer details on the customer ID.\",\"Aggregate the joined data to calculate the average rating per customer.\",\"Sort the aggregated data by average rating in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"filtered_feedback\",\"comment\":\"Filtering feedback data to include only ratings of 4 or higher.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating average rating per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting data by average rating in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow should a column name starting with a number be written?","output":"A column name starting with a number should be enclosed in backticks, such as '`123column` + col2'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid tuple membership check.","output":"'col1 not in (4, 5, 6)'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"This table contains customer feedback with satisfaction scores and product information.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_3_id\",\"column_name\":\"satisfaction_score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer satisfaction score ranging from 1 to 5.\"},{\"column\":\"column_4_id\",\"column_name\":\"feedback_text\",\"column_type\":\"xsd:string\",\"column_description\":\"Textual feedback provided by the customer.\"}]},\"table_id2\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.xlsx\",\"sheet_name\":\"Products\",\"label\":\"Product Catalog\",\"description\":\"This table contains information about products available in the catalog.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]}},\"instructions\":[\"Load the customer feedback data from the CSV file located at C:/data/customer_feedback.csv.\",\"Filter the feedback data to include only entries where the satisfaction score is below 3.\",\"Aggregate the filtered data to count the number of negative feedbacks per product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading the customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"satisfaction_score < 3\"},\"output\":\"negative_feedback\",\"comment\":\"Filtering feedback data to include only entries with satisfaction scores below 3.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"negative_feedback\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"satisfaction_score\",\"name\":\"negative_feedback_count\",\"function\":\"count\"}]},\"output\":\"negative_feedback_summary\",\"comment\":\"Aggregating the filtered data to count the number of negative feedbacks per product.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains energy consumption data for the building, including timestamps and energy usage values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_occupancy\",\"location\":\"C:/data/building_occupancy.csv\",\"sheet_name\":null,\"label\":\"Building Occupancy Data\",\"description\":\"This table contains data on the number of occupants in the building per month.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"month\",\"column_type\":\"xsd:string\",\"column_description\":\"The month for which the occupancy data is recorded.\"},{\"column\":\"col_2\",\"column_name\":\"occupants\",\"column_type\":\"xsd:integer\",\"column_description\":\"The number of occupants in the building for the given month.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column for demonstration purposes.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the building occupancy data from the CSV file.\",\"Filter the data to include only entries from the year 2023.\",\"Aggregate the data to calculate the total energy consumption per month.\",\"Join the aggregated data with the building occupancy data on the month column.\",\"Calculate the energy consumption per occupant by dividing the total energy consumption by the number of occupants.\",\"Sort the resulting data by energy consumption per occupant in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_occupancy.csv\",\"type\":\"csv\"},\"output\":\"building_occupancy\",\"comment\":\"Loading the building occupancy data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only entries from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_consumption\",\"groupBy\":\"timestamp.dt.month\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Aggregating the data to calculate the total energy consumption per month.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"monthly_energy_consumption\",\"building_occupancy\"],\"joinOn\":[\"month\"],\"joinType\":\"inner\"},\"output\":\"energy_occupancy_data\",\"comment\":\"Joining the aggregated energy data with the building occupancy data on the month column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"energy_occupancy_data\",\"columnName\":\"energy_per_occupant\",\"formula\":\"total_energy_consumption / occupants\"},\"output\":\"energy_per_occupant_data\",\"comment\":\"Calculating the energy consumption per occupant by dividing the total energy consumption by the number of occupants.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"energy_per_occupant_data\",\"sortBy\":\"energy_per_occupant\",\"order\":\"desc\"},\"output\":\"sorted_energy_per_occupant_data\",\"comment\":\"Sorting the resulting data by energy consumption per occupant in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"Contains detailed records of employees including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_info\",\"location\":\"C:/data/department_info.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Department Information\",\"description\":\"Contains information about each department including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant data column.\"}]}},\"instructions\":[\"Load the employee records from the CSV file located at C:/data/employee_records.csv.\",\"Load the department information from the Excel file located at C:/data/department_info.xlsx.\",\"Filter the employee records to include only those with a salary greater than 50,000.\",\"Join the filtered employee records with the department information on the department_id column.\",\"Calculate a new column 'annual_bonus' as 10% of the salary for each employee.\",\"Aggregate the data to find the total salary and total annual bonus for each department.\",\"Sort the aggregated data by total salary in descending order.\",\"Apply a standardization transformation to the total salary and total annual bonus columns.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading employee records from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_info.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_info\",\"comment\":\"Loading department information from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employees with a salary greater than 50,000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_salary_employees\",\"department_info\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"employee_department_data\",\"comment\":\"Joining filtered employees with department information on department_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"employee_department_data\",\"columnName\":\"annual_bonus\",\"formula\":\"salary * 0.10\"},\"output\":\"employee_with_bonus\",\"comment\":\"Calculating annual bonus as 10% of the salary.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"employee_with_bonus\",\"groupBy\":\"department_id\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"total_salary\",\"function\":\"sum\"},{\"column\":\"annual_bonus\",\"name\":\"total_annual_bonus\",\"function\":\"sum\"}]},\"output\":\"department_aggregates\",\"comment\":\"Aggregating total salary and annual bonus by department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"department_aggregates\",\"sortBy\":\"total_salary\",\"order\":\"desc\"},\"output\":\"sorted_department_aggregates\",\"comment\":\"Sorting departments by total salary in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_department_aggregates\",\"transformations\":[{\"columns\":[\"total_salary\",\"total_annual_bonus\"],\"operation\":\"standardize\"}]},\"output\":\"standardized_department_aggregates\",\"comment\":\"Standardizing total salary and total annual bonus columns.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"Table containing details of financial transactions including transaction amount and currency code.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"transaction_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"Currency code of the transaction amount.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of the transaction.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/exchange_rates.xlsx\",\"sheet_name\":\"Rates\",\"label\":\"Exchange Rates\",\"description\":\"Table containing exchange rates for various currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"Currency code for which the exchange rate is provided.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"A table containing data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file.\",\"Load the 'exchange_rates' table from the Excel file.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Join the filtered transactions with the 'exchange_rates' table on the 'currency_code' column.\",\"Calculate a new column 'amount_in_usd' by multiplying 'transaction_amount' with 'exchange_rate'.\",\"Aggregate the data by 'transaction_type' to find the total 'amount_in_usd' for each type.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"transaction_amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with exchange rates on currency code.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"amount_in_usd\",\"formula\":\"transaction_amount * exchange_rate\"},\"output\":\"transactions_with_usd\",\"comment\":\"Calculating amount in USD by multiplying transaction amount with exchange rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_usd\",\"groupBy\":\"transaction_type\",\"aggregations\":[{\"column\":\"amount_in_usd\",\"name\":\"total_amount_usd\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by transaction type to find the total amount in USD for each type.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat general functions are supported in Tamarind expressions?","output":"Supported general functions include 'abs', 'round', 'min', and 'max'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing all financial transactions with details such as transaction ID, customer ID, amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"customer_info\",\"location\":\"C:/data/customer_info.csv\",\"sheet_name\":null,\"label\":\"Customer Information\",\"description\":\"A table containing customer details such as customer ID, name, and contact information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_info\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact information of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"A table containing data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column for this workflow.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Join the filtered transactions with the 'customer_info' table on the 'customer_id' column.\",\"Calculate a new column 'transaction_fee' as 2% of the 'amount' column in the joined table.\",\"Aggregate the data by 'customer_id' to find the total transaction amount and total transaction fee for each customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_info.csv\",\"type\":\"csv\"},\"output\":\"customer_info\",\"comment\":\"Loading the customer information dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_info\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer information on customer_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"transaction_fee\",\"formula\":\"amount * 0.02\"},\"output\":\"joined_data_with_fee\",\"comment\":\"Calculating transaction fee as 2% of the amount.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_fee\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"},{\"column\":\"transaction_fee\",\"name\":\"total_transaction_fee\",\"function\":\"sum\"}]},\"output\":\"customer_aggregated_data\",\"comment\":\"Aggregating data by customer_id to find total transaction amount and total transaction fee.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"This table contains details of shipments including shipment ID, origin, destination, and weight.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"origin\",\"column_type\":\"xsd:string\",\"column_description\":\"The origin location of the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"The destination location of the shipment.\"},{\"column\":\"col_4\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The weight of the shipment in kilograms.\"}]},\"table_id2\":{\"name\":\"vehicle_data\",\"location\":\"C:/data/logistics/vehicle_data.csv\",\"sheet_name\":null,\"label\":\"Vehicle Data\",\"description\":\"This table contains details of vehicles including vehicle ID, type, and capacity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"vehicle_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each vehicle.\"},{\"column\":\"col_2\",\"column_name\":\"type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of vehicle.\"},{\"column\":\"col_3\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The capacity of the vehicle in kilograms.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of shipments including shipment ID, origin, destination, and weight.\",\"Filter the shipment data to include only shipments with a weight greater than 500 kg.\",\"Sort the filtered shipment data by destination in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 500\"},\"output\":\"filtered_shipments\",\"comment\":\"Filtering shipments to include only those with a weight greater than 500 kg.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"filtered_shipments\",\"sortBy\":\"destination\",\"order\":\"asc\"},\"output\":\"sorted_shipments\",\"comment\":\"Sorting the filtered shipments by destination in ascending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid math function using trigonometry.","output":"'sin(col1) + cos(col2)'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'applyTransformations' action with multiple transformations.","output":"{ \"name\": \"applyTransformations\", \"parameters\": { \"table\": \"sales_data\", \"transformations\": [ { \"columns\": [\"revenue\"], \"operation\": \"log_transform\" }, { \"columns\": [\"revenue\"], \"operation\": \"min_max_scale\", \"parameters\": { \"range\": [0, 1] } } ] }, \"output\": \"transformed_sales\" }"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'forecastData' action using 'isolation_forest'.","output":"{ \"name\": \"forecastData\", \"parameters\": { \"table\": \"network_traffic\", \"columns\": [\"traffic_volume\"], \"forecastParameters\": { \"algorithm\": \"isolation_forest\" }, \"output\": \"anomaly_detection\" } }"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'applyTimeSeriesTransformations' action with a non-existent column.","output":"{ \"name\": \"applyTimeSeriesTransformations\", \"parameters\": { \"table\": \"temperature_data\", \"transformations\": [ { \"columns\": [\"non_existent_column\"], \"operation\": \"rolling_mean\", \"parameters\": { \"window\": 7 } } ] }, \"output\": \"smoothed_temperature\" } // Error: 'non_existent_column' does not exist in the table."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Rating given by the customer, ranging from 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"feedback\",\"column_type\":\"xsd:string\",\"column_description\":\"Textual feedback provided by the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table is not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant data column.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Load the customer details table from the Excel file.\",\"Filter the feedback table to include only feedback with a rating of 4 or above.\",\"Join the filtered feedback with the customer details on customer ID.\",\"Aggregate the joined table to calculate the average rating per customer.\",\"Sort the aggregated data by average rating in descending order.\",\"Identify the top 10 customers based on average rating.\",\"Calculate a new column to determine the difference between the highest and lowest ratings for each customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"filtered_feedback\",\"comment\":\"Filtering feedback to include only ratings of 4 or above.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating the average rating per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting data by average rating in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_data\",\"query\":\"index < 10\"},\"output\":\"top_customers\",\"comment\":\"Identifying the top 10 customers based on average rating.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"top_customers\",\"columnName\":\"rating_difference\",\"formula\":\"`average_rating` - `average_rating`\"},\"output\":\"top_customers_with_difference\",\"comment\":\"Calculating the difference between highest and lowest ratings for each customer.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/top_customers.csv\",\"type\":\"csv\"},\"output\":\"top_customers_with_difference\",\"comment\":\"Saving the final table with top customers and their rating differences to a new CSV file.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"Contains customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comment\",\"column_type\":\"xsd:string\",\"column_description\":\"Customer feedback comments.\"}]},\"table_id2\":{\"name\":\"customer_transactions\",\"location\":\"C:/data/customer_transactions.xlsx\",\"sheet_name\":\"Transactions\",\"label\":\"Customer Transaction Data\",\"description\":\"Contains transaction details including quantity and price.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of items purchased.\"},{\"column\":\"col_3\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price per item.\"}]},\"table_id3\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.xlsx\",\"sheet_name\":\"Demographics\",\"label\":\"Customer Demographic Data\",\"description\":\"Contains demographic information about customers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the customer.\"}]}},\"instructions\":[\"Load the customer feedback data from the CSV file.\",\"Load the customer transaction data from the Excel file.\",\"Filter the feedback data to include only feedback with a rating of 4 or higher.\",\"Join the filtered feedback data with the transaction data on the customer ID.\",\"Calculate a new column in the joined table for the total transaction amount by multiplying quantity and price.\",\"Aggregate the joined table to find the average transaction amount per customer.\",\"Sort the aggregated data by average transaction amount in descending order.\",\"Identify the top 10 customers based on the average transaction amount.\",\"Load the customer demographic data from another Excel sheet.\",\"Join the top 10 customers data with the demographic data to enrich the customer profiles.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_transactions.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_transactions\",\"comment\":\"Loading customer transaction data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback data to include only feedback with a rating of 4 or higher.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"customer_transactions\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered feedback data with transaction data on customer ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"total_transaction_amount\",\"formula\":\"quantity * price\"},\"output\":\"joined_data_with_total\",\"comment\":\"Calculating total transaction amount by multiplying quantity and price.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_total\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"total_transaction_amount\",\"name\":\"average_transaction_amount\",\"function\":\"mean\"}]},\"output\":\"average_transaction_per_customer\",\"comment\":\"Aggregating data to find the average transaction amount per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_transaction_per_customer\",\"sortBy\":\"average_transaction_amount\",\"order\":\"desc\"},\"output\":\"sorted_customers\",\"comment\":\"Sorting data by average transaction amount in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_customers\",\"query\":\"index < 10\"},\"output\":\"top_10_customers\",\"comment\":\"Identifying the top 10 customers based on average transaction amount.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographic data from an Excel sheet.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"top_10_customers\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"enriched_customer_profiles\",\"comment\":\"Joining top 10 customers data with demographic data to enrich customer profiles.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid math function using inverse trigonometry.","output":"'arcsin(col1) + arccos(col2)'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if an operation references a non-existent column?","output":"If an operation requires a column that does not exist, an error is raised."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/HR/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"A table containing records of employees including their salary, hire date, and other personal details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Full name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"hire_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date the employee was hired.\"}]},\"table_id2\":{\"name\":\"department_info\",\"location\":\"C:/HR/department_info.xlsx\",\"sheet_name\":\"Departments\",\"label\":\"Department Information\",\"description\":\"Contains information about different departments within the company.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_B\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]}},\"instructions\":[\"Load the employee records from the CSV file located in the HR directory.\",\"Filter the employee records to include only those with a salary greater than 50,000.\",\"Sort the filtered employee records by their hire date in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records from the CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employee records to include only those with a salary greater than 50,000.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"high_salary_employees\",\"sortBy\":\"hire_date\",\"order\":\"asc\"},\"output\":\"sorted_high_salary_employees\",\"comment\":\"Sorting the filtered employee records by hire date in ascending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"A table containing the manufacturing schedule for the month, including item IDs, priority levels, and required quantities.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each item.\"},{\"column\":\"col_2\",\"column_name\":\"priority\",\"column_type\":\"xsd:string\",\"column_description\":\"Priority level of the item, e.g., high, medium, low.\"},{\"column\":\"col_3\",\"column_name\":\"required_quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the item required for production.\"},{\"column\":\"col_4\",\"column_name\":\"production_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Scheduled date for production.\"}]},\"table_id2\":{\"name\":\"inventory\",\"location\":\"C:/data/manufacturing/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory Levels\",\"description\":\"A table showing current stock levels for each item in the inventory.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each item.\"},{\"column\":\"col_2\",\"column_name\":\"current_stock\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the item.\"}]},\"table_id3\":{\"name\":\"employee_records\",\"location\":\"C:/data/manufacturing/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"A table containing records of employees, including their roles and shifts.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"role\",\"column_type\":\"xsd:string\",\"column_description\":\"Role of the employee in the company.\"},{\"column\":\"col_3\",\"column_name\":\"shift\",\"column_type\":\"xsd:string\",\"column_description\":\"Shift assigned to the employee.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the manufacturing schedule for the month.\",\"Filter the production schedule to include only high-priority items.\",\"Join the filtered schedule with the 'inventory' table to check stock levels.\",\"Calculate the required raw materials by subtracting current stock from required quantities.\",\"Sort the final table by production date to prioritize manufacturing.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the manufacturing schedule for the month from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"priority == 'high'\"},\"output\":\"high_priority_schedule\",\"comment\":\"Filtering the production schedule to include only high-priority items.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/inventory.csv\",\"type\":\"csv\"},\"output\":\"inventory\",\"comment\":\"Loading the current inventory levels from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_priority_schedule\",\"inventory\"],\"joinOn\":[\"item_id\"],\"joinType\":\"inner\"},\"output\":\"schedule_with_inventory\",\"comment\":\"Joining the filtered schedule with inventory to check stock levels.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"schedule_with_inventory\",\"columnName\":\"required_raw_materials\",\"formula\":\"`required_quantity` - `current_stock`\"},\"output\":\"schedule_with_materials\",\"comment\":\"Calculating the required raw materials by subtracting current stock from required quantities.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"schedule_with_materials\",\"sortBy\":\"production_date\",\"order\":\"asc\"},\"output\":\"final_production_plan\",\"comment\":\"Sorting the final table by production date to prioritize manufacturing.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Department Data\",\"description\":\"Contains information about departments including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/employee_data.csv.\",\"Load the department data from the Excel file located at C:/data/department_data.xlsx.\",\"Filter the employee data to include only employees with a salary greater than 50,000.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Calculate a new column in the joined table to determine the annual bonus as 10% of the salary.\",\"Sort the resulting table by the annual bonus in descending order.\",\"Aggregate the data by department to find the total salary and total bonus for each department.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading employee data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_data\",\"comment\":\"Loading department data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employees with a salary greater than 50,000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_salary_employees\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"employee_department_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"employee_department_data\",\"columnName\":\"annual_bonus\",\"formula\":\"salary * 0.10\"},\"output\":\"employee_with_bonus\",\"comment\":\"Calculating annual bonus as 10% of the salary.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"employee_with_bonus\",\"sortBy\":\"annual_bonus\",\"order\":\"desc\"},\"output\":\"sorted_employee_data\",\"comment\":\"Sorting employees by annual bonus in descending order.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sorted_employee_data\",\"groupBy\":\"department_id\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"total_salary\",\"function\":\"sum\"},{\"column\":\"annual_bonus\",\"name\":\"total_bonus\",\"function\":\"sum\"}]},\"output\":\"department_summary\",\"comment\":\"Aggregating data by department to find total salary and bonus.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_summary.csv\",\"type\":\"csv\"},\"output\":\"department_summary\",\"comment\":\"Saving the aggregated department summary to a CSV file.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'identifyTable' overwrite an existing table?","output":"Yes, if the table is already loaded, it is overwritten unless explicitly prevented."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains energy consumption data for the building, including timestamps and energy usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"}]},\"table_id2\":{\"name\":\"building_occupancy\",\"location\":\"C:/data/building_occupancy.csv\",\"sheet_name\":null,\"label\":\"Building Occupancy Data\",\"description\":\"This table contains data on the number of occupants in the building per month.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"month\",\"column_type\":\"xsd:string\",\"column_description\":\"The month of the record.\"},{\"column\":\"col_2\",\"column_name\":\"occupants\",\"column_type\":\"xsd:integer\",\"column_description\":\"The number of occupants in the building.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the building occupancy data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Aggregate the data to calculate the total energy consumption per month.\",\"Join the aggregated data with the building occupancy data on the month column.\",\"Calculate the energy consumption per occupant by dividing the total energy consumption by the number of occupants.\",\"Sort the resulting data by energy consumption per occupant in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_occupancy.csv\",\"type\":\"csv\"},\"output\":\"building_occupancy\",\"comment\":\"Loading the building occupancy data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"filtered_energy_2023\",\"comment\":\"Filtering the energy consumption data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_2023\",\"groupBy\":\"timestamp.dt.month\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy\",\"function\":\"sum\"}]},\"output\":\"monthly_energy\",\"comment\":\"Aggregating the data to calculate the total energy consumption per month.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"monthly_energy\",\"building_occupancy\"],\"joinOn\":[\"month\"],\"joinType\":\"inner\"},\"output\":\"energy_occupancy\",\"comment\":\"Joining the aggregated energy data with the building occupancy data on the month column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"energy_occupancy\",\"columnName\":\"energy_per_occupant\",\"formula\":\"total_energy / occupants\"},\"output\":\"energy_per_occupant_data\",\"comment\":\"Calculating the energy consumption per occupant by dividing the total energy consumption by the number of occupants.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"energy_per_occupant_data\",\"sortBy\":\"energy_per_occupant\",\"order\":\"desc\"},\"output\":\"sorted_energy_per_occupant\",\"comment\":\"Sorting the resulting data by energy consumption per occupant in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with columns for customer ID, feedback rating, and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"feedback_rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Rating given by the customer, ranging from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Additional comments provided by the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers, including their ID, name, and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"Table containing sales data for various products, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Load the customer details table from the Excel file.\",\"Filter the customer feedback to include only feedback with a rating of 4 or above.\",\"Join the filtered feedback with customer details using the customer ID.\",\"Aggregate the joined data to calculate the average feedback rating for each region.\",\"Sort the aggregated data by average feedback rating in descending order.\",\"Export the sorted data to a new CSV file for further analysis.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"feedback_rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback to include only ratings of 4 or above.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"feedback_with_details\",\"comment\":\"Joining filtered feedback with customer details using customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"feedback_with_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"feedback_rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_by_region\",\"comment\":\"Calculating the average feedback rating for each region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_rating_by_region\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_average_rating\",\"comment\":\"Sorting the data by average feedback rating in descending order.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sorted_average_rating.csv\",\"type\":\"csv\"},\"output\":\"sorted_average_rating\",\"comment\":\"Exporting the sorted data to a new CSV file for further analysis.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data with columns for date, usage, and other relevant metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location where the energy was consumed.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data with columns for date, temperature, and other weather-related metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature recorded on the date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity recorded on the date.\"}]},\"table_id3\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy_prices.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains historical energy prices with columns for date and price per kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the price record.\"},{\"column\":\"col_2\",\"column_name\":\"price_per_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kWh on the date.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Load the weather data from the Excel file located at C:/data/weather_data.xlsx.\",\"Filter the energy consumption data to include only records where the usage is greater than 100 kWh.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column in the joined table that represents the cost by multiplying usage by the rate of 0.15.\",\"Aggregate the data by date to calculate the total usage and average temperature.\",\"Sort the aggregated data by total usage in descending order.\",\"Apply a rolling mean transformation with a window of 7 days on the total usage column.\",\"Forecast the future energy usage for the next 30 days using the Holt-Winters method.\",\"Identify any outliers in the energy usage data using the Isolation Forest algorithm.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"usage > 100\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering energy consumption data to include only records where usage is greater than 100 kWh.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered energy consumption data with weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"cost\",\"formula\":\"usage * 0.15\"},\"output\":\"joined_data_with_cost\",\"comment\":\"Calculating the cost by multiplying usage by the rate of 0.15.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_cost\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_usage\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by date to calculate total usage and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_usage\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total usage in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"transformations\":[{\"columns\":[\"total_usage\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":7}}]},\"output\":\"transformed_data\",\"comment\":\"Applying a rolling mean transformation with a window of 7 days on the total usage column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_data\",\"columns\":[\"total_usage\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7}},\"output\":\"forecast_results\",\"comment\":\"Forecasting future energy usage for the next 30 days using the Holt-Winters method.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_data\",\"columns\":[\"total_usage\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"isolation_forest\"}},\"output\":\"outlier_detection_results\",\"comment\":\"Identifying any outliers in the energy usage data using the Isolation Forest algorithm.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains the daily production targets for each machine.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date for the production target.\"},{\"column\":\"col_2\",\"column_name\":\"machine_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The identifier for the machine.\"},{\"column\":\"col_3\",\"column_name\":\"production_target\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The production target for the machine on the given date.\"}]},\"table_id2\":{\"name\":\"machine_availability\",\"location\":\"C:/data/machine_availability.csv\",\"sheet_name\":null,\"label\":\"Machine Availability\",\"description\":\"This table contains the availability status of machines.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"machine_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The identifier for the machine.\"},{\"column\":\"col_2\",\"column_name\":\"availability_status\",\"column_type\":\"xsd:string\",\"column_description\":\"The availability status of the machine.\"}]},\"table_id3\":{\"name\":\"employee_schedule\",\"location\":\"C:/data/employee_schedule.csv\",\"sheet_name\":null,\"label\":\"Employee Schedule\",\"description\":\"This table contains the work schedule for employees.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The identifier for the employee.\"},{\"column\":\"col_2\",\"column_name\":\"shift\",\"column_type\":\"xsd:string\",\"column_description\":\"The shift assigned to the employee.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the daily production targets.\",\"Filter the production schedule to include only the targets for the current month.\",\"Join the filtered production schedule with the 'machine_availability' table on the 'machine_id' column.\",\"Aggregate the joined table to calculate the total production target per machine for the current month.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule table containing daily production targets.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"`date`.dt.month == pd.Timestamp.now().month\"},\"output\":\"current_month_schedule\",\"comment\":\"Filtering the production schedule to include only the targets for the current month.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/machine_availability.csv\",\"type\":\"csv\"},\"output\":\"machine_availability\",\"comment\":\"Loading the machine availability table.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"current_month_schedule\",\"machine_availability\"],\"joinOn\":[\"machine_id\"],\"joinType\":\"inner\"},\"output\":\"joined_schedule\",\"comment\":\"Joining the filtered production schedule with machine availability on the machine_id column.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_schedule\",\"groupBy\":\"machine_id\",\"aggregations\":[{\"column\":\"production_target\",\"name\":\"total_production_target\",\"function\":\"sum\"}]},\"output\":\"monthly_production_target\",\"comment\":\"Aggregating the joined table to calculate the total production target per machine for the current month.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'forecastData' action with an incorrect 'forecastPeriod'.","output":"{ \"name\": \"forecastData\", \"parameters\": { \"table\": \"climate_data\", \"columns\": [\"temperature\"], \"forecastParameters\": { \"forecastPeriod\": -5, \"frequency\": \"D\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 12 }, \"output\": \"climate_forecast\" } } // Error: 'forecastPeriod' must be greater than 0."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees, including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Department Data\",\"description\":\"Contains information about departments, including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"project_data\",\"location\":\"C:/data/project_data.xlsx\",\"sheet_name\":\"Projects\",\"label\":\"Project Data\",\"description\":\"Contains information about various projects, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"project_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the project.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/employee_data.csv.\",\"Filter the employee data to include only employees with a salary greater than $50,000.\",\"Load the department data from the Excel file located at C:/data/department_data.xlsx.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Aggregate the joined data to calculate the average salary per department.\",\"Sort the aggregated data by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading employee data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employees with a salary greater than $50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_data\",\"comment\":\"Loading department data from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employee_data\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"department_id\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating the average salary per department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by average salary in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nAre negative values allowed in expressions?","output":"Yes, negative values can be used in expressions, such as '-a'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Inventory\",\"description\":\"A table containing information about raw materials available in the inventory, including material ID, name, and quantity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"col_2\",\"column_name\":\"material_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the raw material.\"},{\"column\":\"col_3\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the raw material available.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"A table detailing the planned production schedule, including product ID, material ID, and required quantity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the raw material needed for production.\"},{\"column\":\"col_3\",\"column_name\":\"required_quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the material required for production.\"}]},\"table_id3\":{\"name\":\"employee_records\",\"location\":\"C:/data/manufacturing/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"A table containing records of employees, including employee ID, name, and department.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"employee_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the employee works.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains information about available raw materials.\",\"Load the table 'production_schedule' which contains the planned production schedule.\",\"Filter the 'raw_materials' table to include only materials with quantity greater than 100.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Aggregate the joined table to calculate the total quantity of materials needed for each product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials inventory from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"quantity > 100\"},\"output\":\"filtered_raw_materials\",\"comment\":\"Filtering raw materials to include only those with quantity greater than 100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_raw_materials\",\"production_schedule\"],\"joinOn\":[\"material_id\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered raw materials with the production schedule on material_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_table\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"required_quantity\",\"name\":\"total_material_needed\",\"function\":\"sum\"}]},\"output\":\"aggregated_materials\",\"comment\":\"Aggregating the joined table to calculate the total quantity of materials needed for each product.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_data.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"This table contains sales transactions including total revenue, total cost, and transaction dates.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"total_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total cost incurred for the transaction.\"},{\"column\":\"col_5\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the transaction.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customer_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Data\",\"description\":\"This table contains customer information including customer ID and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"This table contains information about inventory levels and product details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:integer\",\"column_description\":\"Current stock level of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\",\"Load the customer data from the Excel file located at C:/data/customer_data.xlsx.\",\"Filter the sales data to include only transactions with a total revenue greater than 1000.\",\"Join the filtered sales data with customer data on the customer_id column using an inner join.\",\"Aggregate the joined data by region to calculate the total sales revenue.\",\"Sort the aggregated data by total sales in descending order.\",\"Calculate a new column 'profit_margin' by subtracting total cost from total revenue in the sales data.\",\"Apply a rolling mean transformation with a window size of 7 on the total revenue column.\",\"Set the date column as the index for the sales data.\",\"Forecast the future sales revenue for the next 30 days using the Holt-Winters method with a seasonal period of 7.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_data.csv\",\"type\":\"csv\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_data\",\"comment\":\"Loading the customer data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`total_revenue` > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customer_data\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer data on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_customers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating sales data by region to calculate total sales revenue.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_regional_sales\",\"comment\":\"Sorting aggregated data by total sales in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sales_data\",\"columnName\":\"profit_margin\",\"formula\":\"`total_revenue` - `total_cost`\"},\"output\":\"sales_data_with_profit\",\"comment\":\"Calculating profit margin by subtracting total cost from total revenue.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sales_data\",\"transformations\":[{\"columns\":[\"total_revenue\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":7}}]},\"output\":\"sales_data_rolling_mean\",\"comment\":\"Applying a rolling mean transformation with a window size of 7 on total revenue.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sales_data\",\"transformations\":[{\"columns\":[\"date\"],\"operation\":\"set_index\"}]},\"output\":\"sales_data_indexed\",\"comment\":\"Setting the date column as the index for the sales data.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sales_data_indexed\",\"columns\":[\"total_revenue\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"dateColumn\":\"date\"}},\"output\":\"sales_forecast\",\"comment\":\"Forecasting future sales revenue for the next 30 days using Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials_inventory\",\"location\":\"C:/data/raw_materials_inventory.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Inventory\",\"description\":\"Contains information about the current stock levels of raw materials.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"Material_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"stock_level\",\"column_name\":\"Stock_Level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"Details the production tasks scheduled for the manufacturing process.\",\"columns\":[{\"column\":\"task_id\",\"column_name\":\"Task_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each production task.\"},{\"column\":\"material_id\",\"column_name\":\"Material_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material required for the task.\"},{\"column\":\"scheduled_quantity\",\"column_name\":\"Scheduled_Quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material scheduled for use in the task.\"},{\"column\":\"usage_rate\",\"column_name\":\"Usage_Rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Rate at which the material is used per task.\"},{\"column\":\"week\",\"column_name\":\"Week\",\"column_type\":\"xsd:string\",\"column_description\":\"Week number for which the task is scheduled.\"}]},\"table_id3\":{\"name\":\"historical_production_data\",\"location\":\"C:/data/historical_production_data.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Historical Production Data\",\"description\":\"Contains past production data for analysis and forecasting.\",\"columns\":[{\"column\":\"date\",\"column_name\":\"Date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the production task.\"},{\"column\":\"output_quantity\",\"column_name\":\"Output_Quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of product output on the given date.\"}]}},\"instructions\":[\"Load the raw materials inventory table from the CSV file.\",\"Load the production schedule from the Excel sheet.\",\"Filter the production schedule to include only the current week's tasks.\",\"Join the filtered production schedule with the raw materials inventory on the material ID.\",\"Calculate the required quantity of each material by multiplying the scheduled quantity by the material usage rate.\",\"Filter the joined table to include only materials with insufficient inventory.\",\"Sort the filtered table by material ID in ascending order.\",\"Aggregate the data to find the total shortage of each material.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/raw_materials_inventory.csv\",\"type\":\"csv\"},\"output\":\"raw_materials_inventory\",\"comment\":\"Loading the raw materials inventory from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from an Excel sheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"`Week` == 'current_week'\"},\"output\":\"current_week_schedule\",\"comment\":\"Filtering the production schedule to include only the current week's tasks.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"current_week_schedule\",\"raw_materials_inventory\"],\"joinOn\":[\"Material_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_schedule_inventory\",\"comment\":\"Joining the filtered production schedule with the raw materials inventory on the material ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_schedule_inventory\",\"columnName\":\"Required_Quantity\",\"formula\":\"`Scheduled_Quantity` * `Usage_Rate`\"},\"output\":\"schedule_with_requirements\",\"comment\":\"Calculating the required quantity of each material by multiplying the scheduled quantity by the material usage rate.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"schedule_with_requirements\",\"query\":\"`Required_Quantity` > `Stock_Level`\"},\"output\":\"insufficient_inventory\",\"comment\":\"Filtering the joined table to include only materials with insufficient inventory.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"insufficient_inventory\",\"sortBy\":\"Material_ID\",\"order\":\"asc\"},\"output\":\"sorted_insufficient_inventory\",\"comment\":\"Sorting the filtered table by material ID in ascending order.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sorted_insufficient_inventory\",\"groupBy\":\"Material_ID\",\"aggregations\":[{\"column\":\"Required_Quantity\",\"name\":\"Total_Shortage\",\"function\":\"sum\"}]},\"output\":\"aggregated_shortages\",\"comment\":\"Aggregating the data to find the total shortage of each material.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing all financial transactions with details such as transaction ID, customer ID, amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer associated with the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The monetary amount of the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.csv\",\"sheet_name\":null,\"label\":\"Customer Details\",\"description\":\"A table containing customer information such as customer ID, name, and contact details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_number\",\"column_type\":\"xsd:string\",\"column_description\":\"The contact number of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"A table with data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"An identifier for the data.\"},{\"column\":\"col_2\",\"column_name\":\"value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"A value associated with the data.\"}]}},\"instructions\":[\"Load the financial transactions table from the CSV file.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Join the filtered transactions with the customer details table on customer ID.\",\"Calculate the total transaction amount for each customer.\",\"Sort the customers by total transaction amount in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.csv\",\"type\":\"csv\"},\"output\":\"customer_details\",\"comment\":\"Loading the customer details dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"transactions_with_customers\",\"comment\":\"Joining filtered transactions with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_customers\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"}]},\"output\":\"customer_transaction_totals\",\"comment\":\"Calculating the total transaction amount for each customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"customer_transaction_totals\",\"sortBy\":\"total_transaction_amount\",\"order\":\"desc\"},\"output\":\"sorted_customers\",\"comment\":\"Sorting customers by total transaction amount in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains demographic and medical information about patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis for the patient.\"},{\"column\":\"col_3\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_visits\",\"location\":\"C:/data/hospital_visits.xlsx\",\"sheet_name\":\"Visits\",\"label\":\"Hospital Visits\",\"description\":\"Records of hospital visits by patients, including dates and visit types.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"visit_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital visit.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the visit to a specific patient.\"},{\"column\":\"col_3\",\"column_name\":\"visit_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the hospital visit.\"}]},\"table_id3\":{\"name\":\"insurance_claims\",\"location\":\"C:/data/insurance_claims.csv\",\"sheet_name\":null,\"label\":\"Insurance Claims\",\"description\":\"Contains data on insurance claims made by patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"claim_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each insurance claim.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the claim to a specific patient.\"},{\"column\":\"col_3\",\"column_name\":\"claim_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount claimed by the patient.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file.\",\"Load the hospital visits table from the Excel file.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Join the filtered patient records with the hospital visits table on patient ID.\",\"Aggregate the joined data to calculate the total number of visits per patient.\",\"Sort the aggregated data by the total number of visits in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hospital_visits.xlsx\",\"type\":\"xlsx\"},\"output\":\"hospital_visits\",\"comment\":\"Loading hospital visits data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetic_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with diabetes.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"diabetic_patients\",\"hospital_visits\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining diabetic patient records with hospital visits on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"visit_id\",\"name\":\"total_visits\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate the total number of visits per patient.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_visits\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total number of visits in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customers\",\"location\":\"C:/data/customers.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"Contains customer information including customer_id, name, and contact details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]},\"table_id2\":{\"name\":\"sales\",\"location\":\"C:/data/sales.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Sales Data\",\"description\":\"Contains sales transactions including transaction_id, customer_id, total_value, and total_cost.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the transaction to a customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"total_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total value of the transaction.\"},{\"column\":\"column_4_id\",\"column_name\":\"total_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total cost of the transaction.\"}]},\"table_id3\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing_campaigns.csv\",\"sheet_name\":null,\"label\":\"Marketing Campaign Data\",\"description\":\"Contains information about past marketing campaigns, including campaign_id, start_date, and end_date.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"column_2_id\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Start date of the marketing campaign.\"},{\"column\":\"column_3_id\",\"column_name\":\"end_date\",\"column_type\":\"xsd:date\",\"column_description\":\"End date of the marketing campaign.\"}]}},\"instructions\":[\"Load the customer data from the CSV file located at C:/data/customers.csv.\",\"Load the sales data from the Excel file located at C:/data/sales.xlsx.\",\"Filter the sales data to include only transactions with a total value greater than $100.\",\"Join the filtered sales data with the customer data on the customer_id column.\",\"Calculate a new column in the joined table to determine the profit margin by subtracting total cost from total revenue.\",\"Aggregate the data by customer_id to find the total sales and average profit margin for each customer.\",\"Sort the aggregated data by total sales in descending order.\",\"Identify the top 10 customers based on total sales.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.csv\",\"type\":\"csv\"},\"output\":\"customers\",\"comment\":\"Loading customer data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales.xlsx\",\"type\":\"xlsx\"},\"output\":\"sales\",\"comment\":\"Loading sales data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales\",\"query\":\"total_value > 100\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with a total value greater than $100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customers\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered sales data with customer data on customer_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"profit_margin\",\"formula\":\"total_value - total_cost\"},\"output\":\"joined_data_with_profit\",\"comment\":\"Calculating profit margin by subtracting total cost from total revenue.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_profit\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"total_value\",\"name\":\"total_sales\",\"function\":\"sum\"},{\"column\":\"profit_margin\",\"name\":\"average_profit_margin\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by customer_id to find total sales and average profit margin.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total sales in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_data\",\"query\":\"index < 10\"},\"output\":\"top_customers\",\"comment\":\"Identifying the top 10 customers based on total sales.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow does 'forecastData' handle missing values in the target columns?","output":"If any column in 'columns' contains missing values, they are interpolated using linear interpolation by default. If interpolation fails, an error is raised."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the purpose of the 'identifyTable' action?","output":"The 'identifyTable' action loads a table into memory from a specified source."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information about customers, including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"purchase_history\",\"location\":\"C:/data/purchase_history.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Purchase History\",\"description\":\"Records of customer purchases, including transaction dates and amounts.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the transaction to a customer.\"},{\"column\":\"col_3\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the transaction occurred.\"},{\"column\":\"col_4\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount spent in the transaction.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for irrelevant data.\"},{\"column\":\"col_2\",\"column_name\":\"info\",\"column_type\":\"xsd:string\",\"column_description\":\"Additional information.\"}]}},\"instructions\":[\"Load the customer demographics data from the CSV file.\",\"Load the purchase history data from the Excel file.\",\"Filter the purchase history to include only transactions from the last year.\",\"Join the filtered purchase history with customer demographics based on customer ID.\",\"Calculate the total spending for each customer.\",\"Segment customers into high, medium, and low spenders based on total spending.\",\"Aggregate the data to find the average spending in each segment.\",\"Sort the segments by average spending in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/purchase_history.xlsx\",\"type\":\"xlsx\"},\"output\":\"purchase_history\",\"comment\":\"Loading purchase history data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"purchase_history\",\"query\":\"`transaction_date` >= '2022-01-01'\"},\"output\":\"filtered_purchase_history\",\"comment\":\"Filtering purchase history to include only transactions from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_purchase_history\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered purchase history with customer demographics based on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_spending\",\"function\":\"sum\"}]},\"output\":\"customer_spending\",\"comment\":\"Calculating the total spending for each customer.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"customer_spending\",\"transformations\":[{\"columns\":[\"total_spending\"],\"operation\":\"min_max_scale\",\"parameters\":{\"range\":[0,1]}}]},\"output\":\"scaled_spending\",\"comment\":\"Scaling total spending to segment customers into high, medium, and low spenders.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"scaled_spending\",\"groupBy\":\"spending_segment\",\"aggregations\":[{\"column\":\"total_spending\",\"name\":\"average_spending\",\"function\":\"mean\"}]},\"output\":\"segment_averages\",\"comment\":\"Aggregating data to find the average spending in each segment.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"segment_averages\",\"sortBy\":\"average_spending\",\"order\":\"desc\"},\"output\":\"sorted_segments\",\"comment\":\"Sorting the segments by average spending in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid arithmetic expression using multiplication and division.","output":"'col1 * (col2 + col3) / col4'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the purpose of the 'applyTimeSeriesTransformations' action?","output":"The 'applyTimeSeriesTransformations' action applies a sequence of transformations to a time series dataset, such as resampling, rolling calculations, lagging, and trend decomposition."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/healthcare/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains patient information including IDs, names, and diagnoses.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"medication\",\"location\":\"C:/data/healthcare/medication.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Contains information about medications prescribed to patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"}]},\"table_id3\":{\"name\":\"hospital_visits\",\"location\":\"C:/data/healthcare/hospital_visits.csv\",\"sheet_name\":null,\"label\":\"Hospital Visits\",\"description\":\"Records of patient visits to the hospital, not relevant for this workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"visit_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital visit.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_3\",\"column_name\":\"visit_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the hospital visit.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Join the filtered patient records with the medication table on patient ID.\",\"Aggregate the joined table to calculate the total number of diabetes patients per medication.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/healthcare/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetes_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with diabetes.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/healthcare/medication.csv\",\"type\":\"csv\"},\"output\":\"medication\",\"comment\":\"Loading medication records from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"diabetes_patients\",\"medication\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient records with medication records on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"medication_name\",\"aggregations\":[{\"column\":\"patient_id\",\"name\":\"total_patients\",\"function\":\"count\"}]},\"output\":\"medication_summary\",\"comment\":\"Aggregating data to calculate the total number of diabetes patients per medication.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains energy consumption data for the building, including timestamps and consumption values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Weather\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature and humidity readings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature reading in degrees Celsius.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The humidity percentage.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.csv\",\"sheet_name\":null,\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for building equipment.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"equipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for the equipment.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from the last year.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate the average daily energy consumption.\",\"Aggregate the data to find the total energy consumption per month.\",\"Sort the monthly energy consumption data in descending order.\",\"Apply a rolling mean to the daily energy consumption to smooth out fluctuations.\",\"Forecast the next month's energy consumption using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp` >= '2022-01-01'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"timestamp\",\"date\"],\"joinType\":\"inner\"},\"output\":\"energy_weather_data\",\"comment\":\"Joining the filtered energy consumption data with the weather data on the date column.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_weather_data\",\"groupBy\":\"timestamp\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"average_daily_consumption\",\"function\":\"mean\"}]},\"output\":\"energy_weather_with_avg\",\"comment\":\"Calculating the average daily energy consumption.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_weather_with_avg\",\"groupBy\":\"month\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"total_monthly_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Aggregating the data to find the total energy consumption per month.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_energy_consumption\",\"sortBy\":\"total_monthly_consumption\",\"order\":\"desc\"},\"output\":\"sorted_monthly_energy_consumption\",\"comment\":\"Sorting the monthly energy consumption data in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"energy_weather_with_avg\",\"transformations\":[{\"columns\":[\"average_daily_consumption\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":7}}]},\"output\":\"smoothed_energy_consumption\",\"comment\":\"Applying a rolling mean to the daily energy consumption to smooth out fluctuations.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"smoothed_energy_consumption\",\"columns\":[\"average_daily_consumption\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"timestamp\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the next month's energy consumption using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid attribute access expression.","output":"'a > 10'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"building_energy\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and energy usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy used in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.xlsx\",\"sheet_name\":\"Info\",\"label\":\"Building Information\",\"description\":\"This table contains information about the buildings, such as location and type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The geographical location of the building.\"},{\"column\":\"col_3\",\"column_name\":\"building_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of building, e.g., residential, commercial.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"random_id\",\"column_type\":\"xsd:string\",\"column_description\":\"A random identifier.\"},{\"column\":\"col_2\",\"column_name\":\"random_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"A random value.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only entries where the energy usage is above 500 kWh.\",\"Aggregate the filtered data to calculate the total energy consumption per building.\",\"Sort the aggregated data by total energy consumption in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"building_energy\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"building_energy\",\"query\":\"energy_usage_kwh > 500\"},\"output\":\"filtered_energy\",\"comment\":\"Filtering the data to include only entries where the energy usage is above 500 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_usage_kwh\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"aggregated_energy\",\"comment\":\"Aggregating the filtered data to calculate the total energy consumption per building.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_energy\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_energy\",\"comment\":\"Sorting the aggregated data by total energy consumption in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains details about product specifications including product ID, name, and price.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]},\"table_id2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx\",\"sheet_name\":\"2023_data\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including product ID, estimated demand, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"estimated_demand\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Estimated demand for the product.\"},{\"column\":\"col_3\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the market research entry.\"}]},\"table_id3\":{\"name\":\"historical_sales\",\"location\":\"C:/data/historical_sales.csv\",\"sheet_name\":null,\"label\":\"Historical Sales Data\",\"description\":\"Contains historical sales data which is not relevant for the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"sale_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each sale.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_3\",\"column_name\":\"sale_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the sale.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel sheet.\",\"Filter the market research data to include only entries from the last year.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Calculate a new column for potential revenue by multiplying the price by the estimated demand.\",\"Aggregate the joined data by product category to find the total potential revenue.\",\"Sort the aggregated data by total potential revenue in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading market research data from an Excel sheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"`date` >= '2022-01-01'\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only entries from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_revenue\",\"formula\":\"`price` * `estimated_demand`\"},\"output\":\"data_with_revenue\",\"comment\":\"Calculating potential revenue by multiplying price by estimated demand.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_revenue\",\"groupBy\":\"product_category\",\"aggregations\":[{\"column\":\"potential_revenue\",\"name\":\"total_potential_revenue\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by product category to find total potential revenue.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_potential_revenue\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total potential revenue in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains hourly energy consumption data for a facility, including timestamps and consumption values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Weather Data\",\"description\":\"This table contains daily weather data, including temperature and humidity readings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average daily temperature in degrees Celsius.\"}]},\"table_id3\":{\"name\":\"historical_energy_data\",\"location\":\"C:/data/historical_energy_data.csv\",\"sheet_name\":null,\"label\":\"Historical Energy Data\",\"description\":\"This table contains historical energy consumption data for previous years.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"year\",\"column_type\":\"xsd:gYear\",\"column_description\":\"The year of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"annual_consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total energy consumed in that year.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Load the weather data from the Excel file located at C:/data/weather_data.xlsx.\",\"Filter the energy consumption data to include only records from the year 2023.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column 'energy_efficiency' by dividing energy consumption by temperature.\",\"Aggregate the joined data by month, calculating the total energy consumption and average temperature.\",\"Sort the aggregated data by total energy consumption in descending order.\",\"Forecast the energy consumption for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"timestamp >= '2023-01-01' and timestamp < '2024-01-01'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"timestamp\",\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered energy consumption data with the weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"energy_consumption / temperature\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating a new column 'energy_efficiency' by dividing energy consumption by temperature.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"timestamp.dt.to_period('M')\",\"aggregations\":[{\"column\":\"energy_consumption\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating the joined data by month, calculating the total energy consumption and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total energy consumption in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"energy_consumption\",\"columns\":[\"energy_consumption\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"timestamp\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the energy consumption for the next 30 days using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat power and root functions are supported in Tamarind expressions?","output":"Supported power and root functions include 'sqrt'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption records with timestamps and usage values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy_prices.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains historical energy prices with timestamps.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kilowatt-hour.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Aggregate the energy consumption data by month to calculate total usage.\",\"Calculate the average daily energy consumption for each month.\",\"Apply a rolling mean transformation with a window of 3 months to smooth the monthly data.\",\"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"timestamp >= '2023-01-01' and timestamp < '2024-01-01'\"},\"output\":\"energy_2023\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_2023\",\"groupBy\":\"timestamp.dt.to_period('M')\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_usage\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_usage\",\"comment\":\"Aggregating the energy consumption data by month to calculate total usage.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"monthly_energy_usage\",\"columnName\":\"average_daily_usage\",\"formula\":\"total_usage / 30\"},\"output\":\"monthly_energy_with_avg\",\"comment\":\"Calculating the average daily energy consumption for each month.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"monthly_energy_with_avg\",\"transformations\":[{\"columns\":[\"total_usage\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":3}}]},\"output\":\"smoothed_monthly_energy\",\"comment\":\"Applying a rolling mean transformation with a window of 3 months to smooth the monthly data.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"smoothed_monthly_energy\",\"columns\":[\"total_usage\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"timestamp\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"confidenceInterval\":95}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/marketing/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information of customers including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age_group\",\"column_type\":\"xsd:string\",\"column_description\":\"Age group of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"sales_transactions\",\"location\":\"C:/data/marketing/sales_transactions.xlsx\",\"sheet_name\":\"Transactions\",\"label\":\"Sales Transactions\",\"description\":\"Contains records of sales transactions including transaction ID, customer ID, and total amount.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"total_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total amount of the transaction.\"}]},\"table_id3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/marketing/product_catalog.csv\",\"sheet_name\":null,\"label\":\"Product Catalog\",\"description\":\"Contains details of products including product ID, name, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the sales transactions table from the Excel file.\",\"Filter the sales transactions to include only those with a total amount greater than $100.\",\"Join the filtered sales transactions with the customer demographics table on customer ID.\",\"Aggregate the joined data to calculate the total sales per age group.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/sales_transactions.xlsx\",\"type\":\"xlsx\"},\"output\":\"sales_transactions\",\"comment\":\"Loading sales transactions data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_transactions\",\"query\":\"`total_amount` > 100\"},\"output\":\"filtered_sales_transactions\",\"comment\":\"Filtering sales transactions to include only those with a total amount greater than $100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales_transactions\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered sales transactions with customer demographics on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"age_group\",\"aggregations\":[{\"column\":\"total_amount\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"sales_per_age_group\",\"comment\":\"Aggregating joined data to calculate total sales per age group.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'min_max_scale' transformation.","output":"{ \"name\": \"applyTransformations\", \"parameters\": { \"table\": \"sales_data\", \"transformations\": [ { \"columns\": [\"revenue\"], \"operation\": \"min_max_scale\", \"parameters\": { \"range\": [0,1] } } ] }, \"output\": \"normalized_sales\" }"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid expression using an unsupported function.","output":"'len(col1)' // Error: 'len' is not a supported function in Tamarind expressions."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"This table contains detailed records of financial transactions including amounts and currency codes.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code of the transaction amount.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of the transaction, e.g., sale, refund.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/exchange_rates.xlsx\",\"sheet_name\":\"Rates\",\"label\":\"Exchange Rates\",\"description\":\"This table provides the exchange rates for various currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at C:/data/financial_transactions.csv.\",\"Load the 'exchange_rates' table from the Excel file located at C:/data/exchange_rates.xlsx.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Join the filtered transactions with the 'exchange_rates' table on the 'currency_code' column.\",\"Calculate a new column 'amount_in_usd' by multiplying 'amount' with 'exchange_rate'.\",\"Aggregate the joined table by 'transaction_type' to calculate the total 'amount_in_usd' for each type.\",\"Sort the aggregated data by 'total_amount_in_usd' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with exchange rates on currency code.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"amount_in_usd\",\"formula\":\"amount * exchange_rate\"},\"output\":\"transactions_with_usd\",\"comment\":\"Calculating amount in USD by multiplying amount with exchange rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_usd\",\"groupBy\":\"transaction_type\",\"aggregations\":[{\"column\":\"amount_in_usd\",\"name\":\"total_amount_in_usd\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by transaction type to calculate total amount in USD.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_amount_in_usd\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total amount in USD in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_data\",\"location\":\"C:/data/patient_data.csv\",\"sheet_name\":null,\"label\":\"Patient Data\",\"description\":\"Contains demographic information about patients including age, gender, and patient_id.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"column_2_id\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"column_3_id\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the patient.\"}]},\"table_id2\":{\"name\":\"medical_records\",\"location\":\"C:/data/medical_records.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Medical Records\",\"description\":\"Contains medical records including blood pressure readings for patients.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"column_2_id\",\"column_name\":\"blood_pressure\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Blood pressure reading of the patient.\"},{\"column\":\"column_3_id\",\"column_name\":\"visit_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the medical visit.\"}]},\"table_id3\":{\"name\":\"insurance_data\",\"location\":\"C:/data/insurance_data.csv\",\"sheet_name\":null,\"label\":\"Insurance Data\",\"description\":\"Contains insurance details for patients, not relevant for this workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"column_2_id\",\"column_name\":\"insurance_provider\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the insurance provider.\"},{\"column\":\"column_3_id\",\"column_name\":\"policy_number\",\"column_type\":\"xsd:string\",\"column_description\":\"Insurance policy number.\"}]}},\"instructions\":[\"Load the patient data from the CSV file located at C:/data/patient_data.csv.\",\"Load the medical records from the Excel file located at C:/data/medical_records.xlsx.\",\"Filter the patient data to include only those patients who are above 50 years old.\",\"Join the filtered patient data with the medical records on the patient_id column.\",\"Calculate the average blood pressure for each patient.\",\"Sort the resulting data by average blood pressure in descending order.\",\"Identify patients with an average blood pressure above 140 as high-risk patients.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_data.csv\",\"type\":\"csv\"},\"output\":\"patient_data\",\"comment\":\"Loading patient demographic data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/medical_records.xlsx\",\"type\":\"xlsx\"},\"output\":\"medical_records\",\"comment\":\"Loading medical records from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_data\",\"query\":\"age > 50\"},\"output\":\"filtered_patient_data\",\"comment\":\"Filtering patient data to include only patients above 50 years old.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_patient_data\",\"medical_records\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient data with medical records on patient_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"blood_pressure\",\"name\":\"average_blood_pressure\",\"function\":\"mean\"}]},\"output\":\"average_blood_pressure_data\",\"comment\":\"Calculating the average blood pressure for each patient.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_blood_pressure_data\",\"sortBy\":\"average_blood_pressure\",\"order\":\"desc\"},\"output\":\"sorted_blood_pressure_data\",\"comment\":\"Sorting data by average blood pressure in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_blood_pressure_data\",\"query\":\"average_blood_pressure > 140\"},\"output\":\"high_risk_patients\",\"comment\":\"Identifying patients with an average blood pressure above 140 as high-risk patients.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data with columns for date, energy usage, and other relevant metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy used on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature recorded on the given date.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data with columns for date, temperature, and other weather-related metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature recorded on the given date.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the energy management workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at 'C:/data/energy_consumption.csv'.\",\"Load the weather data from the Excel file located at 'C:/data/weather_data.xlsx'.\",\"Filter the energy consumption data to include only records from the year 2023.\",\"Filter the weather data to include only records from the year 2023.\",\"Join the filtered energy consumption data with the filtered weather data on the 'date' column.\",\"Calculate a new column 'energy_efficiency' by dividing 'energy_usage' by 'temperature'.\",\"Aggregate the joined data by 'month' to calculate the total energy usage and average temperature.\",\"Sort the aggregated data by 'total_energy_usage' in descending order.\",\"Apply a rolling mean transformation with a window of 3 months on the 'average_temperature' column.\",\"Forecast the next 6 months of energy usage using the Holt-Winters method with a seasonal period of 12.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date`.year == 2023\"},\"output\":\"filtered_energy_consumption\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"weather_data\",\"query\":\"`date`.year == 2023\"},\"output\":\"filtered_weather_data\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"filtered_weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`energy_usage` / `temperature`\"},\"output\":\"data_with_efficiency\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date.month\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy_usage\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy_usage\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"transformations\":[{\"columns\":[\"average_temperature\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":3}}]},\"output\":\"transformed_data\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_data\",\"columns\":[\"total_energy_usage\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"forecast_results\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/marketing/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"This table contains demographic information of customers including age, income, and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"income\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual income of the customer.\"},{\"column\":\"col_4\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer resides.\"}]},\"table_id2\":{\"name\":\"customer_purchases\",\"location\":\"C:/data/marketing/customer_purchases.csv\",\"sheet_name\":null,\"label\":\"Customer Purchases\",\"description\":\"This table contains purchase history of customers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"purchase_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each purchase.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking purchase to a customer.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount spent on the purchase.\"},{\"column\":\"col_4\",\"column_name\":\"purchase_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the purchase.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Filter the customer data to include only those aged between 25 and 40.\",\"Aggregate the filtered data to find the average income by region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading the customer demographics dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_demographics\",\"query\":\"`age` >= 25 and `age` <= 40\"},\"output\":\"filtered_customers\",\"comment\":\"Filtering customer data to include only those aged between 25 and 40.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_customers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"income\",\"name\":\"average_income\",\"function\":\"mean\"}]},\"output\":\"average_income_by_region\",\"comment\":\"Aggregating filtered data to find the average income by region.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid subscript expression.","output":"'df[\"col1\"] + df[\"col2\"]'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow can columns be accessed using attribute notation?","output":"Columns can be accessed using dot notation if they are valid Python identifiers, such as 'df.a'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/hospital/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains patient information including diagnoses and personal details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"}]},\"table_id2\":{\"name\":\"medication_records\",\"location\":\"C:/data/hospital/medication_records.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Details of medications prescribed to patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"},{\"column\":\"col_3\",\"column_name\":\"dosage\",\"column_type\":\"xsd:string\",\"column_description\":\"Dosage of the medication prescribed.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/data/hospital/staff_records.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff Records\",\"description\":\"Information about hospital staff members.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the staff member works.\"}]}},\"instructions\":[\"Load the patient records table from the hospital database.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Join the filtered patient records with the medication records to find prescribed medications.\",\"Aggregate the data to find the total number of diabetic patients prescribed each medication.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hospital/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from the hospital database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetic_patients\",\"comment\":\"Filtering patient records to include only those with a diagnosis of diabetes.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hospital/medication_records.csv\",\"type\":\"csv\"},\"output\":\"medication_records\",\"comment\":\"Loading medication records from the hospital database.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"diabetic_patients\",\"medication_records\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"diabetic_medications\",\"comment\":\"Joining diabetic patient records with medication records to find prescribed medications.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"diabetic_medications\",\"groupBy\":\"medication\",\"aggregations\":[{\"column\":\"patient_id\",\"name\":\"total_diabetic_patients\",\"function\":\"count\"}]},\"output\":\"medication_summary\",\"comment\":\"Aggregating data to find the total number of diabetic patients prescribed each medication.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"This table contains information about shipments including weight, destination, and other logistics details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"Destination of the shipment.\"}]},\"table_id2\":{\"name\":\"transport_modes\",\"location\":\"C:/data/logistics/transport_modes.csv\",\"sheet_name\":null,\"label\":\"Transport Modes\",\"description\":\"This table contains information about different modes of transport used in logistics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"mode_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transport mode.\"},{\"column\":\"col_2\",\"column_name\":\"mode_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the transport mode.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at 'C:/data/logistics/shipment_data.csv'.\",\"Filter the 'shipment_data' to include only shipments with a weight greater than 1000 kg.\",\"Aggregate the filtered data to calculate the total weight of shipments by destination.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"`weight` > 1000\"},\"output\":\"heavy_shipments\",\"comment\":\"Filtering shipments to include only those with a weight greater than 1000 kg.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"heavy_shipments\",\"groupBy\":\"destination\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"total_weight_by_destination\",\"comment\":\"Aggregating the filtered shipments to calculate the total weight by destination.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat parameters are required for the 'applyTransformations' action?","output":"'applyTransformations' requires a 'table' (name of the table) and 'transformations' (a list of transformations, each specifying 'columns', 'operation', and optional 'parameters')."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.csv\",\"sheet_name\":null,\"label\":\"Department Data\",\"description\":\"Contains information about departments including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"project_data\",\"location\":\"C:/data/project_data.csv\",\"sheet_name\":null,\"label\":\"Project Data\",\"description\":\"Contains information about projects including project IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"project_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the project.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/employee_data.csv.\",\"Filter the employee data to include only those with a salary greater than 50000.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Aggregate the joined data to calculate the average salary per department.\",\"Sort the aggregated data by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading the employee data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employees with a salary greater than 50000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.csv\",\"type\":\"csv\"},\"output\":\"department_data\",\"comment\":\"Loading the department data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employee_data\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating the average salary per department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by average salary in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains the production schedule with order details and priority levels.\",\"columns\":[{\"column\":\"order_id\",\"column_name\":\"ORDER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each order.\"},{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product to be manufactured.\"},{\"column\":\"priority\",\"column_name\":\"PRIORITY\",\"column_type\":\"xsd:string\",\"column_description\":\"Priority level of the order (e.g., high, medium, low).\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the product to be manufactured.\"}]},\"table_id2\":{\"name\":\"inventory\",\"location\":\"C:/data/manufacturing/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory\",\"description\":\"This table contains current stock levels for each product.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product in stock.\"},{\"column\":\"stock_level\",\"column_name\":\"STOCK_LEVEL\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the product.\"}]},\"table_id3\":{\"name\":\"employee_records\",\"location\":\"C:/data/manufacturing/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"This table contains records of employees working in the manufacturing plant.\",\"columns\":[{\"column\":\"employee_id\",\"column_name\":\"EMPLOYEE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"name\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"department\",\"column_name\":\"DEPARTMENT\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the employee works.\"}]}},\"instructions\":[\"Load the production schedule table from the CSV file.\",\"Filter the production schedule to include only high-priority orders.\",\"Join the filtered production schedule with the inventory table to check stock availability.\",\"Aggregate the joined data to calculate the total quantity needed for each product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"PRIORITY == 'high'\"},\"output\":\"high_priority_orders\",\"comment\":\"Filtering the production schedule to include only high-priority orders.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/inventory.csv\",\"type\":\"csv\"},\"output\":\"inventory\",\"comment\":\"Loading the inventory data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_priority_orders\",\"inventory\"],\"joinOn\":[\"PRODUCT_ID\"],\"joinType\":\"inner\"},\"output\":\"orders_with_stock\",\"comment\":\"Joining high-priority orders with inventory to check stock availability.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"orders_with_stock\",\"groupBy\":\"PRODUCT_ID\",\"aggregations\":[{\"column\":\"QUANTITY\",\"name\":\"total_quantity_needed\",\"function\":\"sum\"}]},\"output\":\"total_quantity_per_product\",\"comment\":\"Aggregating the data to calculate the total quantity needed for each product.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains records of energy consumption for the building, including date, energy usage, and number of occupants.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"occupants\",\"column_type\":\"xsd:integer\",\"column_description\":\"The number of occupants in the building.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature and humidity for the year 2023.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature recorded on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The humidity level recorded on the given date.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.csv\",\"sheet_name\":null,\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for building equipment, including dates and tasks.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"},{\"column\":\"col_2\",\"column_name\":\"task\",\"column_type\":\"xsd:string\",\"column_description\":\"The maintenance task to be performed.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Load the weather data from the Excel file.\",\"Join the energy consumption data with the weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing energy consumption by the number of occupants.\",\"Aggregate the data to find the total energy consumption per month.\",\"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date`.year == 2023\"},\"output\":\"energy_2023\",\"comment\":\"Filtering the energy consumption data to include only records from the year 2023.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"energy_2023\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"energy_weather_data\",\"comment\":\"Joining the energy consumption data with the weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"energy_weather_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`energy_usage` / `occupants`\"},\"output\":\"energy_with_efficiency\",\"comment\":\"Calculating energy efficiency by dividing energy consumption by the number of occupants.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_with_efficiency\",\"groupBy\":[\"date\"],\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy\",\"function\":\"sum\"}]},\"output\":\"monthly_energy\",\"comment\":\"Aggregating the data to find the total energy consumption per month.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"monthly_energy\",\"columns\":[\"total_energy\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"date\"}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat types of arithmetic operations are supported in Tamarind expressions?","output":"Supported arithmetic operations include addition ('+'), subtraction ('-'), multiplication ('*'), division ('/'), exponentiation ('**'), and modulus ('%')."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_usage\",\"location\":\"C:/data/energy_usage.csv\",\"sheet_name\":null,\"label\":\"Energy Usage Data\",\"description\":\"This table contains daily energy consumption data including total usage and solar generation.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy usage record.\"},{\"column\":\"col_2\",\"column_name\":\"total_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total energy usage for the day.\"},{\"column\":\"col_3\",\"column_name\":\"solar_generation\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy generated by solar panels.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains daily weather information including temperature and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature for the day.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity for the day.\"}]},\"table_id3\":{\"name\":\"historical_energy_prices\",\"location\":\"C:/data/historical_energy_prices.csv\",\"sheet_name\":null,\"label\":\"Historical Energy Prices\",\"description\":\"This table contains historical energy prices data, which is not relevant for the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the price record.\"},{\"column\":\"col_2\",\"column_name\":\"price_per_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kilowatt-hour.\"}]}},\"instructions\":[\"Load the table 'energy_usage' which contains daily energy consumption data.\",\"Load the table 'weather_data' which contains daily weather information.\",\"Filter the 'energy_usage' table to include only data from the year 2023.\",\"Join the 'energy_usage' table with the 'weather_data' table on the 'date' column.\",\"Calculate a new column 'adjusted_usage' by subtracting 'solar_generation' from 'total_usage'.\",\"Aggregate the joined table by 'month' to calculate the total 'adjusted_usage'.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_usage.csv\",\"type\":\"csv\"},\"output\":\"energy_usage\",\"comment\":\"Loading the energy usage data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.csv\",\"type\":\"csv\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_usage\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"energy_usage_2023\",\"comment\":\"Filtering energy usage data to include only records from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"energy_usage_2023\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_energy_weather\",\"comment\":\"Joining energy usage data with weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_energy_weather\",\"columnName\":\"adjusted_usage\",\"formula\":\"`total_usage` - `solar_generation`\"},\"output\":\"energy_with_adjusted_usage\",\"comment\":\"Calculating adjusted energy usage by subtracting solar generation from total usage.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_with_adjusted_usage\",\"groupBy\":\"month\",\"aggregations\":[{\"column\":\"adjusted_usage\",\"name\":\"total_adjusted_usage\",\"function\":\"sum\"}]},\"output\":\"monthly_adjusted_usage\",\"comment\":\"Aggregating the adjusted energy usage by month to calculate total adjusted usage.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comment\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comment from the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"col_3\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.csv\",\"sheet_name\":null,\"label\":\"Product Catalog\",\"description\":\"Table containing information about products offered.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback to include only those with a rating below 3.\",\"Load the customer details table from the Excel file.\",\"Join the filtered feedback with customer details using customer ID.\",\"Aggregate the joined data to count the number of complaints per region.\",\"Sort the aggregated data by the number of complaints in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating < 3\"},\"output\":\"low_rating_feedback\",\"comment\":\"Filtering feedback to include only those with a rating below 3.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_rating_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_feedback_details\",\"comment\":\"Joining filtered feedback with customer details using customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_feedback_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"customer_id\",\"name\":\"complaint_count\",\"function\":\"count\"}]},\"output\":\"complaints_per_region\",\"comment\":\"Aggregating data to count the number of complaints per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"complaints_per_region\",\"sortBy\":\"complaint_count\",\"order\":\"desc\"},\"output\":\"sorted_complaints\",\"comment\":\"Sorting the aggregated data by the number of complaints in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"This table contains detailed specifications of products including their status, dimensions, and supplier information.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Approval status of the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]},\"table_id2\":{\"name\":\"supplier_details\",\"location\":\"C:/data/supplier_details.csv\",\"sheet_name\":null,\"label\":\"Supplier Details\",\"description\":\"This table contains information about suppliers, including their ID and contact details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"},{\"column\":\"column_3_id\",\"column_name\":\"contact_email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address for contacting the supplier.\"}]},\"table_id3\":{\"name\":\"product_reviews\",\"location\":\"C:/data/product_reviews.csv\",\"sheet_name\":null,\"label\":\"Product Reviews\",\"description\":\"This table contains customer reviews for various products.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"review_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each review.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product being reviewed.\"},{\"column\":\"column_3_id\",\"column_name\":\"review_text\",\"column_type\":\"xsd:string\",\"column_description\":\"Text of the customer review.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a status of 'approved'.\",\"Join the filtered product specifications with the supplier details on the supplier ID.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"status == 'approved'\"},\"output\":\"approved_product_specifications\",\"comment\":\"Filtering the product specifications to include only approved products.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"approved_product_specifications\",\"supplier_details\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"product_supplier_info\",\"comment\":\"Joining the approved product specifications with supplier details based on supplier ID.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat are the column naming rules in Tamarind expressions?","output":"If a column name is a valid Python identifier, it can be used as is. If it contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`)."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'fillna' operation do?","output":"The 'fillna' operation fills missing values with a specified strategy. It requires 'parameters.method' which can be 'zero', 'ffill', 'bfill', or 'mean'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:integer\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comment\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comment from the customer.\"}]},\"table_2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.csv\",\"sheet_name\":null,\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers, including their region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.xlsx\",\"sheet_name\":\"Products\",\"label\":\"Product Catalog\",\"description\":\"Table containing product details, not relevant to customer feedback analysis.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback to include only those with a rating of 4 or 5.\",\"Join the filtered feedback with the customer details table on customer ID.\",\"Aggregate the data to find the average rating per region.\",\"Sort the aggregated data by average rating in descending order.\",\"Export the sorted data to a new CSV file for further analysis.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading the customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback to include only ratings of 4 or 5.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.csv\",\"type\":\"csv\"},\"output\":\"customer_details\",\"comment\":\"Loading the customer details data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"feedback_with_details\",\"comment\":\"Joining positive feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"feedback_with_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_by_region\",\"comment\":\"Aggregating data to find the average rating per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_rating_by_region\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_average_rating_by_region\",\"comment\":\"Sorting the aggregated data by average rating in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of each product, including product ID, name, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]},\"table_id2\":{\"name\":\"market_analysis\",\"location\":\"C:/data/market_analysis.xlsx\",\"sheet_name\":\"Analysis\",\"label\":\"Market Analysis\",\"description\":\"Contains market analysis data including demand and potential customer base for various products.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"average_demand\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Average demand for the product over the last year.\"},{\"column\":\"col_3\",\"column_name\":\"potential_customers\",\"column_type\":\"xsd:integer\",\"column_description\":\"Estimated number of potential customers for the product.\"},{\"column\":\"col_4\",\"column_name\":\"year\",\"column_type\":\"xsd:integer\",\"column_description\":\"Year of the market analysis data.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Irrelevant\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market analysis data from the Excel sheet.\",\"Filter the market analysis data to include only entries from the last two years.\",\"Join the product specifications with the filtered market analysis data on the product ID.\",\"Calculate the potential market size by multiplying the average demand by the number of potential customers.\",\"Aggregate the joined data by product category to find the total potential market size.\",\"Sort the aggregated data by total potential market size in descending order.\",\"Filter the sorted data to include only products with a potential market size greater than 100,000 units.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_analysis.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_analysis\",\"comment\":\"Loading the market analysis data from an Excel sheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_analysis\",\"query\":\"year >= 2021\"},\"output\":\"filtered_market_analysis\",\"comment\":\"Filtering market analysis data to include only entries from the last two years.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_analysis\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market analysis data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_market_size\",\"formula\":\"average_demand * potential_customers\"},\"output\":\"joined_data_with_market_size\",\"comment\":\"Calculating potential market size by multiplying average demand by the number of potential customers.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_market_size\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"potential_market_size\",\"name\":\"total_potential_market_size\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by product category to find total potential market size.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_potential_market_size\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total potential market size in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_data\",\"query\":\"total_potential_market_size > 100000\"},\"output\":\"filtered_sorted_data\",\"comment\":\"Filtering sorted data to include only products with a potential market size greater than 100,000 units.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_data.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"This table contains sales records including date, product, and sales amount.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the sales transaction.\"},{\"column\":\"col_2\",\"column_name\":\"product\",\"column_type\":\"xsd:string\",\"column_description\":\"The product sold.\"},{\"column\":\"col_3\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of sales in dollars.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customer_data.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"This table contains customer information including customer ID and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location of the customer.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\",\"Filter the sales data to include only records from the year 2023.\",\"Aggregate the filtered sales data by month to calculate total sales.\",\"Sort the aggregated sales data by month in ascending order.\",\"Apply a logarithmic transformation to the total sales column to stabilize variance.\",\"Set the month column as the index for the time series data.\",\"Perform a time-series forecast on the total sales column using the Holt-Winters method with a seasonal period of 12 months.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_data.csv\",\"type\":\"csv\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`date`.dt.year == 2023\"},\"output\":\"sales_data_2023\",\"comment\":\"Filtering sales data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_data_2023\",\"groupBy\":\"date.dt.to_period('M')\",\"aggregations\":[{\"column\":\"sales_amount\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"monthly_sales\",\"comment\":\"Aggregating sales data by month to calculate total sales.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_sales\",\"sortBy\":\"date\",\"order\":\"asc\"},\"output\":\"sorted_monthly_sales\",\"comment\":\"Sorting the aggregated sales data by month in ascending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_monthly_sales\",\"transformations\":[{\"columns\":[\"total_sales\"],\"operation\":\"log_transform\"}]},\"output\":\"log_transformed_sales\",\"comment\":\"Applying a logarithmic transformation to the total sales column to stabilize variance.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"log_transformed_sales\",\"transformations\":[{\"columns\":[\"date\"],\"operation\":\"set_index\"}]},\"output\":\"time_series_sales\",\"comment\":\"Setting the month column as the index for the time series data.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"time_series_sales\",\"columns\":[\"total_sales\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"date\"}},\"output\":\"sales_forecast\",\"comment\":\"Performing a time-series forecast on the total sales column using the Holt-Winters method with a seasonal period of 12 months.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of products including priority levels.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"priority\",\"column_type\":\"xsd:string\",\"column_description\":\"Priority level of the product.\"},{\"column\":\"col_3\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of the product.\"}]},\"table_id2\":{\"name\":\"supplier_information\",\"location\":\"C:/data/supplier_information.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains information about suppliers including their IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"Contains inventory levels for various products.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"inventory_level\",\"column_type\":\"xsd:integer\",\"column_description\":\"Current inventory level of the product.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a high priority.\",\"Join the filtered product specifications with the supplier information table.\",\"Aggregate the joined data to calculate the total cost of high-priority products by supplier.\",\"Sort the aggregated data by total cost in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"priority == 'high'\"},\"output\":\"high_priority_products\",\"comment\":\"Filtering the product specifications to include only those with a high priority.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_information.csv\",\"type\":\"csv\"},\"output\":\"supplier_information\",\"comment\":\"Loading the supplier information table from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_priority_products\",\"supplier_information\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered product specifications with the supplier information table.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"supplier_id\",\"aggregations\":[{\"column\":\"cost\",\"name\":\"total_cost\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined data to calculate the total cost of high-priority products by supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_cost\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total cost in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow should a valid column name with spaces be written?","output":"A valid column name with spaces should be enclosed in backticks, such as '`a column name` + col2'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'calculateDerivedColumn' action with an unsupported function.","output":"{ \"name\": \"calculateDerivedColumn\", \"parameters\": { \"table\": \"sales\", \"columnName\": \"invalid_column\", \"formula\": \"unsupported_function(revenue)\" }, \"output\": \"sales_invalid\" } // Error: 'unsupported_function' is not allowed."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow are datasets identified in Tamarind workflows?","output":"Datasets are identified by unique names and must exist before being referenced by an action."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'forecastData' action using 'holt_winters'.","output":"{ \"name\": \"forecastData\", \"parameters\": { \"table\": \"energy_consumption\", \"columns\": [\"usage\"], \"forecastParameters\": { \"forecastPeriod\": 15, \"frequency\": \"D\", \"dateColumn\": \"timestamp\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"confidenceInterval\": 90 }, \"output\": \"energy_forecast\" } }"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid attribute access expression using multiple columns.","output":"'a + b'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'sin_transform' transformation.","output":"{ \"name\": \"applyTransformations\", \"parameters\": { \"table\": \"weather_data\", \"transformations\": [ { \"columns\": [\"day_of_year\"], \"operation\": \"sin_transform\" } ] }, \"output\": \"seasonal_adjusted\" }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"building_energy\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and energy usage in kilowatt-hours.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"Date and time of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Energy consumption in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.xlsx\",\"sheet_name\":\"Info\",\"label\":\"Building Information\",\"description\":\"Contains information about buildings, such as address and type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"address\",\"column_type\":\"xsd:string\",\"column_description\":\"Physical address of the building.\"},{\"column\":\"col_3\",\"column_name\":\"building_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of the building, e.g., residential, commercial.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the data entry.\"},{\"column\":\"col_2\",\"column_name\":\"value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Some irrelevant value.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each building.\",\"Sort the buildings by total energy consumption in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"building_energy\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"building_energy\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"filtered_energy_2023\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_2023\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_usage_kwh\",\"name\":\"total_energy\",\"function\":\"sum\"}]},\"output\":\"total_energy_per_building\",\"comment\":\"Calculating the total energy consumption for each building.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"total_energy_per_building\",\"sortBy\":\"total_energy\",\"order\":\"desc\"},\"output\":\"sorted_energy_consumption\",\"comment\":\"Sorting the buildings by total energy consumption in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid expression using lambda.","output":"'lambda x: x + 1' // Error: Lambda expressions are not allowed in Tamarind."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications for each product, including demand score and cost.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"Product ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"demand_score\",\"column_name\":\"Demand Score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Score indicating the market demand for the product.\"},{\"column\":\"product_cost\",\"column_name\":\"Product Cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost associated with producing the product.\"}]},\"table_id2\":{\"name\":\"supplier_information\",\"location\":\"C:/data/supplier_information.xlsx\",\"sheet_name\":\"Suppliers\",\"label\":\"Supplier Information\",\"description\":\"Contains information about suppliers, including their IDs and contact details.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"Supplier ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"supplier_name\",\"column_name\":\"Supplier Name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id3\":{\"name\":\"market_trends\",\"location\":\"C:/data/market_trends.csv\",\"sheet_name\":null,\"label\":\"Market Trends\",\"description\":\"Contains data on current market trends, not directly relevant to the current workflow.\",\"columns\":[{\"column\":\"trend_id\",\"column_name\":\"Trend ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each market trend.\"},{\"column\":\"trend_description\",\"column_name\":\"Trend Description\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the market trend.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a high demand score.\",\"Load the supplier information table from the Excel file.\",\"Join the filtered product specifications with the supplier information on the supplier ID.\",\"Calculate the cost efficiency for each product by dividing the product cost by the demand score.\",\"Sort the products by cost efficiency in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"`demand_score` > 7\"},\"output\":\"high_demand_products\",\"comment\":\"Filtering products to include only those with a demand score greater than 7.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_information.xlsx\",\"type\":\"xlsx\"},\"output\":\"supplier_information\",\"comment\":\"Loading supplier information from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_demand_products\",\"supplier_information\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"product_supplier_info\",\"comment\":\"Joining high demand products with supplier information on supplier ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"product_supplier_info\",\"columnName\":\"cost_efficiency\",\"formula\":\"`product_cost` / `demand_score`\"},\"output\":\"products_with_efficiency\",\"comment\":\"Calculating cost efficiency by dividing product cost by demand score.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"products_with_efficiency\",\"sortBy\":\"cost_efficiency\",\"order\":\"asc\"},\"output\":\"sorted_products\",\"comment\":\"Sorting products by cost efficiency in ascending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if a non-numeric column is used in an aggregation function?","output":"Non-numeric columns in aggregation are ignored."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains records of energy consumption with columns for date, usage, and other relevant details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains weather information with columns for date, temperature, and other weather-related metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature recorded on that date.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the energy management workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Filter the data to include only records where energy usage is greater than 100 kWh.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Aggregate the joined data by date to calculate the total energy usage and average temperature.\",\"Sort the aggregated data by total energy usage in descending order.\",\"Calculate a new column for the energy cost by multiplying total energy usage by a rate of 0.15.\",\"Forecast future energy consumption for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"usage > 100\"},\"output\":\"filtered_energy\",\"comment\":\"Filtering data to include only records with energy usage greater than 100 kWh.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.csv\",\"type\":\"csv\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered energy data with weather data on the date column.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_usage\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by date to calculate total energy usage and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_usage\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total energy usage in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_data\",\"columnName\":\"energy_cost\",\"formula\":\"total_usage * 0.15\"},\"output\":\"data_with_cost\",\"comment\":\"Calculating energy cost by multiplying total energy usage by a rate of 0.15.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"data_with_cost\",\"columns\":[\"total_usage\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"forecast_results\",\"comment\":\"Forecasting future energy consumption for the next 30 days using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/HR_data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"Contains detailed records of all employees, including their ID, name, department, and years of service.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Full name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the employee works.\"},{\"column\":\"col_4\",\"column_name\":\"years_of_service\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of years the employee has been with the company.\"}]},\"table_id2\":{\"name\":\"salary_details\",\"location\":\"C:/HR_data/salary_details.csv\",\"sheet_name\":null,\"label\":\"Salary Details\",\"description\":\"Contains salary information for employees, including base salary and bonuses.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"base_salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Base salary of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"bonus\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Bonus amount received by the employee.\"}]},\"table_id3\":{\"name\":\"training_records\",\"location\":\"C:/HR_data/training_records.csv\",\"sheet_name\":null,\"label\":\"Training Records\",\"description\":\"Records of training sessions attended by employees, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"training_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the training session.\"},{\"column\":\"col_3\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the training was attended.\"}]}},\"instructions\":[\"Load the employee records table from the HR database.\",\"Filter the employee records to include only those who have been with the company for more than 5 years.\",\"Load the salary details table from the HR database.\",\"Join the filtered employee records with the salary details on the employee ID.\",\"Calculate a new column for each employee that shows their total compensation, including bonuses.\",\"Aggregate the data to find the average total compensation by department.\",\"Sort the aggregated data by average total compensation in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_data/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading employee records from the HR database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"years_of_service > 5\"},\"output\":\"filtered_employees\",\"comment\":\"Filtering employees with more than 5 years of service.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_data/salary_details.csv\",\"type\":\"csv\"},\"output\":\"salary_details\",\"comment\":\"Loading salary details from the HR database.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employees\",\"salary_details\"],\"joinOn\":[\"employee_id\"],\"joinType\":\"inner\"},\"output\":\"employee_salary_data\",\"comment\":\"Joining filtered employee records with salary details on employee ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"employee_salary_data\",\"columnName\":\"total_compensation\",\"formula\":\"base_salary + bonus\"},\"output\":\"employee_compensation\",\"comment\":\"Calculating total compensation for each employee.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"employee_compensation\",\"groupBy\":\"department\",\"aggregations\":[{\"column\":\"total_compensation\",\"name\":\"average_compensation\",\"function\":\"mean\"}]},\"output\":\"average_compensation_by_department\",\"comment\":\"Aggregating data to find average total compensation by department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_compensation_by_department\",\"sortBy\":\"average_compensation\",\"order\":\"desc\"},\"output\":\"sorted_compensation_data\",\"comment\":\"Sorting data by average total compensation in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials\",\"description\":\"This table contains information about the raw materials used in the manufacturing process, including material ID, name, and quantity.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"material_name\",\"column_name\":\"MATERIAL_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the material.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the material available.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"This table contains the production schedule, including production line and material requirements.\",\"columns\":[{\"column\":\"production_line\",\"column_name\":\"PRODUCTION_LINE\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the production line.\"},{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Material ID required for production.\"},{\"column\":\"required_quantity\",\"column_name\":\"REQUIRED_QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material required for production.\"}]},\"table_id3\":{\"name\":\"employee_records\",\"location\":\"C:/data/manufacturing/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"This table contains records of employees working in the manufacturing plant, including employee ID, name, and department.\",\"columns\":[{\"column\":\"employee_id\",\"column_name\":\"EMPLOYEE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"employee_name\",\"column_name\":\"EMPLOYEE_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"department\",\"column_name\":\"DEPARTMENT\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the employee works.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains information about the materials used in manufacturing.\",\"Filter the 'raw_materials' table to include only materials with a quantity greater than 100.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the material_id column.\",\"Aggregate the joined table to calculate the total quantity of materials needed per production line.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"QUANTITY > 100\"},\"output\":\"filtered_raw_materials\",\"comment\":\"Filtering raw materials to include only those with a quantity greater than 100.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_raw_materials\",\"production_schedule\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered raw materials with the production schedule on material_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_table\",\"groupBy\":\"PRODUCTION_LINE\",\"aggregations\":[{\"column\":\"QUANTITY\",\"name\":\"total_material_quantity\",\"function\":\"sum\"}]},\"output\":\"aggregated_materials\",\"comment\":\"Aggregating the joined table to calculate the total quantity of materials needed per production line.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"This table contains demographic information about customers, including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"marketing_campaign_results\",\"location\":\"C:/data/marketing_campaign_results.xlsx\",\"sheet_name\":\"Results\",\"label\":\"Marketing Campaign Results\",\"description\":\"This table contains results from various marketing campaigns, including success metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"success\",\"column_type\":\"xsd:boolean\",\"column_description\":\"Indicates whether the campaign was successful.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the marketing campaign results table from the Excel file.\",\"Filter the marketing campaign results to include only successful campaigns.\",\"Join the filtered marketing campaign results with the customer demographics table on customer ID.\",\"Aggregate the joined data to find the total number of successful campaigns per age group.\",\"Sort the aggregated data by the total number of successful campaigns in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading the customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing_campaign_results.xlsx\",\"type\":\"xlsx\"},\"output\":\"marketing_campaign_results\",\"comment\":\"Loading the marketing campaign results from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"marketing_campaign_results\",\"query\":\"success == True\"},\"output\":\"successful_campaigns\",\"comment\":\"Filtering the marketing campaign results to include only successful campaigns.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"successful_campaigns\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the successful campaigns with customer demographics on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"age\",\"aggregations\":[{\"column\":\"campaign_id\",\"name\":\"total_successful_campaigns\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined data to find the total number of successful campaigns per age group.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_successful_campaigns\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by the total number of successful campaigns in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"Contains daily energy consumption data with timestamps.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Weather Data\",\"description\":\"Contains daily weather data including temperature and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The daily average temperature in Celsius.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The daily average humidity percentage.\"}]},\"table_id3\":{\"name\":\"historical_energy_prices\",\"location\":\"C:/data/historical_energy_prices.csv\",\"sheet_name\":null,\"label\":\"Historical Energy Prices\",\"description\":\"Contains historical energy prices data, not relevant for the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the price record.\"},{\"column\":\"col_2\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kWh.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from 2023.\",\"Join the energy consumption data with the weather data on the date column.\",\"Calculate the average daily energy consumption.\",\"Aggregate the data to find the total energy consumption per month.\",\"Apply a rolling mean to smooth the daily energy consumption data with a window of 7 days.\",\"Forecast the next 30 days of energy consumption using the Holt-Winters method.\",\"Identify any anomalies in the energy consumption data using the Isolation Forest algorithm.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records from 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the energy consumption data with the weather data on the date column.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"average_daily_consumption\",\"function\":\"mean\"}]},\"output\":\"data_with_avg_consumption\",\"comment\":\"Calculating the average daily energy consumption.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_avg_consumption\",\"groupBy\":\"date.dt.to_period('M')\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"total_monthly_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Aggregating the data to find the total energy consumption per month.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"data_with_avg_consumption\",\"transformations\":[{\"columns\":[\"consumption\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":7}}]},\"output\":\"smoothed_energy_consumption\",\"comment\":\"Applying a rolling mean to smooth the daily energy consumption data with a window of 7 days.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"smoothed_energy_consumption\",\"columns\":[\"consumption\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the next 30 days of energy consumption using the Holt-Winters method.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"smoothed_energy_consumption\",\"columns\":[\"consumption\"],\"forecastParameters\":{\"forecastPeriod\":30,\"algorithm\":\"isolation_forest\",\"dateColumn\":\"date\"}},\"output\":\"anomaly_detection\",\"comment\":\"Identifying any anomalies in the energy consumption data using the Isolation Forest algorithm.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_data.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"This table contains sales transactions including product IDs, sales amounts, and transaction dates.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product sold.\"},{\"column\":\"column_3_id\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of sales in dollars.\"},{\"column\":\"column_4_id\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the transaction.\"}]},\"table_id2\":{\"name\":\"product_info\",\"location\":\"C:/data/product_info.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Product Information\",\"description\":\"This table contains information about products including product IDs and categories.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product.\"},{\"column\":\"column_2_id\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"The category to which the product belongs.\"}]},\"table_id3\":{\"name\":\"customer_data\",\"location\":\"C:/data/customer_data.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"This table contains customer information including customer IDs and demographics.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\",\"Load the product information from the Excel file located at C:/data/product_info.xlsx.\",\"Filter the sales data to include only transactions from the year 2023.\",\"Join the filtered sales data with the product information on the product_id column.\",\"Aggregate the joined data to calculate the total sales per product category.\",\"Sort the aggregated data by total sales in descending order.\",\"Apply a log transformation to the total sales column to stabilize variance.\",\"Forecast future sales for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_data.csv\",\"type\":\"csv\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_info.xlsx\",\"type\":\"xlsx\"},\"output\":\"product_info\",\"comment\":\"Loading the product information from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`transaction_date`.dt.year == 2023\"},\"output\":\"filtered_sales_data\",\"comment\":\"Filtering sales data to include only transactions from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales_data\",\"product_info\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered sales data with product information on product_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"sales_amount\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined data to calculate total sales per product category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total sales in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_data\",\"transformations\":[{\"columns\":[\"total_sales\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_data\",\"comment\":\"Applying a log transformation to the total sales column to stabilize variance.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_data\",\"columns\":[\"total_sales\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"dateColumn\":\"transaction_date\"}},\"output\":\"forecast_results\",\"comment\":\"Forecasting future sales for the next 30 days using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat alternative syntax is used for Boolean operations in eval()?","output":"In eval(), Boolean operations use '&' for 'and', '|' for 'or', and '~' for 'not', such as '(col1 > 10) & (col2 < 5)'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid attribute access expression.","output":"'a > 10'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/HR_data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"A table containing detailed records of employees including their salary, hire date, and other personal information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The annual salary of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"hire_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the employee was hired.\"}]},\"table_id2\":{\"name\":\"department_info\",\"location\":\"C:/HR_data/department_info.csv\",\"sheet_name\":null,\"label\":\"Department Information\",\"description\":\"A table containing information about different departments within the company.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the department.\"}]}},\"instructions\":[\"Load the employee records table from the HR database.\",\"Filter the employee records to include only those with a salary greater than $50,000.\",\"Sort the filtered employee records by their hire date in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_data/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records from the HR database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employee records to include only those with a salary greater than $50,000.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"high_salary_employees\",\"sortBy\":\"hire_date\",\"order\":\"asc\"},\"output\":\"sorted_high_salary_employees\",\"comment\":\"Sorting the filtered employee records by their hire date in ascending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains demographic and health information of patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the patient.\"},{\"column\":\"col_4\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the patient in kilograms.\"},{\"column\":\"col_5\",\"column_name\":\"height\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Height of the patient in centimeters.\"}]},\"table_id2\":{\"name\":\"medical_tests\",\"location\":\"C:/data/medical_tests.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Medical Tests Results\",\"description\":\"Contains results of various medical tests conducted on patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"test_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the test was conducted.\"},{\"column\":\"col_3\",\"column_name\":\"cholesterol_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cholesterol level of the patient.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Contains information about hospital staff members.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the staff member works.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file located at C:/data/patient_records.csv.\",\"Load the medical tests results table from the Excel file located at C:/data/medical_tests.xlsx.\",\"Filter the patient records to include only those who are above 50 years old.\",\"Join the filtered patient records with the medical tests results on the patient_id column.\",\"Calculate a new column 'BMI' in the joined table using the formula: weight / (height/100)**2.\",\"Aggregate the data by gender to calculate the average BMI.\",\"Sort the aggregated data by average BMI in descending order.\",\"Apply a log transformation to the average BMI column to stabilize variance.\",\"Forecast the average BMI for the next 12 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/medical_tests.xlsx\",\"type\":\"xlsx\"},\"output\":\"medical_tests\",\"comment\":\"Loading medical tests results from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"age > 50\"},\"output\":\"filtered_patients\",\"comment\":\"Filtering patient records to include only those above 50 years old.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_patients\",\"medical_tests\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient records with medical tests results on patient_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"BMI\",\"formula\":\"weight / (height/100)**2\"},\"output\":\"joined_data_with_bmi\",\"comment\":\"Calculating BMI using weight and height.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_bmi\",\"groupBy\":\"gender\",\"aggregations\":[{\"column\":\"BMI\",\"name\":\"average_BMI\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by gender to calculate average BMI.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_BMI\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by average BMI in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_data\",\"transformations\":[{\"columns\":[\"average_BMI\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_data\",\"comment\":\"Applying log transformation to average BMI.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_data\",\"columns\":[\"average_BMI\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"test_date\"}},\"output\":\"bmi_forecast\",\"comment\":\"Forecasting average BMI for the next 12 months using Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan column names be enclosed in double quotes instead of backticks?","output":"No, column names must be enclosed in backticks, not double quotes."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"This table contains customer feedback including ratings and comments.\",\"columns\":[{\"column\":\"feedback_id\",\"column_name\":\"FEEDBACK_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Unique identifier for each feedback entry.\"},{\"column\":\"customer_id\",\"column_name\":\"CUSTOMER_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"rating\",\"column_name\":\"RATING\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"comments\",\"column_name\":\"COMMENTS\",\"column_type\":\"xsd:string\",\"column_description\":\"Customer comments on the service.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.csv\",\"sheet_name\":null,\"label\":\"Customer Details\",\"description\":\"This table contains details about customers including their region.\",\"columns\":[{\"column\":\"customer_id\",\"column_name\":\"CUSTOMER_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"name\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"region\",\"column_name\":\"REGION\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_id3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.csv\",\"sheet_name\":null,\"label\":\"Product Catalog\",\"description\":\"This table contains information about products offered by the company.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"product_name\",\"column_name\":\"PRODUCT_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"price\",\"column_name\":\"PRICE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback to include only entries with a rating below 3.\",\"Join the filtered feedback with the customer details table using customer ID.\",\"Aggregate the joined table to count the number of negative feedbacks per region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading the customer feedback table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"RATING < 3\"},\"output\":\"negative_feedback\",\"comment\":\"Filtering feedback to include only entries with a rating below 3.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.csv\",\"type\":\"csv\"},\"output\":\"customer_details\",\"comment\":\"Loading the customer details table from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"negative_feedback\",\"customer_details\"],\"joinOn\":[\"CUSTOMER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_feedback\",\"comment\":\"Joining the filtered feedback with customer details using customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_feedback\",\"groupBy\":\"REGION\",\"aggregations\":[{\"column\":\"FEEDBACK_ID\",\"name\":\"negative_feedback_count\",\"function\":\"count\"}]},\"output\":\"regional_negative_feedback\",\"comment\":\"Aggregating the joined table to count the number of negative feedbacks per region.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the 'xgboost' algorithm used for?","output":"Use 'xgboost' for high-performance machine learning-based time series forecasting, capturing complex relationships in data."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid column name using double quotes.","output":"'\"a column name\" + col2' // Error: Column names must be enclosed in backticks (`), not double quotes."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow does 'identifyTable' use query strings in the location parameter?","output":"The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/hr/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees including their salaries and department IDs.\",\"columns\":[{\"column\":\"emp_id\",\"column_name\":\"Employee ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"salary\",\"column_name\":\"Salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The annual salary of the employee.\"},{\"column\":\"department_id\",\"column_name\":\"Department ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/hr/department_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Department Data\",\"description\":\"Contains information about departments including department IDs and names.\",\"columns\":[{\"column\":\"dept_id\",\"column_name\":\"Department ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"dept_name\",\"column_name\":\"Department Name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the department.\"}]},\"table_id3\":{\"name\":\"project_data\",\"location\":\"C:/data/hr/project_data.xlsx\",\"sheet_name\":\"Projects\",\"label\":\"Project Data\",\"description\":\"Contains information about projects and their associated departments.\",\"columns\":[{\"column\":\"project_id\",\"column_name\":\"Project ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"dept_id\",\"column_name\":\"Department ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department associated with the project.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/hr/employee_data.csv.\",\"Filter the employee data to include only those with a salary greater than 50,000.\",\"Load the department data from the Excel file located at C:/data/hr/department_data.xlsx.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Calculate a new column in the joined table to determine the annual bonus as 10% of the salary.\",\"Aggregate the data by department to calculate the total salary and total bonus for each department.\",\"Sort the aggregated data by total salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading employee data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employees with salary greater than 50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/department_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_data\",\"comment\":\"Loading department data from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employee_data\",\"department_data\"],\"joinOn\":[\"department_id\",\"dept_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"annual_bonus\",\"formula\":\"salary * 0.10\"},\"output\":\"joined_data_with_bonus\",\"comment\":\"Calculating annual bonus as 10% of the salary.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_bonus\",\"groupBy\":\"dept_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"total_salary\",\"function\":\"sum\"},{\"column\":\"annual_bonus\",\"name\":\"total_bonus\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by department to calculate total salary and total bonus.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_salary\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total salary in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"A table containing patient demographic information including age, weight, and height.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"column_2_id\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"column_3_id\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the patient in kilograms.\"},{\"column\":\"column_4_id\",\"column_name\":\"height\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Height of the patient in centimeters.\"}]},\"table_id2\":{\"name\":\"medical_tests\",\"location\":\"C:/data/medical_tests.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Medical Tests Results\",\"description\":\"A table containing results of various medical tests for patients.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"column_2_id\",\"column_name\":\"test_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medical test.\"},{\"column\":\"column_3_id\",\"column_name\":\"test_result\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Result of the medical test.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff Information\",\"description\":\"A table containing information about hospital staff members.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"column_3_id\",\"column_name\":\"position\",\"column_type\":\"xsd:string\",\"column_description\":\"Position of the staff member in the hospital.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file located at C:/data/patient_records.csv.\",\"Load the medical tests results table from the Excel file located at C:/data/medical_tests.xlsx.\",\"Filter the patient records to include only patients older than 50 years.\",\"Join the filtered patient records with the medical tests results on the patient_id column.\",\"Calculate a new column 'BMI' in the joined table using the formula: weight / (height/100)**2.\",\"Filter the joined table to include only patients with a BMI greater than 25.\",\"Aggregate the filtered data by gender to calculate the average BMI.\",\"Sort the aggregated data by average BMI in descending order.\",\"Apply a log transformation to the average BMI column to stabilize variance.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/medical_tests.xlsx\",\"type\":\"xlsx\"},\"output\":\"medical_tests\",\"comment\":\"Loading the medical tests results dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"age > 50\"},\"output\":\"filtered_patient_records\",\"comment\":\"Filtering patient records to include only patients older than 50 years.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_patient_records\",\"medical_tests\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered patient records with medical tests results on patient_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_table\",\"columnName\":\"BMI\",\"formula\":\"weight / (height/100)**2\"},\"output\":\"joined_table_with_bmi\",\"comment\":\"Calculating BMI using the formula: weight / (height/100)**2.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"joined_table_with_bmi\",\"query\":\"BMI > 25\"},\"output\":\"overweight_patients\",\"comment\":\"Filtering the joined table to include only patients with a BMI greater than 25.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"overweight_patients\",\"groupBy\":\"gender\",\"aggregations\":[{\"column\":\"BMI\",\"name\":\"average_BMI\",\"function\":\"mean\"}]},\"output\":\"average_bmi_by_gender\",\"comment\":\"Aggregating data by gender to calculate the average BMI.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_bmi_by_gender\",\"sortBy\":\"average_BMI\",\"order\":\"desc\"},\"output\":\"sorted_average_bmi\",\"comment\":\"Sorting the aggregated data by average BMI in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_average_bmi\",\"transformations\":[{\"columns\":[\"average_BMI\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_data\",\"comment\":\"Applying a log transformation to the average BMI column to stabilize variance.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'forecastData' action with a missing required parameter.","output":"{ \"name\": \"forecastData\", \"parameters\": { \"table\": \"sales_data\", \"forecastParameters\": { \"forecastPeriod\": 30, \"frequency\": \"D\", \"algorithm\": \"xgboost\" }, \"output\": \"forecast_results\" } } // Error: 'columns' parameter is missing."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow are transformations applied in 'applyTransformations'?","output":"Transformations are applied sequentially in the order they appear in the 'transformations' list."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comments from customers.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.csv\",\"sheet_name\":null,\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"column_3_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"Table containing sales data for various products.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the sales transaction.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Load the customer details table from the CSV file.\",\"Filter the feedback to include only those with a rating below 3.\",\"Join the filtered feedback with customer details using the customer ID.\",\"Aggregate the data to find the average rating per region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.csv\",\"type\":\"csv\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating < 3\"},\"output\":\"low_rating_feedback\",\"comment\":\"Filtering feedback to include only those with a rating below 3.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_rating_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_feedback_details\",\"comment\":\"Joining filtered feedback with customer details using customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_feedback_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_per_region\",\"comment\":\"Aggregating data to find the average rating per region.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"building_energy_data\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and energy usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.xlsx\",\"sheet_name\":\"Info\",\"label\":\"Building Information\",\"description\":\"Contains information about buildings such as address, size, and type.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_B\",\"column_name\":\"address\",\"column_type\":\"xsd:string\",\"column_description\":\"The physical address of the building.\"},{\"column\":\"col_C\",\"column_name\":\"building_size\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The size of the building in square meters.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_X\",\"column_name\":\"random_id\",\"column_type\":\"xsd:string\",\"column_description\":\"A random identifier.\"},{\"column\":\"col_Y\",\"column_name\":\"random_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"A random value.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only entries with energy usage above 100 kWh.\",\"Aggregate the filtered data to calculate the total energy consumption per building.\",\"Sort the aggregated data by total energy consumption in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"building_energy_data\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"building_energy_data\",\"query\":\"`energy_usage_kWh` > 100\"},\"output\":\"filtered_energy_data\",\"comment\":\"Filtering the data to include only entries with energy usage above 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_data\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_usage_kWh\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"aggregated_energy_data\",\"comment\":\"Aggregating the filtered data to calculate the total energy consumption per building.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_energy_data\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_energy_data\",\"comment\":\"Sorting the aggregated data by total energy consumption in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_performance\",\"location\":\"C:/data/employee_performance.csv\",\"sheet_name\":null,\"label\":\"Employee Performance Data\",\"description\":\"This table contains performance scores for employees, including their names, departments, and scores.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"employee_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"The department in which the employee works.\"},{\"column\":\"col_4\",\"column_name\":\"performance_score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The performance score of the employee.\"}]},\"table_id2\":{\"name\":\"employee_salaries\",\"location\":\"C:/data/employee_salaries.csv\",\"sheet_name\":null,\"label\":\"Employee Salary Data\",\"description\":\"This table contains salary information for employees, including their base salary and bonuses.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"base_salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The base salary of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"bonus\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The bonus amount for the employee.\"}]}},\"instructions\":[\"Load the employee performance data from the CSV file.\",\"Filter the data to include only employees with a performance score above 85.\",\"Sort the filtered data by employee name in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_performance.csv\",\"type\":\"csv\"},\"output\":\"employee_performance\",\"comment\":\"Loading the employee performance data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_performance\",\"query\":\"performance_score > 85\"},\"output\":\"high_performance_employees\",\"comment\":\"Filtering the data to include only employees with a performance score above 85.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"high_performance_employees\",\"sortBy\":\"employee_name\",\"order\":\"asc\"},\"output\":\"sorted_high_performance_employees\",\"comment\":\"Sorting the filtered data by employee name in ascending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/finance/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"This table contains all financial transactions with details such as transaction ID, customer ID, amount, and date.\",\"columns\":[{\"column\":\"transaction_id\",\"column_name\":\"TRANSACTION_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"customer_id\",\"column_name\":\"CUSTOMER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer associated with the transaction.\"},{\"column\":\"amount\",\"column_name\":\"AMOUNT\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The monetary value of the transaction.\"},{\"column\":\"date\",\"column_name\":\"DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/finance/customer_details.csv\",\"sheet_name\":null,\"label\":\"Customer Details\",\"description\":\"This table contains details about customers including customer ID, name, and contact information.\",\"columns\":[{\"column\":\"customer_id\",\"column_name\":\"CUSTOMER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"name\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"email\",\"column_name\":\"EMAIL\",\"column_type\":\"xsd:string\",\"column_description\":\"The email address of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_financial_data\",\"location\":\"C:/data/finance/irrelevant_financial_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Financial Data\",\"description\":\"This table contains financial data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"data_id\",\"column_name\":\"DATA_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the data entry.\"},{\"column\":\"value\",\"column_name\":\"VALUE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Some financial value.\"}]}},\"instructions\":[\"Load the financial transactions table from the CSV file.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Join the filtered transactions with the customer details table on customer ID.\",\"Aggregate the joined data to calculate the total transaction amount per customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"`AMOUNT` > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/customer_details.csv\",\"type\":\"csv\"},\"output\":\"customer_details\",\"comment\":\"Loading the customer details dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_details\"],\"joinOn\":[\"CUSTOMER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"CUSTOMER_ID\",\"aggregations\":[{\"column\":\"AMOUNT\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"}]},\"output\":\"customer_transaction_totals\",\"comment\":\"Aggregating the joined data to calculate the total transaction amount per customer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_details\",\"location\":\"C:/legal_data/case_details.csv\",\"sheet_name\":null,\"label\":\"Case Details\",\"description\":\"Contains information about legal cases, including status and client ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the case (e.g., Open, Closed).\"},{\"column\":\"col_3\",\"column_name\":\"client_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the client associated with the case.\"}]},\"table_id2\":{\"name\":\"client_info\",\"location\":\"C:/legal_data/client_info.csv\",\"sheet_name\":null,\"label\":\"Client Information\",\"description\":\"Contains personal and contact information for clients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"client_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each client.\"},{\"column\":\"col_2\",\"column_name\":\"client_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the client.\"},{\"column\":\"col_3\",\"column_name\":\"contact_number\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact number of the client.\"}]},\"table_id3\":{\"name\":\"lawyer_info\",\"location\":\"C:/legal_data/lawyer_info.csv\",\"sheet_name\":null,\"label\":\"Lawyer Information\",\"description\":\"Contains details about lawyers, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]}},\"instructions\":[\"Load the table 'case_details' from the legal database.\",\"Load the table 'client_info' from the legal database.\",\"Filter the 'case_details' table to include only cases with a status of 'Open'.\",\"Join the filtered 'case_details' table with the 'client_info' table on 'client_id'.\",\"Aggregate the joined table to count the number of open cases per client.\",\"Sort the aggregated data by the number of open cases in descending order.\",\"Calculate a derived column 'case_ratio' as the ratio of open cases to total cases for each client.\",\"Filter the final table to include only clients with a 'case_ratio' greater than 0.5.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_details.csv\",\"type\":\"csv\"},\"output\":\"case_details\",\"comment\":\"Loading the case details dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/client_info.csv\",\"type\":\"csv\"},\"output\":\"client_info\",\"comment\":\"Loading the client information dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_details\",\"query\":\"status == 'Open'\"},\"output\":\"open_cases\",\"comment\":\"Filtering case details to include only open cases.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"open_cases\",\"client_info\"],\"joinOn\":[\"client_id\"],\"joinType\":\"inner\"},\"output\":\"open_cases_with_clients\",\"comment\":\"Joining open cases with client information based on client_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"open_cases_with_clients\",\"groupBy\":\"client_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"open_case_count\",\"function\":\"count\"}]},\"output\":\"client_case_counts\",\"comment\":\"Aggregating data to count the number of open cases per client.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"client_case_counts\",\"sortBy\":\"open_case_count\",\"order\":\"desc\"},\"output\":\"sorted_client_case_counts\",\"comment\":\"Sorting clients by the number of open cases in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_client_case_counts\",\"columnName\":\"case_ratio\",\"formula\":\"open_case_count / total_cases\"},\"output\":\"client_case_ratios\",\"comment\":\"Calculating the ratio of open cases to total cases for each client.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"client_case_ratios\",\"query\":\"case_ratio > 0.5\"},\"output\":\"high_ratio_clients\",\"comment\":\"Filtering clients with a case ratio greater than 0.5.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/HR_Database/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"Contains detailed records of employees including their salary, department ID, and other personal information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_details\",\"location\":\"C:/HR_Database/department_details.csv\",\"sheet_name\":null,\"label\":\"Department Details\",\"description\":\"Contains information about each department, including department ID and department name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the department.\"}]},\"table_id3\":{\"name\":\"project_assignments\",\"location\":\"C:/HR_Database/project_assignments.csv\",\"sheet_name\":null,\"label\":\"Project Assignments\",\"description\":\"Lists the projects assigned to each employee, not relevant for salary analysis.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"}]}},\"instructions\":[\"Load the employee records table from the HR database.\",\"Filter the employee records to include only those with a salary above $50,000.\",\"Join the filtered employee records with the department details table on the department ID.\",\"Aggregate the joined data to calculate the average salary per department.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_Database/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records from the HR database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employee records to include only those with a salary above $50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_Database/department_details.csv\",\"type\":\"csv\"},\"output\":\"department_details\",\"comment\":\"Loading the department details from the HR database.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_salary_employees\",\"department_details\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_employee_department\",\"comment\":\"Joining filtered employee records with department details on department ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_employee_department\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"average_salary_per_department\",\"comment\":\"Calculating the average salary per department.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/hr/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees, including their salaries and department affiliations.\",\"columns\":[{\"column\":\"emp_id\",\"column_name\":\"EMP_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"name\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"salary\",\"column_name\":\"SALARY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current salary of the employee.\"},{\"column\":\"department_id\",\"column_name\":\"DEPARTMENT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/hr/department_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Department Data\",\"description\":\"Contains information about departments, including department names and IDs.\",\"columns\":[{\"column\":\"department_id\",\"column_name\":\"DEPARTMENT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"department_name\",\"column_name\":\"DEPARTMENT_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/hr/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data\",\"description\":\"Contains unrelated data not needed for the current workflow.\",\"columns\":[{\"column\":\"col1\",\"column_name\":\"COL1\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant column.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/hr/employee_data.csv.\",\"Load the department data from the Excel file located at C:/data/hr/department_data.xlsx.\",\"Filter the employee data to include only employees with a salary greater than 50,000.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Calculate a new column in the joined table to determine the annual bonus as 10% of the salary.\",\"Aggregate the data by department to find the total salary and total bonus for each department.\",\"Sort the aggregated data by total salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading employee data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/department_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_data\",\"comment\":\"Loading department data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"SALARY > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employees with a salary greater than 50,000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employee_data\",\"department_data\"],\"joinOn\":[\"DEPARTMENT_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"annual_bonus\",\"formula\":\"SALARY * 0.10\"},\"output\":\"joined_data_with_bonus\",\"comment\":\"Calculating annual bonus as 10% of the salary.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_bonus\",\"groupBy\":\"DEPARTMENT_ID\",\"aggregations\":[{\"column\":\"SALARY\",\"name\":\"total_salary\",\"function\":\"sum\"},{\"column\":\"annual_bonus\",\"name\":\"total_bonus\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating total salary and bonus by department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_salary\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total salary in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data with columns for date, consumption in kWh, and other related metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"},{\"column\":\"col_3\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location where the energy consumption was recorded.\"}]},\"table_id2\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy_prices.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains historical energy prices with columns for date, price per kWh, and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price_per_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kilowatt-hour.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region where the energy price was recorded.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Filter the data to include only records where consumption is greater than 100 kWh.\",\"Aggregate the filtered data to calculate the total consumption per month.\",\"Sort the aggregated data by total consumption in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"consumption_kWh > 100\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering data to include only records where consumption is greater than 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_consumption\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"consumption_kWh\",\"name\":\"total_monthly_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_consumption\",\"comment\":\"Aggregating filtered data to calculate total consumption per month.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_consumption\",\"sortBy\":\"total_monthly_consumption\",\"order\":\"desc\"},\"output\":\"sorted_monthly_consumption\",\"comment\":\"Sorting aggregated data by total consumption in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains energy consumption data for the building, including timestamps and number of occupants.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"occupants\",\"column_type\":\"xsd:integer\",\"column_description\":\"The number of occupants in the building at the time of recording.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_2023.xlsx\",\"sheet_name\":\"Weather\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature and humidity readings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The recorded temperature in Celsius.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The recorded humidity percentage.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for building equipment.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"equipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for the equipment.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"},{\"column\":\"col_3\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"The status of the maintenance task.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from the year 2023.\",\"Filter the weather data to include only records from the year 2023.\",\"Join the filtered energy consumption data with the filtered weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing energy consumption by the number of occupants.\",\"Aggregate the data by month to calculate the total energy consumption and average temperature.\",\"Sort the aggregated data by total energy consumption in descending order.\",\"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_2023.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"timestamp >= '2023-01-01' and timestamp < '2024-01-01'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering energy consumption data to include only records from the year 2023.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"weather_data\",\"query\":\"date >= '2023-01-01' and date < '2024-01-01'\"},\"output\":\"filtered_weather_data\",\"comment\":\"Filtering weather data to include only records from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"filtered_weather_data\"],\"joinOn\":[\"timestamp\",\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered energy consumption data with filtered weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"energy_usage / occupants\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating energy efficiency by dividing energy consumption by the number of occupants.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"month\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating data by month to calculate total energy consumption and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total energy consumption in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"columns\":[\"total_energy\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"month\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_data.xlsx\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"This table contains sales records including date, product category, and sales amount.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the sales record.\"},{\"column\":\"col_2\",\"column_name\":\"product_category\",\"column_type\":\"xsd:string\",\"column_description\":\"The category of the product sold.\"},{\"column\":\"col_3\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of sales in dollars.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customer_data.xlsx\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"This table contains customer information including customer ID and demographics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"The age of the customer.\"}]}},\"instructions\":[\"Load the sales data from the Excel file located at C:/data/sales_data.xlsx.\",\"Filter the sales data to include only records from the year 2023.\",\"Calculate the total sales for each product category.\",\"Apply a rolling mean transformation with a window size of 7 days to smooth the sales data.\",\"Forecast the next 30 days of sales using the Holt-Winters method with a seasonal period of 7 days.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"sales_data_2023\",\"comment\":\"Filtering sales data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_data_2023\",\"groupBy\":\"product_category\",\"aggregations\":[{\"column\":\"sales_amount\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"total_sales_by_category\",\"comment\":\"Calculating the total sales for each product category.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"total_sales_by_category\",\"transformations\":[{\"columns\":[\"total_sales\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":7}}]},\"output\":\"smoothed_sales_data\",\"comment\":\"Applying a rolling mean transformation with a window size of 7 days to smooth the sales data.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"smoothed_sales_data\",\"columns\":[\"total_sales\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"dateColumn\":\"date\"}},\"output\":\"sales_forecast\",\"comment\":\"Forecasting the next 30 days of sales using the Holt-Winters method with a seasonal period of 7 days.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customers\",\"location\":\"C:/data/customers.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"Contains information about customers including their ID, name, and contact details.\",\"columns\":[{\"column\":\"customer_id\",\"column_name\":\"CUSTOMER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"customer_name\",\"column_name\":\"CUSTOMER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"contact_email\",\"column_name\":\"CONTACT_EMAIL\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]},\"table_id2\":{\"name\":\"sales\",\"location\":\"C:/data/sales.xlsx\",\"sheet_name\":\"SalesData\",\"label\":\"Sales Data\",\"description\":\"Contains sales transactions including transaction ID, customer ID, date, and amount.\",\"columns\":[{\"column\":\"transaction_id\",\"column_name\":\"TRANSACTION_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"customer_id\",\"column_name\":\"CUSTOMER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking to the customer who made the purchase.\"},{\"column\":\"transaction_date\",\"column_name\":\"TRANSACTION_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the transaction occurred.\"},{\"column\":\"amount\",\"column_name\":\"AMOUNT\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total amount of the transaction.\"}]},\"table_id3\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing_campaigns.csv\",\"sheet_name\":null,\"label\":\"Marketing Campaigns\",\"description\":\"Contains details of past marketing campaigns including campaign ID, name, and effectiveness.\",\"columns\":[{\"column\":\"campaign_id\",\"column_name\":\"CAMPAIGN_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"campaign_name\",\"column_name\":\"CAMPAIGN_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the marketing campaign.\"},{\"column\":\"effectiveness\",\"column_name\":\"EFFECTIVENESS\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Effectiveness score of the campaign.\"}]}},\"instructions\":[\"Load the customer data from the CSV file located at C:/data/customers.csv.\",\"Load the sales data from the Excel file located at C:/data/sales.xlsx.\",\"Filter the sales data to include only transactions from the last quarter.\",\"Join the filtered sales data with the customer data on the customer_id column.\",\"Calculate the total sales for each customer.\",\"Sort the customers by total sales in descending order.\",\"Identify the top 10 customers based on total sales.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.csv\",\"type\":\"csv\"},\"output\":\"customers\",\"comment\":\"Loading customer data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales.xlsx\",\"type\":\"xlsx\"},\"output\":\"sales\",\"comment\":\"Loading sales data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales\",\"query\":\"`transaction_date` >= '2023-07-01' and `transaction_date` <= '2023-09-30'\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions from the last quarter.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customers\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer data on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_customers\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"customer_sales\",\"comment\":\"Calculating total sales for each customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"customer_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_customers\",\"comment\":\"Sorting customers by total sales in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_customers\",\"query\":\"index < 10\"},\"output\":\"top_10_customers\",\"comment\":\"Identifying the top 10 customers based on total sales.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains the manufacturing schedule for the upcoming month, including order details and priority levels.\",\"columns\":[{\"column\":\"order_id\",\"column_name\":\"ORDER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each order.\"},{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product being manufactured.\"},{\"column\":\"priority\",\"column_name\":\"PRIORITY\",\"column_type\":\"xsd:string\",\"column_description\":\"Priority level of the order (e.g., high, medium, low).\"},{\"column\":\"production_time\",\"column_name\":\"PRODUCTION_TIME\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Estimated production time in hours for the order.\"}]},\"table_id2\":{\"name\":\"inventory_status\",\"location\":\"C:/data/manufacturing/inventory_status.xlsx\",\"sheet_name\":\"Current\",\"label\":\"Current Inventory Status\",\"description\":\"This table provides the current status of inventory items, including quantities and reorder levels.\",\"columns\":[{\"column\":\"item_id\",\"column_name\":\"ITEM_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:integer\",\"column_description\":\"Current quantity of the item in stock.\"},{\"column\":\"reorder_level\",\"column_name\":\"REORDER_LEVEL\",\"column_type\":\"xsd:integer\",\"column_description\":\"The stock level at which the item should be reordered.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the manufacturing schedule for the upcoming month.\",\"Filter the production schedule to include only high-priority orders.\",\"Aggregate the filtered schedule to calculate the total production time required for each product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the manufacturing schedule for the upcoming month from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"PRIORITY == 'high'\"},\"output\":\"high_priority_orders\",\"comment\":\"Filtering the production schedule to include only high-priority orders.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"high_priority_orders\",\"groupBy\":\"PRODUCT_ID\",\"aggregations\":[{\"column\":\"PRODUCTION_TIME\",\"name\":\"total_production_time\",\"function\":\"sum\"}]},\"output\":\"aggregated_production_time\",\"comment\":\"Calculating the total production time required for each product.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Rating given by the customer.\"},{\"column\":\"col_3\",\"column_name\":\"comment\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comment from the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Table containing data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for irrelevant data.\"},{\"column\":\"col_2\",\"column_name\":\"value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Some irrelevant value.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Load the customer details table from the Excel file.\",\"Filter the feedback table to include only feedback with a rating of 4 or higher.\",\"Join the filtered feedback table with the customer details table on the customer ID.\",\"Aggregate the joined table to calculate the average rating per customer.\",\"Sort the aggregated data by average rating in descending order.\",\"Calculate a new column to determine if the average rating is above 4.5.\",\"Filter the sorted data to include only customers with an average rating above 4.5.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"filtered_feedback\",\"comment\":\"Filtering feedback to include only ratings of 4 or higher.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating average rating per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting data by average rating in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_data\",\"columnName\":\"is_high_rating\",\"formula\":\"average_rating > 4.5\"},\"output\":\"data_with_high_rating\",\"comment\":\"Determining if the average rating is above 4.5.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"data_with_high_rating\",\"query\":\"is_high_rating == True\"},\"output\":\"high_rating_customers\",\"comment\":\"Filtering data to include only customers with an average rating above 4.5.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Rating given by the customer, ranging from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comments provided by the customer.\"}]},\"table_2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]},\"table_3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.xlsx\",\"sheet_name\":\"Products\",\"label\":\"Product Catalog\",\"description\":\"Table containing information about products available for sale.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback data to include only entries with a rating below 3.\",\"Load the customer details table from the Excel file.\",\"Join the filtered feedback data with the customer details on the customer ID.\",\"Aggregate the joined data to count the number of complaints per customer.\",\"Sort the aggregated data by the number of complaints in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating < 3\"},\"output\":\"low_rating_feedback\",\"comment\":\"Filtering feedback to include only entries with a rating below 3.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_rating_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_feedback_details\",\"comment\":\"Joining filtered feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_feedback_details\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"customer_id\",\"name\":\"complaint_count\",\"function\":\"count\"}]},\"output\":\"complaints_per_customer\",\"comment\":\"Aggregating data to count the number of complaints per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"complaints_per_customer\",\"sortBy\":\"complaint_count\",\"order\":\"desc\"},\"output\":\"sorted_complaints\",\"comment\":\"Sorting the aggregated data by the number of complaints in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/data/legal/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases\",\"description\":\"A table containing details of various legal cases including case ID, lawyer ID, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"case_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/data/legal/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers\",\"description\":\"A table containing details of lawyers including lawyer ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_judges\",\"location\":\"C:/data/legal/court_judges.csv\",\"sheet_name\":null,\"label\":\"Court Judges\",\"description\":\"A table containing details of court judges, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"judge_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each judge.\"},{\"column\":\"col_2\",\"column_name\":\"judge_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the judge.\"}]}},\"instructions\":[\"Load the table 'court_cases' which contains details of various legal cases.\",\"Filter the 'court_cases' table to include only cases from the year 2023.\",\"Join the filtered 'court_cases' table with the 'lawyers' table on the lawyer_id column.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"`case_date`.dt.year == 2023\"},\"output\":\"filtered_cases\",\"comment\":\"Filtering court cases to include only those from the year 2023.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/lawyers.csv\",\"type\":\"csv\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_cases\",\"lawyers\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"cases_with_lawyers\",\"comment\":\"Joining filtered court cases with lawyers data on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"cases_with_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating data to find the total number of cases handled by each lawyer.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if a formula results in division by zero?","output":"If division by zero occurs, the result is 'null'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid subscript expression using indexing.","output":"'col1[0] + col2[1]'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/data/legal/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases\",\"description\":\"A table containing details of various legal cases including case ID, lawyer ID, and case year.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"case_year\",\"column_type\":\"xsd:integer\",\"column_description\":\"The year the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/data/legal/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers\",\"description\":\"A table containing details of lawyers including lawyer ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_judges\",\"location\":\"C:/data/legal/court_judges.csv\",\"sheet_name\":null,\"label\":\"Court Judges\",\"description\":\"A table containing details of judges including judge ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"judge_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each judge.\"},{\"column\":\"col_2\",\"column_name\":\"judge_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the judge.\"}]}},\"instructions\":[\"Load the table 'court_cases' which contains details of various legal cases.\",\"Filter the 'court_cases' table to include only cases from the year 2022.\",\"Join the filtered 'court_cases' table with the 'lawyers' table on the lawyer_id column.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"case_year == 2022\"},\"output\":\"filtered_cases_2022\",\"comment\":\"Filtering court cases to include only those from the year 2022.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/lawyers.csv\",\"type\":\"csv\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_cases_2022\",\"lawyers\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"cases_with_lawyers\",\"comment\":\"Joining filtered cases with lawyers data on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"cases_with_lawyers\",\"groupBy\":\"lawyer_name\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating to find the total number of cases handled by each lawyer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/hospital/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"A table containing detailed records of patients including their diagnoses, age, and other medical information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"age\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Age of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_staff\",\"location\":\"C:/data/hospital/staff_records.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff Records\",\"description\":\"A table containing records of hospital staff including their roles and departments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"role\",\"column_type\":\"xsd:string\",\"column_description\":\"Role of the staff member in the hospital.\"}]}},\"instructions\":[\"Load the patient records table from the hospital database.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Calculate the average age of patients with diabetes.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hospital/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records from the hospital database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetes_patients\",\"comment\":\"Filtering patient records to include only those with a diagnosis of diabetes.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"diabetes_patients\",\"groupBy\":null,\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"average_age_diabetes_patients\",\"comment\":\"Calculating the average age of patients with diabetes.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/healthcare/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"A table containing patient information including age, BMI, and other health metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"Patient_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"Age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"BMI\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Body Mass Index of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_visits\",\"location\":\"C:/data/healthcare/hospital_visits.xlsx\",\"sheet_name\":\"Visits\",\"label\":\"Hospital Visits\",\"description\":\"A table detailing hospital visit records including patient ID and visit dates.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"Visit_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital visit.\"},{\"column\":\"col_2\",\"column_name\":\"Patient_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_3\",\"column_name\":\"Visit_Date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the hospital visit.\"}]}},\"instructions\":[\"Load the patient records table from the provided CSV file.\",\"Filter the patient records to include only those with a BMI greater than 25.\",\"Aggregate the filtered data to find the average age of patients with a BMI greater than 25.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/healthcare/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"BMI > 25\"},\"output\":\"filtered_patients\",\"comment\":\"Filtering patient records to include only those with a BMI greater than 25.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_patients\",\"groupBy\":null,\"aggregations\":[{\"column\":\"Age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"average_age_bmi_over_25\",\"comment\":\"Calculating the average age of patients with a BMI greater than 25.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid arithmetic expression using subtraction and multiplication.","output":"'col1 - col2 * 3'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_usage\",\"location\":\"C:/data/building_management/energy_usage.csv\",\"sheet_name\":null,\"label\":\"Energy Usage Data\",\"description\":\"This table contains daily energy consumption data for the building, including date and energy consumption in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"}]},\"table_id2\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/building_management/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for building equipment.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"equipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for each piece of equipment.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"}]}},\"instructions\":[\"Load the table 'energy_usage' which contains daily energy consumption data for the building.\",\"Filter the data to include only records where energy consumption exceeds 1000 kWh.\",\"Aggregate the filtered data to calculate the total energy consumption for each month.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_management/energy_usage.csv\",\"type\":\"csv\"},\"output\":\"energy_usage\",\"comment\":\"Loading the energy usage data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_usage\",\"query\":\"energy_consumption > 1000\"},\"output\":\"high_energy_usage\",\"comment\":\"Filtering the data to include only records where energy consumption exceeds 1000 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"high_energy_usage\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"energy_consumption\",\"name\":\"monthly_total_energy\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Aggregating the filtered data to calculate the total energy consumption for each month.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'rolling_mean' operation be used?","output":"Use 'rolling_mean' when you need to compute a moving average over a specific time window, such as smoothing daily stock price fluctuations."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/finance/transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing all financial transactions with details such as transaction ID, customer ID, and amount.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer associated with the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The monetary amount of the transaction.\"}]},\"table_id2\":{\"name\":\"customer_info\",\"location\":\"C:/data/finance/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Information\",\"description\":\"A table containing customer details such as customer ID, name, and contact information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_info\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact information for the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_financial_data\",\"location\":\"C:/data/finance/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Financial Data\",\"description\":\"A table containing financial data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the data entry.\"},{\"column\":\"col_B\",\"column_name\":\"data_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Value of the financial data.\"}]}},\"instructions\":[\"Load the table 'financial_transactions' from the CSV file located at C:/data/finance/transactions.csv.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Join the filtered transactions with the 'customer_info' table on the 'customer_id' column.\",\"Aggregate the joined data to calculate the total transaction amount for each customer.\",\"Sort the aggregated data by total transaction amount in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_info\",\"comment\":\"Loading the customer information dataset from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_info\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer information based on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate the total transaction amount for each customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_transaction_amount\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total transaction amount in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"Contains detailed records of employees including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.csv\",\"sheet_name\":null,\"label\":\"Department Data\",\"description\":\"Contains information about departments including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"project_assignments\",\"location\":\"C:/data/project_assignments.csv\",\"sheet_name\":null,\"label\":\"Project Assignments\",\"description\":\"Contains records of employee assignments to various projects.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"}]}},\"instructions\":[\"Load the employee records from the CSV file located at C:/data/employee_records.csv.\",\"Filter the employee records to include only those with a salary greater than 50,000.\",\"Join the filtered employee records with the department data on the department_id column.\",\"Aggregate the joined data to calculate the average salary per department.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employee records to include only those with a salary greater than 50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.csv\",\"type\":\"csv\"},\"output\":\"department_data\",\"comment\":\"Loading the department data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_salary_employees\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered employee records with department data on department_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"average_salary_per_department\",\"comment\":\"Aggregating the joined data to calculate the average salary per department.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials\",\"description\":\"This table contains information about the raw materials used in the manufacturing process, including material ID, name, quantity, and unit price.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"column_2_id\",\"column_name\":\"material_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the material.\"},{\"column\":\"column_3_id\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the material available.\"},{\"column\":\"column_4_id\",\"column_name\":\"unit_price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price per unit of the material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains the schedule for manufacturing different products, including product ID, material ID, and scheduled production date.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material used in the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"scheduled_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the product is scheduled for production.\"}]},\"table_id3\":{\"name\":\"employee_data\",\"location\":\"C:/data/manufacturing/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"This table contains information about employees involved in the manufacturing process, including employee ID, name, and role.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"column_2_id\",\"column_name\":\"employee_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"column_3_id\",\"column_name\":\"role\",\"column_type\":\"xsd:string\",\"column_description\":\"Role of the employee in the manufacturing process.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains information about the materials used in manufacturing.\",\"Load the table 'production_schedule' which contains the schedule for manufacturing different products.\",\"Filter the 'raw_materials' table to include only materials with a quantity greater than 100.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Calculate a new column 'total_cost' in the joined table by multiplying 'quantity' with 'unit_price'.\",\"Aggregate the data by 'product_id' to find the total cost of materials for each product.\",\"Sort the aggregated data by 'total_cost' in descending order to identify the most expensive products to manufacture.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"quantity > 100\"},\"output\":\"filtered_raw_materials\",\"comment\":\"Filtering raw materials to include only those with a quantity greater than 100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_raw_materials\",\"production_schedule\"],\"joinOn\":[\"material_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered raw materials with production schedule on material_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"total_cost\",\"formula\":\"quantity * unit_price\"},\"output\":\"joined_data_with_cost\",\"comment\":\"Calculating total cost by multiplying quantity with unit price.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_cost\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"total_cost\",\"name\":\"total_material_cost\",\"function\":\"sum\"}]},\"output\":\"aggregated_costs\",\"comment\":\"Aggregating data by product_id to find the total cost of materials for each product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_costs\",\"sortBy\":\"total_material_cost\",\"order\":\"desc\"},\"output\":\"sorted_costs\",\"comment\":\"Sorting the aggregated data by total cost in descending order.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/cost_analysis.csv\",\"type\":\"csv\"},\"output\":\"cost_analysis\",\"comment\":\"Saving the sorted data to a new table 'cost_analysis'.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/healthcare_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains patient information including ID, diagnosis, and blood pressure readings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"blood_pressure\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Blood pressure reading of the patient.\"}]},\"table_id2\":{\"name\":\"medication_data\",\"location\":\"C:/pharmacy_data/medication_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Medication Data\",\"description\":\"Details of medications prescribed to patients, including patient ID and medication name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/healthcare_data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Information about hospital staff, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file located in the healthcare data directory.\",\"Filter the patient records to include only those with a diagnosis of hypertension.\",\"Load the medication data from the Excel file located in the pharmacy data directory.\",\"Join the filtered patient records with the medication data on the patient ID.\",\"Calculate the average blood pressure for each patient in the joined table.\",\"Sort the resulting table by average blood pressure in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from the CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'hypertension'\"},\"output\":\"hypertension_patients\",\"comment\":\"Filtering patient records to include only those with hypertension.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/pharmacy_data/medication_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"medication_data\",\"comment\":\"Loading medication data from the Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"hypertension_patients\",\"medication_data\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient records with medication data on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"blood_pressure\",\"name\":\"average_blood_pressure\",\"function\":\"mean\"}]},\"output\":\"average_blood_pressure_data\",\"comment\":\"Calculating the average blood pressure for each patient.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_blood_pressure_data\",\"sortBy\":\"average_blood_pressure\",\"order\":\"desc\"},\"output\":\"sorted_blood_pressure_data\",\"comment\":\"Sorting the table by average blood pressure in descending order.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/analysis_results/sorted_blood_pressure_data.csv\",\"type\":\"csv\"},\"output\":\"sorted_blood_pressure_data\",\"comment\":\"Saving the sorted table as a new CSV file.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/data/legal/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases\",\"description\":\"A table containing details of various court cases including case ID, lawyer ID, and case date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"case_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the court case was filed.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/data/legal/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers\",\"description\":\"A table containing information about lawyers including their ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_judges\",\"location\":\"C:/data/legal/court_judges.csv\",\"sheet_name\":null,\"label\":\"Court Judges\",\"description\":\"A table containing information about judges, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"judge_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each judge.\"},{\"column\":\"col_2\",\"column_name\":\"judge_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the judge.\"}]}},\"instructions\":[\"Load the 'court_cases' table containing details of various court cases.\",\"Filter the 'court_cases' table to include only cases from the year 2023.\",\"Join the filtered 'court_cases' table with the 'lawyers' table on the lawyer_id column.\",\"Aggregate the joined table to count the number of cases handled by each lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"`case_date`.dt.year == 2023\"},\"output\":\"court_cases_2023\",\"comment\":\"Filtering court cases to include only those from the year 2023.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/lawyers.csv\",\"type\":\"csv\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"court_cases_2023\",\"lawyers\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"cases_with_lawyers\",\"comment\":\"Joining the filtered court cases with the lawyers table on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"cases_with_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"case_count\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating the data to count the number of cases handled by each lawyer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and energy usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Timestamp of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Energy usage in kilowatt-hours (kWh).\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.csv\",\"sheet_name\":null,\"label\":\"Building Information\",\"description\":\"Contains information about buildings, such as address and type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"address\",\"column_type\":\"xsd:string\",\"column_description\":\"Address of the building.\"},{\"column\":\"col_3\",\"column_name\":\"building_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of the building, e.g., residential, commercial.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.csv\",\"sheet_name\":null,\"label\":\"Maintenance Schedule\",\"description\":\"Schedule of maintenance activities for buildings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Date of the scheduled maintenance.\"},{\"column\":\"col_3\",\"column_name\":\"activity\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the maintenance activity.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records where energy usage exceeds 1000 kWh.\",\"Aggregate the filtered data to calculate the total energy consumption per building.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"energy_usage > 1000\"},\"output\":\"high_usage_data\",\"comment\":\"Filtering the data to include only records where energy usage exceeds 1000 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"high_usage_data\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"total_energy_per_building\",\"comment\":\"Aggregating the filtered data to calculate the total energy consumption per building.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"building_energy_data\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and consumption values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.xlsx\",\"sheet_name\":\"Info\",\"label\":\"Building Information\",\"description\":\"Contains additional information about buildings such as location and type.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_B\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location of the building.\"},{\"column\":\"col_C\",\"column_name\":\"building_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of the building (e.g., residential, commercial).\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each building.\",\"Identify buildings with energy consumption above 10,000 kWh.\",\"Sort the filtered buildings by their energy consumption in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"building_energy_data\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"building_energy_data\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"energy_data_2023\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_data_2023\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_consumption\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"total_energy_per_building\",\"comment\":\"Calculating the total energy consumption for each building.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"total_energy_per_building\",\"query\":\"`total_energy_consumption` > 10000\"},\"output\":\"high_energy_buildings\",\"comment\":\"Identifying buildings with energy consumption above 10,000 kWh.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"high_energy_buildings\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_high_energy_buildings\",\"comment\":\"Sorting the filtered buildings by their energy consumption in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data with columns for date, consumption in kWh, and other relevant metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"},{\"column\":\"col_3\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location where the energy consumption was recorded.\"}]},\"table_id2\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy_prices.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains historical energy prices with columns for date, price per kWh, and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price_per_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kilowatt-hour.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region where the energy price was recorded.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Filter the data to include only records where consumption is greater than 1000 kWh.\",\"Calculate the average daily consumption.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"consumption_kwh > 1000\"},\"output\":\"high_consumption_data\",\"comment\":\"Filtering data to include only records where consumption is greater than 1000 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"high_consumption_data\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"consumption_kwh\",\"name\":\"average_daily_consumption\",\"function\":\"mean\"}]},\"output\":\"average_daily_consumption\",\"comment\":\"Calculating the average daily consumption.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'forecastData' action with an unsupported forecasting algorithm.","output":"{ \"name\": \"forecastData\", \"parameters\": { \"table\": \"sales_data\", \"columns\": [\"revenue\"], \"forecastParameters\": { \"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"date\", \"algorithm\": \"unsupported_algorithm\", \"confidenceInterval\": 95 }, \"output\": \"forecast_results\" } } // Error: 'unsupported_algorithm' is not a valid forecasting model."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2023\",\"description\":\"This table contains sales records for the year 2023, including transaction dates, total revenue, and other sales-related information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the sales transaction.\"},{\"column\":\"col_2\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total revenue generated from the sales transaction.\"},{\"column\":\"col_3\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The identifier for the product sold.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customers_2023.xlsx\",\"sheet_name\":\"CustomerInfo\",\"label\":\"Customer Information\",\"description\":\"This table contains customer information including customer IDs, names, and contact details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_number\",\"column_type\":\"xsd:string\",\"column_description\":\"The contact number of the customer.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Filter the sales data to include only records with a total revenue greater than 1000.\",\"Aggregate the filtered sales data by month to calculate the total revenue for each month.\",\"Sort the aggregated monthly sales data in ascending order by month.\",\"Apply a rolling mean transformation with a window size of 3 to smooth the monthly revenue data.\",\"Forecast the next 6 months of sales using the Holt-Winters method with a seasonal period of 12.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data for 2023 from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`total_revenue` > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_sales\",\"groupBy\":\"transaction_date\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"monthly_total_revenue\",\"function\":\"sum\"}]},\"output\":\"monthly_sales\",\"comment\":\"Aggregating sales data by month to calculate total revenue.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_sales\",\"sortBy\":\"transaction_date\",\"order\":\"asc\"},\"output\":\"sorted_monthly_sales\",\"comment\":\"Sorting the aggregated monthly sales data by month in ascending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_monthly_sales\",\"transformations\":[{\"columns\":[\"monthly_total_revenue\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":3}}]},\"output\":\"smoothed_sales\",\"comment\":\"Applying a rolling mean transformation with a window size of 3 to smooth the monthly revenue data.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"smoothed_sales\",\"columns\":[\"monthly_total_revenue\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"transaction_date\"}},\"output\":\"sales_forecast\",\"comment\":\"Forecasting the next 6 months of sales using the Holt-Winters method with a seasonal period of 12.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Contains customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"feedback_id\",\"column_name\":\"FEEDBACK_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Unique identifier for each feedback entry.\"},{\"column\":\"customer_id\",\"column_name\":\"CUSTOMER_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Identifier linking feedback to a specific customer.\"},{\"column\":\"rating\",\"column_name\":\"RATING\",\"column_type\":\"xsd:integer\",\"column_description\":\"Rating given by the customer, ranging from 1 to 5.\"},{\"column\":\"comment\",\"column_name\":\"COMMENT\",\"column_type\":\"xsd:string\",\"column_description\":\"Textual feedback provided by the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Contains personal details of customers.\",\"columns\":[{\"column\":\"customer_id\",\"column_name\":\"CUSTOMER_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"name\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Full name of the customer.\"},{\"column\":\"email\",\"column_name\":\"EMAIL\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"},{\"column\":\"phone\",\"column_name\":\"PHONE\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact phone number of the customer.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"Contains sales data for various products.\",\"columns\":[{\"column\":\"sale_id\",\"column_name\":\"SALE_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Unique identifier for each sale.\"},{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:integer\",\"column_description\":\"Identifier for the product sold.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of units sold.\"},{\"column\":\"sale_date\",\"column_name\":\"SALE_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the sale was made.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Load the customer details table from the Excel file.\",\"Filter the feedback table to include only feedback with ratings less than 3.\",\"Join the filtered feedback with customer details on customer ID.\",\"Aggregate the joined table to count the number of negative feedbacks per customer.\",\"Sort the aggregated data by the count of negative feedbacks in descending order.\",\"Calculate a new column to determine the percentage of negative feedbacks for each customer.\",\"Filter the sorted data to include only customers with more than 50% negative feedback.\",\"Join the filtered data with the original customer details to get full customer information.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"RATING < 3\"},\"output\":\"negative_feedback\",\"comment\":\"Filtering feedback to include only entries with ratings less than 3.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"negative_feedback\",\"customer_details\"],\"joinOn\":[\"CUSTOMER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_feedback_details\",\"comment\":\"Joining filtered feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_feedback_details\",\"groupBy\":\"CUSTOMER_ID\",\"aggregations\":[{\"column\":\"FEEDBACK_ID\",\"name\":\"negative_feedback_count\",\"function\":\"count\"}]},\"output\":\"aggregated_feedback\",\"comment\":\"Aggregating data to count negative feedbacks per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_feedback\",\"sortBy\":\"negative_feedback_count\",\"order\":\"desc\"},\"output\":\"sorted_feedback\",\"comment\":\"Sorting aggregated data by negative feedback count in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_feedback\",\"columnName\":\"negative_feedback_percentage\",\"formula\":\"negative_feedback_count / sum(negative_feedback_count) * 100\"},\"output\":\"feedback_with_percentage\",\"comment\":\"Calculating the percentage of negative feedbacks for each customer.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"feedback_with_percentage\",\"query\":\"negative_feedback_percentage > 50\"},\"output\":\"high_negative_feedback_customers\",\"comment\":\"Filtering data to include only customers with more than 50% negative feedback.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_negative_feedback_customers\",\"customer_details\"],\"joinOn\":[\"CUSTOMER_ID\"],\"joinType\":\"inner\"},\"output\":\"final_customer_info\",\"comment\":\"Joining filtered data with original customer details for full information.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does a Tamarind workflow consist of?","output":"A Tamarind workflow consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.csv\",\"sheet_name\":null,\"label\":\"Department Data\",\"description\":\"Contains information about departments including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"project_data\",\"location\":\"C:/data/project_data.csv\",\"sheet_name\":null,\"label\":\"Project Data\",\"description\":\"Contains information about projects including project IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"project_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the project.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/employee_data.csv.\",\"Filter the employee data to include only those with a salary greater than 50,000.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Aggregate the joined data to calculate the average salary per department.\",\"Sort the aggregated data by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading the employee data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employee data to include only those with a salary greater than 50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.csv\",\"type\":\"csv\"},\"output\":\"department_data\",\"comment\":\"Loading the department data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employee_data\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating joined data to calculate the average salary per department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by average salary in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains the energy consumption data for the building, including date, energy usage, and building area.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"},{\"column\":\"col_3\",\"column_name\":\"building_area\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The area of the building in square meters.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including date, temperature, and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity on the given date.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for the building, including dates and tasks.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the scheduled maintenance.\"},{\"column\":\"col_2\",\"column_name\":\"task\",\"column_type\":\"xsd:string\",\"column_description\":\"The maintenance task to be performed.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from the last year.\",\"Filter the weather data to include only records from the last year.\",\"Join the filtered energy consumption data with the filtered weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing energy consumption by the building area.\",\"Aggregate the joined data by month to calculate the total energy consumption and average temperature.\",\"Sort the aggregated data by total energy consumption in descending order.\",\"Apply a rolling mean transformation to the energy efficiency column over a 3-month window.\",\"Forecast the next 6 months of energy consumption using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date` >= '2022-01-01'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records from the last year.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"weather_data\",\"query\":\"`date` >= '2022-01-01'\"},\"output\":\"filtered_weather_data\",\"comment\":\"Filtering the weather data to include only records from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"filtered_weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered energy consumption data with the filtered weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`energy_usage` / `building_area`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating a new column for energy efficiency by dividing energy consumption by the building area.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating the joined data by month to calculate the total energy consumption and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total energy consumption in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"transformations\":[{\"columns\":[\"energy_efficiency\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":3}}]},\"output\":\"transformed_data\",\"comment\":\"Applying a rolling mean transformation to the energy efficiency column over a 3-month window.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_data\",\"columns\":[\"total_energy_consumption\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"forecast_results\",\"comment\":\"Forecasting the next 6 months of energy consumption using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with details such as transaction ID, amount, currency code, and transaction type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code of the transaction amount.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of transaction, e.g., sale, refund.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/exchange_rates.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Exchange Rates\",\"description\":\"A table containing exchange rates for various currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"NotNeeded\",\"label\":\"Irrelevant Data\",\"description\":\"A table that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at C:/data/financial_transactions.csv.\",\"Load the 'exchange_rates' table from the Excel file located at C:/data/exchange_rates.xlsx.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Join the filtered 'financial_transactions' table with the 'exchange_rates' table on the 'currency_code' column.\",\"Calculate a new column 'amount_in_usd' in the joined table by multiplying 'amount' with 'exchange_rate'.\",\"Aggregate the data by 'transaction_type' to calculate the total 'amount_in_usd' for each type.\",\"Sort the aggregated data by 'total_amount_in_usd' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with exchange rates on the currency code.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"amount_in_usd\",\"formula\":\"amount * exchange_rate\"},\"output\":\"transactions_with_usd\",\"comment\":\"Calculating the amount in USD by multiplying the amount with the exchange rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_usd\",\"groupBy\":\"transaction_type\",\"aggregations\":[{\"column\":\"amount_in_usd\",\"name\":\"total_amount_in_usd\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by transaction type to calculate total amount in USD.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_amount_in_usd\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total amount in USD in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of all shipments including product IDs, quantities, and status.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product being shipped.\"},{\"column\":\"col_3\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the product in the shipment.\"},{\"column\":\"col_4\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment (e.g., pending, shipped, delivered).\"}]},\"table_id2\":{\"name\":\"warehouse_inventory\",\"location\":\"C:/data/logistics/warehouse_inventory.csv\",\"sheet_name\":null,\"label\":\"Warehouse Inventory\",\"description\":\"Contains current inventory levels for each product in different warehouses.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product in the warehouse.\"},{\"column\":\"col_3\",\"column_name\":\"inventory_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current inventory level of the product in the warehouse.\"}]},\"table_id3\":{\"name\":\"transport_routes\",\"location\":\"C:/data/logistics/transport_routes.csv\",\"sheet_name\":null,\"label\":\"Transport Routes\",\"description\":\"Contains information about various transport routes and their capacities.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"route_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transport route.\"},{\"column\":\"col_2\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Maximum capacity of the transport route.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of all shipments.\",\"Load the table 'warehouse_inventory' which contains current inventory levels.\",\"Filter the 'shipment_data' to include only shipments with a status of 'pending'.\",\"Join the filtered 'shipment_data' with 'warehouse_inventory' on 'product_id'.\",\"Calculate a new column 'inventory_shortage' by subtracting 'quantity' from 'inventory_level'.\",\"Filter the joined table to include only rows where 'inventory_shortage' is greater than zero.\",\"Aggregate the filtered data by 'warehouse_id' to calculate the total 'inventory_shortage'.\",\"Sort the aggregated data by 'total_inventory_shortage' in descending order.\",\"Identify the top 5 warehouses with the highest inventory shortages.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data containing details of all shipments.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouse_inventory.csv\",\"type\":\"csv\"},\"output\":\"warehouse_inventory\",\"comment\":\"Loading the warehouse inventory data containing current inventory levels.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"`status` == 'pending'\"},\"output\":\"pending_shipments\",\"comment\":\"Filtering shipment data to include only pending shipments.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"pending_shipments\",\"warehouse_inventory\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining pending shipments with warehouse inventory on product_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"inventory_shortage\",\"formula\":\"`quantity` - `inventory_level`\"},\"output\":\"joined_data_with_shortage\",\"comment\":\"Calculating inventory shortage by subtracting inventory level from quantity.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"joined_data_with_shortage\",\"query\":\"`inventory_shortage` > 0\"},\"output\":\"shortage_data\",\"comment\":\"Filtering data to include only rows where inventory shortage is greater than zero.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"shortage_data\",\"groupBy\":\"warehouse_id\",\"aggregations\":[{\"column\":\"inventory_shortage\",\"name\":\"total_inventory_shortage\",\"function\":\"sum\"}]},\"output\":\"aggregated_shortage_data\",\"comment\":\"Aggregating data by warehouse_id to calculate total inventory shortage.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_shortage_data\",\"sortBy\":\"total_inventory_shortage\",\"order\":\"desc\"},\"output\":\"sorted_shortage_data\",\"comment\":\"Sorting aggregated data by total inventory shortage in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_shortage_data\",\"query\":\"index < 5\"},\"output\":\"top_5_warehouses\",\"comment\":\"Identifying the top 5 warehouses with the highest inventory shortages.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customers\",\"location\":\"C:/data/marketing/customers.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"Contains customer information including spending habits.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"annual_spending\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total annual spending of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id2\":{\"name\":\"campaigns\",\"location\":\"C:/data/marketing/campaigns.csv\",\"sheet_name\":null,\"label\":\"Campaign Data\",\"description\":\"Contains information about marketing campaigns.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each campaign.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking customer to campaign.\"},{\"column\":\"col_3\",\"column_name\":\"campaign_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the marketing campaign.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/marketing/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant identifier.\"},{\"column\":\"col_2\",\"column_name\":\"irrelevant_info\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant information.\"}]}},\"instructions\":[\"Load the customer data from the CSV file located at C:/data/marketing/customers.csv.\",\"Filter the customer data to include only those with an annual spending greater than $5000.\",\"Join the filtered customer data with the campaign data on the customer_id column.\",\"Aggregate the joined data to calculate the total spending per campaign.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customers.csv\",\"type\":\"csv\"},\"output\":\"customers\",\"comment\":\"Loading customer data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customers\",\"query\":\"annual_spending > 5000\"},\"output\":\"high_spending_customers\",\"comment\":\"Filtering customers with annual spending greater than $5000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/campaigns.csv\",\"type\":\"csv\"},\"output\":\"campaigns\",\"comment\":\"Loading campaign data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_spending_customers\",\"campaigns\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"customer_campaign_data\",\"comment\":\"Joining filtered customer data with campaign data on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"customer_campaign_data\",\"groupBy\":\"campaign_id\",\"aggregations\":[{\"column\":\"annual_spending\",\"name\":\"total_spending\",\"function\":\"sum\"}]},\"output\":\"campaign_spending_summary\",\"comment\":\"Aggregating data to calculate total spending per campaign.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'dropna' operation do?","output":"The 'dropna' operation removes rows with missing values."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_records\",\"location\":\"C:/legal_data/case_records.csv\",\"sheet_name\":null,\"label\":\"Case Records\",\"description\":\"A table containing records of legal cases, including case ID, lawyer ID, filing date, and case duration.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"filing_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"},{\"column\":\"col_4\",\"column_name\":\"case_duration\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Duration of the case in days.\"}]},\"table_id2\":{\"name\":\"lawyer_details\",\"location\":\"C:/legal_data/lawyer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Lawyer Details\",\"description\":\"A table containing details about lawyers, including lawyer ID, name, and specialization.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"},{\"column\":\"col_3\",\"column_name\":\"specialization\",\"column_type\":\"xsd:string\",\"column_description\":\"Area of specialization for the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedules\",\"location\":\"C:/legal_data/court_schedules.xlsx\",\"sheet_name\":\"Schedules\",\"label\":\"Court Schedules\",\"description\":\"A table containing court schedules, including court ID, date, and time.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"court_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court.\"},{\"column\":\"col_2\",\"column_name\":\"schedule_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court schedule.\"},{\"column\":\"col_3\",\"column_name\":\"schedule_time\",\"column_type\":\"xsd:time\",\"column_description\":\"Time of the court schedule.\"}]}},\"instructions\":[\"Load the 'case_records' table from the CSV file located at C:/legal_data/case_records.csv.\",\"Load the 'lawyer_details' table from the Excel file located at C:/legal_data/lawyer_details.xlsx.\",\"Filter the 'case_records' table to include only cases filed after January 1, 2020.\",\"Join the filtered 'case_records' table with the 'lawyer_details' table on the 'lawyer_id' column.\",\"Aggregate the joined table to calculate the total number of cases handled by each lawyer.\",\"Sort the aggregated data by the total number of cases in descending order.\",\"Calculate a derived column 'average_case_duration' by dividing 'total_case_duration' by 'number_of_cases'.\",\"Filter the sorted data to include only lawyers with more than 10 cases.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_records.csv\",\"type\":\"csv\"},\"output\":\"case_records\",\"comment\":\"Loading the case records from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"lawyer_details\",\"comment\":\"Loading the lawyer details from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_records\",\"query\":\"`filing_date` > '2020-01-01'\"},\"output\":\"filtered_case_records\",\"comment\":\"Filtering case records to include only cases filed after January 1, 2020.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_case_records\",\"lawyer_details\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered case records with lawyer details on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_table\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"number_of_cases\",\"function\":\"count\"},{\"column\":\"case_duration\",\"name\":\"total_case_duration\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate the total number of cases and total case duration for each lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"number_of_cases\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by the total number of cases in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_data\",\"columnName\":\"average_case_duration\",\"formula\":\"`total_case_duration` / `number_of_cases`\"},\"output\":\"data_with_average_duration\",\"comment\":\"Calculating the average case duration for each lawyer.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"data_with_average_duration\",\"query\":\"`number_of_cases` > 10\"},\"output\":\"filtered_lawyers\",\"comment\":\"Filtering the data to include only lawyers with more than 10 cases.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications for each product, including demand score and cost components.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"demand_score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Score indicating the demand level for the product.\"},{\"column\":\"col_3\",\"column_name\":\"material_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of materials required for the product.\"},{\"column\":\"col_4\",\"column_name\":\"labor_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of labor required for the product.\"}]},\"table_id2\":{\"name\":\"supplier_details\",\"location\":\"C:/data/supplier_details.csv\",\"sheet_name\":null,\"label\":\"Supplier Details\",\"description\":\"Contains information about suppliers, including their IDs and contact information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"},{\"column\":\"col_3\",\"column_name\":\"contact_info\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact information for the supplier.\"}]},\"table_id3\":{\"name\":\"market_analysis\",\"location\":\"C:/data/market_analysis.xlsx:sheet=Analysis&cols=1:10,rows=1:100\",\"sheet_name\":\"Analysis\",\"label\":\"Market Analysis\",\"description\":\"Contains market analysis data, including trends and forecasts.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"trend_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for market trend.\"},{\"column\":\"col_2\",\"column_name\":\"forecast_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Forecasted value for the trend.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a high demand score.\",\"Join the filtered product specifications with the supplier details table.\",\"Calculate the estimated production cost by adding material and labor costs.\",\"Sort the products by estimated production cost in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"`demand_score` > 7.5\"},\"output\":\"high_demand_products\",\"comment\":\"Filtering products to include only those with a high demand score.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_details.csv\",\"type\":\"csv\"},\"output\":\"supplier_details\",\"comment\":\"Loading supplier details from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_demand_products\",\"supplier_details\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"products_with_suppliers\",\"comment\":\"Joining filtered product specifications with supplier details.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"products_with_suppliers\",\"columnName\":\"estimated_production_cost\",\"formula\":\"`material_cost` + `labor_cost`\"},\"output\":\"products_with_cost\",\"comment\":\"Calculating estimated production cost by adding material and labor costs.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"products_with_cost\",\"sortBy\":\"estimated_production_cost\",\"order\":\"asc\"},\"output\":\"sorted_products\",\"comment\":\"Sorting products by estimated production cost in ascending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_usage\",\"location\":\"C:/data/energy_usage.csv\",\"sheet_name\":null,\"label\":\"Energy Usage Data\",\"description\":\"This table contains daily energy consumption data for various buildings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"consumption_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Energy consumption in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.xlsx\",\"sheet_name\":\"Info\",\"label\":\"Building Information\",\"description\":\"Contains detailed information about each building, including location and type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location of the building.\"},{\"column\":\"col_3\",\"column_name\":\"type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of building, e.g., residential, commercial.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.csv\",\"sheet_name\":null,\"label\":\"Maintenance Schedule\",\"description\":\"Schedule for regular maintenance activities for each building.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"},{\"column\":\"col_3\",\"column_name\":\"activity\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the maintenance activity.\"}]}},\"instructions\":[\"Load the 'energy_usage' table containing daily energy consumption data.\",\"Filter the 'energy_usage' table to include only records where consumption is greater than 100 kWh.\",\"Aggregate the filtered data by building to calculate the total energy consumption.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_usage.csv\",\"type\":\"csv\"},\"output\":\"energy_usage\",\"comment\":\"Loading the energy usage data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_usage\",\"query\":\"consumption_kwh > 100\"},\"output\":\"filtered_energy_usage\",\"comment\":\"Filtering the energy usage data to include only records with consumption greater than 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_usage\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"consumption_kwh\",\"name\":\"total_consumption\",\"function\":\"sum\"}]},\"output\":\"total_energy_consumption\",\"comment\":\"Aggregating the filtered data by building to calculate the total energy consumption.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat exponential and logarithmic functions are supported in Tamarind expressions?","output":"Supported exponential and logarithmic functions include 'exp', 'expm1', 'log', 'log10', and 'log1p'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if a transformation fails due to invalid parameters?","output":"If a transformation fails due to invalid parameters, an error is raised."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid expression using 'is' operator.","output":"'col1 is None' // Error: 'is' operator is not supported in Tamarind expressions."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'log_transform' operation do?","output":"The 'log_transform' operation applies the natural logarithm to stabilize variance in the dataset."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information about customers, including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age_group\",\"column_type\":\"xsd:string\",\"column_description\":\"Age group of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"marketing_campaign_results\",\"location\":\"C:/data/marketing_campaigns.xlsx\",\"sheet_name\":\"Results\",\"label\":\"Marketing Campaign Results\",\"description\":\"Contains results of various marketing campaigns, including success metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"success\",\"column_type\":\"xsd:boolean\",\"column_description\":\"Indicates whether the campaign was successful.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the marketing workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the marketing campaign results table from the Excel file.\",\"Filter the marketing campaign results to include only successful campaigns.\",\"Join the filtered campaign results with customer demographics based on customer ID.\",\"Aggregate the joined data to calculate the total number of successful campaigns per age group.\",\"Sort the aggregated data by the total number of successful campaigns in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing_campaigns.xlsx\",\"type\":\"xlsx\"},\"output\":\"marketing_campaign_results\",\"comment\":\"Loading marketing campaign results from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"marketing_campaign_results\",\"query\":\"success == True\"},\"output\":\"successful_campaigns\",\"comment\":\"Filtering campaign results to include only successful campaigns.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"successful_campaigns\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining successful campaign results with customer demographics based on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"age_group\",\"aggregations\":[{\"column\":\"campaign_id\",\"name\":\"total_successful_campaigns\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate the total number of successful campaigns per age group.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_successful_campaigns\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by the total number of successful campaigns in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid arithmetic expression using basic operations.","output":"'col1 + col2 - 5'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the purpose of the 'aggregateData' action?","output":"The 'aggregateData' action aggregates a table based on grouping columns and specified aggregation functions such as sum, mean, median, and count."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid chained comparison.","output":"'10 < col1 <= 50'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_data\",\"location\":\"C:/data/customer_data.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"Contains information about customers including their IDs, names, and contact details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]},\"table_id2\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_data.xlsx\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"Contains sales transactions including sales amounts and dates.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"column_3_id\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"column_4_id\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id3\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing_campaigns.xlsx\",\"sheet_name\":\"Campaigns\",\"label\":\"Marketing Campaigns\",\"description\":\"Details of various marketing campaigns conducted over the years.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"column_2_id\",\"column_name\":\"campaign_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the marketing campaign.\"},{\"column\":\"column_3_id\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The start date of the campaign.\"}]}},\"instructions\":[\"Load the customer data from the CSV file located at C:/data/customer_data.csv.\",\"Load the sales data from the Excel file located at C:/data/sales_data.xlsx.\",\"Filter the sales data to include only transactions from the year 2023.\",\"Join the filtered sales data with the customer data on the customer_id column.\",\"Calculate the total sales for each customer by summing the sales_amount column.\",\"Sort the customers by their total sales in descending order.\",\"Identify the top 10 customers based on total sales.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_data.csv\",\"type\":\"csv\"},\"output\":\"customer_data\",\"comment\":\"Loading customer data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"sales_data\",\"comment\":\"Loading sales data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`transaction_date`.dt.year == 2023\"},\"output\":\"filtered_sales_data\",\"comment\":\"Filtering sales data to include only transactions from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales_data\",\"customer_data\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer data on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_customers\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"sales_amount\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"customer_sales_totals\",\"comment\":\"Calculating total sales for each customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"customer_sales_totals\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_customer_sales\",\"comment\":\"Sorting customers by their total sales in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_customer_sales\",\"query\":\"index < 10\"},\"output\":\"top_10_customers\",\"comment\":\"Identifying the top 10 customers based on total sales.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if 'groupBy' is empty in 'aggregateData'?","output":"If 'groupBy' is empty, the aggregation applies to the entire table."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains demographic and health information of patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_visits\",\"location\":\"C:/data/hospital_visits.xlsx\",\"sheet_name\":\"Visits\",\"label\":\"Hospital Visits\",\"description\":\"Records of hospital visits including length of stay and hospital details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"visit_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital visit.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking to the patient records.\"},{\"column\":\"col_3\",\"column_name\":\"length_of_stay\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of days the patient stayed in the hospital.\"},{\"column\":\"col_4\",\"column_name\":\"hospital_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the hospital.\"},{\"column\":\"col_5\",\"column_name\":\"visit_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the hospital visit.\"}]},\"table_id3\":{\"name\":\"hospital_info\",\"location\":\"C:/data/hospital_info.csv\",\"sheet_name\":null,\"label\":\"Hospital Information\",\"description\":\"Details about hospitals including location and capacity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"hospital_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital.\"},{\"column\":\"col_2\",\"column_name\":\"hospital_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the hospital.\"},{\"column\":\"col_3\",\"column_name\":\"capacity\",\"column_type\":\"xsd:integer\",\"column_description\":\"Capacity of the hospital in terms of number of beds.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file.\",\"Load the hospital visits table from the Excel file.\",\"Filter the patient records to include only those above 65 years old.\",\"Join the filtered patient records with hospital visits on patient ID.\",\"Calculate the total length of stay for each patient.\",\"Aggregate the data to find the total number of visits per hospital.\",\"Sort the aggregated data by the total number of visits in descending order.\",\"Apply a rolling mean transformation to the length of stay column with a window of 3.\",\"Forecast the number of hospital visits for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hospital_visits.xlsx\",\"type\":\"xlsx\"},\"output\":\"hospital_visits\",\"comment\":\"Loading hospital visits from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"age > 65\"},\"output\":\"filtered_patient_records\",\"comment\":\"Filtering patient records to include only those above 65 years old.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_patient_records\",\"hospital_visits\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_patient_visits\",\"comment\":\"Joining filtered patient records with hospital visits on patient ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_patient_visits\",\"columnName\":\"total_length_of_stay\",\"formula\":\"length_of_stay\"},\"output\":\"patient_visits_with_total_stay\",\"comment\":\"Calculating the total length of stay for each patient.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"patient_visits_with_total_stay\",\"groupBy\":\"hospital_id\",\"aggregations\":[{\"column\":\"visit_id\",\"name\":\"total_visits\",\"function\":\"count\"}]},\"output\":\"hospital_visit_counts\",\"comment\":\"Aggregating data to find the total number of visits per hospital.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"hospital_visit_counts\",\"sortBy\":\"total_visits\",\"order\":\"desc\"},\"output\":\"sorted_hospital_visits\",\"comment\":\"Sorting aggregated data by total number of visits in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"joined_patient_visits\",\"transformations\":[{\"columns\":[\"length_of_stay\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":3}}]},\"output\":\"transformed_patient_visits\",\"comment\":\"Applying a rolling mean transformation to the length of stay column with a window of 3.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sorted_hospital_visits\",\"columns\":[\"total_visits\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"dateColumn\":\"visit_date\"}},\"output\":\"hospital_visit_forecast\",\"comment\":\"Forecasting the number of hospital visits for the next 30 days using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"building_energy_data\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and energy usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.csv\",\"sheet_name\":null,\"label\":\"Building Information\",\"description\":\"This table contains information about buildings, such as their location and type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location of the building.\"},{\"column\":\"col_3\",\"column_name\":\"building_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of building, e.g., residential, commercial.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the data.\"},{\"column\":\"col_2\",\"column_name\":\"value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Some irrelevant value.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each building.\",\"Sort the buildings by total energy consumption in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"building_energy_data\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"building_energy_data\",\"query\":\"`timestamp`.year == 2023\"},\"output\":\"filtered_energy_data\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_data\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_usage_kwh\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"total_energy_by_building\",\"comment\":\"Calculating the total energy consumption for each building.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"total_energy_by_building\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_buildings\",\"comment\":\"Sorting the buildings by total energy consumption in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for the building, including timestamps and energy usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature and humidity readings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature reading in degrees Celsius.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for the building, including dates and tasks.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"},{\"column\":\"col_2\",\"column_name\":\"task\",\"column_type\":\"xsd:string\",\"column_description\":\"The maintenance task to be performed.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from the last year.\",\"Filter the weather data to include only records from the last year.\",\"Join the filtered energy consumption data with the filtered weather data on the date column.\",\"Calculate the correlation between energy consumption and temperature.\",\"Aggregate the joined data to find the average energy consumption per month.\",\"Sort the aggregated data by average energy consumption in descending order.\",\"Forecast the next 12 months of energy consumption using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp` >= '2022-01-01'\"},\"output\":\"filtered_energy\",\"comment\":\"Filtering energy consumption data to include only records from the last year.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"weather_data\",\"query\":\"`date` >= '2022-01-01'\"},\"output\":\"filtered_weather\",\"comment\":\"Filtering weather data to include only records from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy\",\"filtered_weather\"],\"joinOn\":[\"timestamp\",\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered energy consumption data with filtered weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"correlation\",\"formula\":\"joined_data[['energy_usage', 'temperature']].corr().iloc[0,1]\"},\"output\":\"correlation_data\",\"comment\":\"Calculating the correlation between energy consumption and temperature.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"month\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"average_energy_usage\",\"function\":\"mean\"}]},\"output\":\"monthly_average_energy\",\"comment\":\"Aggregating the joined data to find the average energy consumption per month.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_average_energy\",\"sortBy\":\"average_energy_usage\",\"order\":\"desc\"},\"output\":\"sorted_monthly_energy\",\"comment\":\"Sorting the aggregated data by average energy consumption in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"joined_data\",\"columns\":[\"energy_usage\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"dateColumn\":\"timestamp\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the next 12 months of energy consumption using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains the energy consumption data for the building, including total energy used and number of occupants.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"total_energy\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total energy consumed in kilowatt-hours.\"},{\"column\":\"col_3\",\"column_name\":\"occupants\",\"column_type\":\"xsd:integer\",\"column_description\":\"The number of occupants in the building.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Weather\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity on the given date.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.csv\",\"sheet_name\":null,\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for the building, including dates and tasks.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the scheduled maintenance.\"},{\"column\":\"col_2\",\"column_name\":\"task\",\"column_type\":\"xsd:string\",\"column_description\":\"The maintenance task to be performed.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from the last year.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing total energy consumption by the number of occupants.\",\"Aggregate the joined data by month to calculate the average energy efficiency.\",\"Sort the aggregated data by energy efficiency in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date` >= '2022-01-01'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered energy consumption data with the weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`total_energy` / `occupants`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating a new column for energy efficiency by dividing total energy consumption by the number of occupants.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"energy_efficiency\",\"name\":\"avg_energy_efficiency\",\"function\":\"mean\"}]},\"output\":\"monthly_efficiency\",\"comment\":\"Aggregating the joined data by month to calculate the average energy efficiency.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_efficiency\",\"sortBy\":\"avg_energy_efficiency\",\"order\":\"desc\"},\"output\":\"sorted_efficiency\",\"comment\":\"Sorting the aggregated data by energy efficiency in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"This table contains details about shipments including shipment ID, origin, destination, and weight.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"column_2_id\",\"column_name\":\"origin\",\"column_type\":\"xsd:string\",\"column_description\":\"The origin location of the shipment.\"},{\"column\":\"column_3_id\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"The destination location of the shipment.\"},{\"column\":\"column_4_id\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The weight of the shipment in kilograms.\"}]},\"table_id2\":{\"name\":\"vehicle_data\",\"location\":\"C:/data/logistics/vehicle_data.csv\",\"sheet_name\":null,\"label\":\"Vehicle Data\",\"description\":\"This table contains details about vehicles used for shipments including vehicle ID, type, and capacity.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"vehicle_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each vehicle.\"},{\"column\":\"column_2_id\",\"column_name\":\"vehicle_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of vehicle.\"},{\"column\":\"column_3_id\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The capacity of the vehicle in kilograms.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details about shipments including shipment ID, origin, destination, and weight.\",\"Filter the 'shipment_data' table to include only shipments with a weight greater than 1000 kg.\",\"Sort the filtered shipments by destination in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"heavy_shipments\",\"comment\":\"Filtering shipments to include only those with a weight greater than 1000 kg.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"heavy_shipments\",\"sortBy\":\"destination\",\"order\":\"asc\"},\"output\":\"sorted_shipments\",\"comment\":\"Sorting the filtered shipments by destination in ascending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid comparison expression using less than or equal to.","output":"'col1 <= col2'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the purpose of the 'filterData' action?","output":"The 'filterData' action filters a table based on a query condition, keeping only the rows that match the condition."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of products including their development status.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"development_status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the product development.\"}]},\"table_id2\":{\"name\":\"design_team_assignments\",\"location\":\"C:/data/design_team_assignments.csv\",\"sheet_name\":null,\"label\":\"Design Team Assignments\",\"description\":\"Lists team members assigned to each product by product ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"team_member_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each team member.\"}]},\"table_id3\":{\"name\":\"marketing_data\",\"location\":\"C:/data/marketing_data.csv\",\"sheet_name\":null,\"label\":\"Marketing Data\",\"description\":\"Contains marketing information for various products.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"marketing_budget\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Budget allocated for marketing each product.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only items with a development status of 'in progress'.\",\"Join the filtered product specifications with the design team assignments table on the product ID.\",\"Aggregate the joined table to calculate the total number of team members assigned to each product.\",\"Sort the aggregated data by the total number of team members in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"development_status == 'in progress'\"},\"output\":\"in_progress_products\",\"comment\":\"Filtering the product specifications to include only items with a development status of 'in progress'.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/design_team_assignments.csv\",\"type\":\"csv\"},\"output\":\"design_team_assignments\",\"comment\":\"Loading the design team assignments table from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"in_progress_products\",\"design_team_assignments\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_product_team\",\"comment\":\"Joining the filtered product specifications with the design team assignments table on the product ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_product_team\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"team_member_id\",\"name\":\"total_team_members\",\"function\":\"count\"}]},\"output\":\"aggregated_team_data\",\"comment\":\"Aggregating the joined table to calculate the total number of team members assigned to each product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_team_data\",\"sortBy\":\"total_team_members\",\"order\":\"desc\"},\"output\":\"sorted_team_data\",\"comment\":\"Sorting the aggregated data by the total number of team members in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat operations are supported in 'applyTransformations'?","output":"Supported operations include: 'interpolate', 'fillna', 'dropna', 'log_transform', 'exp_transform', 'sin_transform', 'cos_transform', 'tan_transform', 'min_max_scale', 'standardize', 'normalize', 'power_transform', 'rolling_quantile', 'rolling_skew', and 'rolling_kurtosis'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if the table specified in 'filterData' does not exist?","output":"If the table does not exist, an error is raised."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_details\",\"location\":\"C:/legal_data/case_details.csv\",\"sheet_name\":null,\"label\":\"Case Details\",\"description\":\"Contains information about various legal cases including case ID, status, and lawyer ID.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"CASE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"column_2_id\",\"column_name\":\"STATUS\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the case, e.g., Open, Closed.\"},{\"column\":\"column_3_id\",\"column_name\":\"LAWYER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"}]},\"table_id2\":{\"name\":\"lawyer_info\",\"location\":\"C:/legal_data/lawyer_info.csv\",\"sheet_name\":null,\"label\":\"Lawyer Information\",\"description\":\"Contains details about lawyers including lawyer ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"LAWYER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"LAWYER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedule\",\"location\":\"C:/legal_data/court_schedule.csv\",\"sheet_name\":null,\"label\":\"Court Schedule\",\"description\":\"Contains information about court schedules, not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"SCHEDULE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each schedule.\"},{\"column\":\"column_2_id\",\"column_name\":\"COURT_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court session.\"}]}},\"instructions\":[\"Load the table 'case_details' which contains information about various legal cases.\",\"Load the table 'lawyer_info' which contains details about lawyers involved in the cases.\",\"Filter the 'case_details' table to include only cases that are marked as 'Open'.\",\"Join the filtered 'case_details' table with the 'lawyer_info' table on the 'lawyer_id' column.\",\"Aggregate the joined table to find the total number of open cases handled by each lawyer.\",\"Sort the aggregated data by the total number of open cases in descending order.\",\"Calculate a derived column 'case_ratio' in the sorted table as the ratio of open cases to total cases handled by each lawyer.\",\"Filter the sorted table to include only lawyers with a 'case_ratio' greater than 0.5.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_details.csv\",\"type\":\"csv\"},\"output\":\"case_details\",\"comment\":\"Loading the case details dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyer_info.csv\",\"type\":\"csv\"},\"output\":\"lawyer_info\",\"comment\":\"Loading the lawyer information dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_details\",\"query\":\"STATUS == 'Open'\"},\"output\":\"open_cases\",\"comment\":\"Filtering the case details to include only open cases.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"open_cases\",\"lawyer_info\"],\"joinOn\":[\"LAWYER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_cases_lawyers\",\"comment\":\"Joining open cases with lawyer information on lawyer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_cases_lawyers\",\"groupBy\":\"LAWYER_NAME\",\"aggregations\":[{\"column\":\"CASE_ID\",\"name\":\"total_open_cases\",\"function\":\"count\"}]},\"output\":\"aggregated_cases\",\"comment\":\"Aggregating data to find the total number of open cases handled by each lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_cases\",\"sortBy\":\"total_open_cases\",\"order\":\"desc\"},\"output\":\"sorted_cases\",\"comment\":\"Sorting the aggregated data by the total number of open cases in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_cases\",\"columnName\":\"case_ratio\",\"formula\":\"total_open_cases / total_cases\"},\"output\":\"cases_with_ratio\",\"comment\":\"Calculating the case ratio as the ratio of open cases to total cases handled by each lawyer.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"cases_with_ratio\",\"query\":\"case_ratio > 0.5\"},\"output\":\"filtered_lawyers\",\"comment\":\"Filtering to include only lawyers with a case ratio greater than 0.5.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/marketing/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information about customers, including age, gender, and location.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"sales_transactions\",\"location\":\"C:/data/marketing/sales_transactions.xlsx\",\"sheet_name\":\"Transactions\",\"label\":\"Sales Transactions\",\"description\":\"Contains detailed sales transactions including customer ID, transaction date, and amount.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the transaction.\"},{\"column\":\"column_4_id\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount spent in the transaction.\"}]},\"table_id3\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing/marketing_campaigns.xlsx\",\"sheet_name\":\"Campaigns\",\"label\":\"Marketing Campaigns\",\"description\":\"Contains information about past marketing campaigns, including campaign ID and results.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each campaign.\"},{\"column\":\"column_2_id\",\"column_name\":\"campaign_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the marketing campaign.\"},{\"column\":\"column_3_id\",\"column_name\":\"results\",\"column_type\":\"xsd:string\",\"column_description\":\"Results of the marketing campaign.\"}]}},\"instructions\":[\"Load the customer demographics data from the CSV file.\",\"Load the sales transactions data from the Excel file.\",\"Filter the sales data to include only transactions from the last year.\",\"Join the filtered sales data with customer demographics data on customer ID.\",\"Calculate the total spending for each customer.\",\"Sort the customers by their total spending in descending order.\",\"Select the top 100 customers based on total spending.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/sales_transactions.xlsx\",\"type\":\"xlsx\"},\"output\":\"sales_transactions\",\"comment\":\"Loading sales transactions data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_transactions\",\"query\":\"`transaction_date` >= '2022-01-01'\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_demographics\",\"comment\":\"Joining filtered sales data with customer demographics data on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_demographics\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_spending\",\"function\":\"sum\"}]},\"output\":\"customer_spending\",\"comment\":\"Calculating the total spending for each customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"customer_spending\",\"sortBy\":\"total_spending\",\"order\":\"desc\"},\"output\":\"sorted_customers\",\"comment\":\"Sorting the customers by their total spending in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_customers\",\"query\":\"index < 100\"},\"output\":\"top_100_customers\",\"comment\":\"Selecting the top 100 customers based on total spending.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"This table contains details of all shipments including weight, delivery date, and destination.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"delivery_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Scheduled delivery date for the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_data\",\"location\":\"C:/data/logistics/warehouse_data.csv\",\"sheet_name\":null,\"label\":\"Warehouse Data\",\"description\":\"Information about warehouse locations and capacities.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse.\"},{\"column\":\"col_3\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Capacity of the warehouse in cubic meters.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of all shipments.\",\"Filter the shipment data to include only shipments with a weight greater than 1000 kg.\",\"Sort the filtered shipment data by delivery date in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"heavy_shipments\",\"comment\":\"Filtering shipments to include only those with a weight greater than 1000 kg.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"heavy_shipments\",\"sortBy\":\"delivery_date\",\"order\":\"asc\"},\"output\":\"sorted_heavy_shipments\",\"comment\":\"Sorting the filtered shipments by delivery date in ascending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/legal_data/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases Data\",\"description\":\"A table containing details of various court cases, including the judge's name, case verdict, and other relevant information.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court case.\"},{\"column\":\"column_2_id\",\"column_name\":\"judge_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the judge presiding over the case.\"},{\"column\":\"column_3_id\",\"column_name\":\"verdict\",\"column_type\":\"xsd:string\",\"column_description\":\"The verdict of the case, e.g., 'guilty' or 'not guilty'.\"}]},\"table_id2\":{\"name\":\"law_firms\",\"location\":\"C:/legal_data/law_firms.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Law Firms Data\",\"description\":\"Information about various law firms, including their names, locations, and specialties.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"firm_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each law firm.\"},{\"column\":\"column_2_id\",\"column_name\":\"firm_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the law firm.\"}]}},\"instructions\":[\"Load the 'court_cases' table from the CSV file located at 'C:/legal_data/court_cases.csv'.\",\"Filter the 'court_cases' table to include only cases where the verdict is 'guilty'.\",\"Aggregate the filtered data by 'judge_name' to count the number of guilty verdicts each judge has given.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"verdict == 'guilty'\"},\"output\":\"guilty_cases\",\"comment\":\"Filtering court cases to include only those with a guilty verdict.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"guilty_cases\",\"groupBy\":\"judge_name\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"guilty_verdict_count\",\"function\":\"count\"}]},\"output\":\"judge_guilty_counts\",\"comment\":\"Aggregating data to count the number of guilty verdicts given by each judge.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/data/legal/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases\",\"description\":\"A table containing details of court cases including verdicts, judges, and case details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court case.\"},{\"column\":\"column_2_id\",\"column_name\":\"verdict\",\"column_type\":\"xsd:string\",\"column_description\":\"The verdict of the court case, e.g., 'guilty', 'not guilty'.\"},{\"column\":\"column_3_id\",\"column_name\":\"judge\",\"column_type\":\"xsd:string\",\"column_description\":\"The judge presiding over the court case.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/data/legal/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers\",\"description\":\"A table containing information about lawyers including their names and cases handled.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"},{\"column\":\"column_3_id\",\"column_name\":\"cases_handled\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of cases handled by the lawyer.\"}]}},\"instructions\":[\"Load the 'court_cases' table from the CSV file located at C:/data/legal/court_cases.csv.\",\"Filter the 'court_cases' table to include only cases where the verdict is 'guilty'.\",\"Aggregate the filtered data by 'judge' to count the number of guilty verdicts per judge.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"verdict == 'guilty'\"},\"output\":\"guilty_cases\",\"comment\":\"Filtering court cases to include only those with a guilty verdict.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"guilty_cases\",\"groupBy\":\"judge\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"guilty_verdicts_count\",\"function\":\"count\"}]},\"output\":\"guilty_verdicts_per_judge\",\"comment\":\"Aggregating data to count the number of guilty verdicts per judge.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications for each product, including product ID, name, and price.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]},\"table_id2\":{\"name\":\"market_analysis\",\"location\":\"C:/data/market_analysis.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Market Analysis\",\"description\":\"Market analysis data including estimated sales and market trends for various products.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"estimated_sales\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Estimated sales figures for the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"year\",\"column_type\":\"xsd:integer\",\"column_description\":\"Year of the market analysis data.\"}]},\"table_id3\":{\"name\":\"historical_sales\",\"location\":\"C:/data/historical_sales.csv\",\"sheet_name\":null,\"label\":\"Historical Sales\",\"description\":\"Contains historical sales data for various products, not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"sales\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Historical sales figures for the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"year\",\"column_type\":\"xsd:integer\",\"column_description\":\"Year of the sales data.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market analysis data from the Excel sheet.\",\"Filter the market analysis data to include only entries from the last year.\",\"Join the product specifications with the filtered market analysis data on the product ID.\",\"Calculate a new column for potential revenue by multiplying the estimated sales by the product price.\",\"Aggregate the joined data by product category to find the total potential revenue.\",\"Sort the aggregated data by total potential revenue in descending order.\",\"Filter the sorted data to include only the top 5 product categories.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_analysis.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_analysis\",\"comment\":\"Loading market analysis data from an Excel sheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_analysis\",\"query\":\"year == 2023\"},\"output\":\"filtered_market_analysis\",\"comment\":\"Filtering market analysis data to include only entries from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_analysis\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market analysis data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_revenue\",\"formula\":\"estimated_sales * price\"},\"output\":\"data_with_revenue\",\"comment\":\"Calculating potential revenue by multiplying estimated sales by product price.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_revenue\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"potential_revenue\",\"name\":\"total_potential_revenue\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by product ID to find total potential revenue.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_potential_revenue\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total potential revenue in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_data\",\"query\":\"index < 5\"},\"output\":\"top_categories\",\"comment\":\"Filtering sorted data to include only the top 5 product categories.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"This table contains sales records including sales amount, customer ID, and other relevant details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"total_sales\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total sales amount for each transaction.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for each customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the sales transaction.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Data\",\"description\":\"This table contains customer information including customer ID and region.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region where the customer is located.\"}]},\"table_id3\":{\"name\":\"product_data\",\"location\":\"C:/data/products.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"This table contains product details including product ID and category.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"The category of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Filter the sales data to include only records where the total sales amount is greater than 1000.\",\"Load the customer data from the Excel file located at C:/data/customers.xlsx.\",\"Join the filtered sales data with the customer data on the customer_id column using an inner join.\",\"Aggregate the joined data by region to calculate the total sales amount for each region.\",\"Sort the aggregated data by total sales amount in descending order.\",\"Calculate a new column in the sorted data for profit margin by subtracting total cost from total sales.\",\"Apply a log transformation to the profit margin column to stabilize variance.\",\"Perform a time-series forecast on the total sales column using the Holt-Winters algorithm with a forecast period of 12 months.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_data\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"total_sales > 1000\"},\"output\":\"filtered_sales_data\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_data\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales_data\",\"customer_data\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_sales\",\"name\":\"total_sales_by_region\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_sales_by_region\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"columnName\":\"profit_margin\",\"formula\":\"total_sales - total_cost\"},\"output\":\"data_with_profit_margin\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"data_with_profit_margin\",\"transformations\":[{\"columns\":[\"profit_margin\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_data\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_data\",\"columns\":[\"total_sales\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"forecast_results\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/HR_data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"Contains detailed information about employees, including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_details\",\"location\":\"C:/HR_data/department_details.xlsx\",\"sheet_name\":\"Departments\",\"label\":\"Department Details\",\"description\":\"Provides information about each department, including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the department.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/HR_data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"random_id\",\"column_type\":\"xsd:string\",\"column_description\":\"A random identifier.\"},{\"column\":\"col_2\",\"column_name\":\"random_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"A random value.\"}]}},\"instructions\":[\"Load the employee records table from the HR database.\",\"Filter the employee records to include only those with a salary greater than $50,000.\",\"Join the filtered employee records with the department details table on the department ID.\",\"Aggregate the joined data to calculate the average salary per department.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_data/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employee records to include only those with a salary greater than $50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_data/department_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_details\",\"comment\":\"Loading the department details dataset from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_salary_employees\",\"department_details\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee records with department details on department ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"average_salary_per_department\",\"comment\":\"Aggregating the joined data to calculate the average salary per department.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_usage\",\"location\":\"C:/data/building/energy_usage.csv\",\"sheet_name\":null,\"label\":\"Building Energy Usage Data\",\"description\":\"This table contains daily energy consumption data for the building, including date and energy consumption in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"}]},\"table_id2\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/building/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Building Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for the building, including dates and tasks.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the scheduled maintenance.\"},{\"column\":\"col_B\",\"column_name\":\"task\",\"column_type\":\"xsd:string\",\"column_description\":\"The maintenance task to be performed.\"}]}},\"instructions\":[\"Load the table 'energy_usage' which contains daily energy consumption data for the building.\",\"Filter the data to include only records where the energy consumption is greater than 100 kWh.\",\"Aggregate the filtered data to calculate the total energy consumption per month.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building/energy_usage.csv\",\"type\":\"csv\"},\"output\":\"energy_usage\",\"comment\":\"Loading the energy usage data for the building from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_usage\",\"query\":\"`energy_consumption` > 100\"},\"output\":\"high_energy_usage\",\"comment\":\"Filtering the energy usage data to include only records where consumption is greater than 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"high_energy_usage\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"energy_consumption\",\"name\":\"monthly_total_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_usage\",\"comment\":\"Aggregating the filtered data to calculate the total energy consumption per month.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information of customers including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"sales_transactions\",\"location\":\"C:/data/sales_transactions.xlsx\",\"sheet_name\":\"Transactions\",\"label\":\"Sales Transactions\",\"description\":\"Contains records of sales transactions including date, amount, and customer ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the transaction occurred.\"},{\"column\":\"col_4\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total amount of the transaction.\"}]},\"table_id3\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing_campaigns.csv\",\"sheet_name\":null,\"label\":\"Marketing Campaigns\",\"description\":\"Contains details of past marketing campaigns including campaign ID, start date, and end date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_2\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Start date of the campaign.\"},{\"column\":\"col_3\",\"column_name\":\"end_date\",\"column_type\":\"xsd:date\",\"column_description\":\"End date of the campaign.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the sales transactions table from the Excel file.\",\"Filter the sales transactions to include only those from the last quarter.\",\"Join the filtered sales transactions with customer demographics on customer ID.\",\"Aggregate the joined table to calculate total sales per customer.\",\"Sort the aggregated data by total sales in descending order.\",\"Identify the top 10% of customers based on total sales.\",\"Calculate the average age of the top 10% customers.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_transactions.xlsx\",\"type\":\"xlsx\"},\"output\":\"sales_transactions\",\"comment\":\"Loading sales transactions data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_transactions\",\"query\":\"`transaction_date` >= '2023-07-01' and `transaction_date` <= '2023-09-30'\"},\"output\":\"filtered_sales_transactions\",\"comment\":\"Filtering sales transactions to include only those from the last quarter.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales_transactions\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered sales transactions with customer demographics on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"aggregated_sales\",\"comment\":\"Aggregating the joined table to calculate total sales per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_sales\",\"comment\":\"Sorting the aggregated data by total sales in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_sales\",\"query\":\"index < int(len(sorted_sales) * 0.1)\"},\"output\":\"top_customers\",\"comment\":\"Identifying the top 10% of customers based on total sales.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"top_customers\",\"groupBy\":null,\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"average_age_top_customers\",\"comment\":\"Calculating the average age of the top 10% customers.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/healthcare_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains detailed records of patients including demographics and diagnoses.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"column_2_id\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"column_3_id\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_staff\",\"location\":\"C:/healthcare_data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Contains information about hospital staff including roles and departments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"column_3_id\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the staff member works.\"}]}},\"instructions\":[\"Load the patient records table from the healthcare database.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Aggregate the filtered records to calculate the average age of diabetic patients.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetic_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with diabetes.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"diabetic_patients\",\"groupBy\":null,\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"average_age_diabetic_patients\",\"comment\":\"Calculating the average age of diabetic patients.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'cos_transform' operation do?","output":"The 'cos_transform' operation applies a cosine transformation to cyclical data."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains the manufacturing schedule with details on products, quantities, and scheduled dates.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the product scheduled for production.\"},{\"column\":\"column_3_id\",\"column_name\":\"scheduled_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the product is scheduled for production.\"}]},\"table_id2\":{\"name\":\"inventory\",\"location\":\"C:/data/manufacturing/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory\",\"description\":\"This table contains current stock levels for each product.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the product.\"}]},\"table_id3\":{\"name\":\"supplier_info\",\"location\":\"C:/data/manufacturing/supplier_info.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"This table contains information about suppliers for each product.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_3_id\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the manufacturing schedule.\",\"Filter the production schedule to include only items scheduled for the current week.\",\"Join the filtered production schedule with the 'inventory' table to check available stock.\",\"Aggregate the joined data to calculate the total quantity required for each product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"`scheduled_date` >= '2023-10-23' and `scheduled_date` <= '2023-10-29'\"},\"output\":\"current_week_schedule\",\"comment\":\"Filtering the production schedule to include only items scheduled for the current week.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/inventory.csv\",\"type\":\"csv\"},\"output\":\"inventory\",\"comment\":\"Loading the inventory data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"current_week_schedule\",\"inventory\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"schedule_with_inventory\",\"comment\":\"Joining the filtered production schedule with the inventory to check available stock.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"schedule_with_inventory\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"quantity\",\"name\":\"total_quantity_required\",\"function\":\"sum\"}]},\"output\":\"total_quantity_per_product\",\"comment\":\"Aggregating the joined data to calculate the total quantity required for each product.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information of customers including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"marketing_campaign_results\",\"location\":\"C:/data/marketing_campaigns.xlsx\",\"sheet_name\":\"Results\",\"label\":\"Marketing Campaign Results\",\"description\":\"Contains results of various marketing campaigns including success metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"success\",\"column_type\":\"xsd:boolean\",\"column_description\":\"Indicates whether the campaign was successful.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for irrelevant data.\"},{\"column\":\"col_2\",\"column_name\":\"value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Some irrelevant value.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the marketing campaign results table from the Excel file.\",\"Filter the marketing campaign results to include only successful campaigns.\",\"Join the customer demographics with the filtered marketing campaign results on customer ID.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading the customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing_campaigns.xlsx\",\"type\":\"xlsx\"},\"output\":\"marketing_campaign_results\",\"comment\":\"Loading the marketing campaign results from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"marketing_campaign_results\",\"query\":\"success == True\"},\"output\":\"successful_campaigns\",\"comment\":\"Filtering the marketing campaign results to include only successful campaigns.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"customer_demographics\",\"successful_campaigns\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"demographics_with_successful_campaigns\",\"comment\":\"Joining customer demographics with successful marketing campaign results on customer ID.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid comparison expression using basic operators.","output":"'col1 > col2'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"This table contains customer feedback including ratings and comments for various products.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_3\",\"column_name\":\"rating\",\"column_type\":\"xsd:integer\",\"column_description\":\"Customer rating for the product, ranging from 1 to 5.\"},{\"column\":\"col_4\",\"column_name\":\"feedback\",\"column_type\":\"xsd:string\",\"column_description\":\"Textual feedback provided by the customer.\"}]},\"table_id2\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.xlsx\",\"sheet_name\":\"Products\",\"label\":\"Product Catalog\",\"description\":\"A comprehensive list of products available in the store.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file located at C:/data/customer_feedback.csv.\",\"Filter the feedback data to include only entries with a rating of 4 or 5.\",\"Aggregate the filtered feedback data to count the number of positive feedbacks per product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback data to include only positive ratings (4 or 5).\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"positive_feedback\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"positive_feedback_count\",\"function\":\"count\"}]},\"output\":\"positive_feedback_summary\",\"comment\":\"Aggregating data to count the number of positive feedbacks per product.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat parameters are required for the 'filterData' action?","output":"'filterData' requires a 'table' parameter (name of the table), a 'query' parameter (expression to filter rows), and an optional 'caseInsensitiveColumns' parameter (list of columns where filters should be case insensitive)."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/healthcare_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains detailed records of patients including demographics and diagnoses.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"medication_records\",\"location\":\"C:/healthcare_data/medication_records.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Contains records of medications prescribed to patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"},{\"column\":\"col_3\",\"column_name\":\"dosage\",\"column_type\":\"xsd:string\",\"column_description\":\"Dosage of the medication prescribed.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/healthcare_data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Contains information about hospital staff members.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"role\",\"column_type\":\"xsd:string\",\"column_description\":\"Role of the staff member in the hospital.\"}]}},\"instructions\":[\"Load the patient records table from the healthcare database.\",\"Filter the patient records to include only those with a diagnosis of hypertension.\",\"Load the medication records table from the healthcare database.\",\"Join the filtered patient records with medication records on patient ID.\",\"Calculate the average age of patients with hypertension.\",\"Aggregate the data to find the total number of medications prescribed for each patient.\",\"Sort the aggregated data by the total number of medications in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from the healthcare database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'hypertension'\"},\"output\":\"hypertension_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with hypertension.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/medication_records.csv\",\"type\":\"csv\"},\"output\":\"medication_records\",\"comment\":\"Loading medication records from the healthcare database.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"hypertension_patients\",\"medication_records\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"hypertension_medications\",\"comment\":\"Joining filtered patient records with medication records on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"hypertension_medications\",\"groupBy\":null,\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"hypertension_medications_with_age\",\"comment\":\"Calculating the average age of patients with hypertension.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"hypertension_medications_with_age\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"medication_name\",\"name\":\"total_medications\",\"function\":\"count\"}]},\"output\":\"medication_counts\",\"comment\":\"Aggregating data to find the total number of medications prescribed for each patient.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"medication_counts\",\"sortBy\":\"total_medications\",\"order\":\"desc\"},\"output\":\"sorted_medication_counts\",\"comment\":\"Sorting the aggregated data by the total number of medications in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid generator expression.","output":"'(x for x in col1)' // Error: Generator expressions are not allowed in Tamarind expressions."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"This table contains details of all shipments including shipment ID, status, weight, and warehouse ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_4\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse associated with the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_data\",\"location\":\"C:/data/logistics/warehouse_data.csv\",\"sheet_name\":null,\"label\":\"Warehouse Data\",\"description\":\"This table contains information about warehouse locations including warehouse ID and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/logistics/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"This table contains inventory details for each warehouse, not relevant for the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"col_2\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse storing the item.\"},{\"column\":\"col_3\",\"column_name\":\"quantity\",\"column_type\":\"xsd:integer\",\"column_description\":\"Quantity of the item in stock.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of all shipments.\",\"Load the table 'warehouse_data' which contains information about warehouse locations.\",\"Filter the 'shipment_data' to include only shipments with a status of 'delivered'.\",\"Join the filtered shipment data with 'warehouse_data' on the 'warehouse_id' column.\",\"Calculate the total weight of shipments for each warehouse.\",\"Sort the resulting data by total weight in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouse_data.csv\",\"type\":\"csv\"},\"output\":\"warehouse_data\",\"comment\":\"Loading the warehouse data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"status == 'delivered'\"},\"output\":\"delivered_shipments\",\"comment\":\"Filtering shipment data to include only delivered shipments.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"delivered_shipments\",\"warehouse_data\"],\"joinOn\":[\"warehouse_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining delivered shipments with warehouse data on warehouse_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"warehouse_id\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating the total weight of shipments for each warehouse.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_weight\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the data by total weight in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating on a scale of 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Customer feedback comments.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]},\"table_id3\":{\"name\":\"promotions\",\"location\":\"C:/data/promotions.xlsx\",\"sheet_name\":\"Promotions\",\"label\":\"Promotions\",\"description\":\"Table containing current promotions available for customers.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"promotion_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each promotion.\"},{\"column\":\"column_2_id\",\"column_name\":\"promotion_details\",\"column_type\":\"xsd:string\",\"column_description\":\"Details of the promotion.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file located at C:/data/customer_feedback.csv.\",\"Load the customer details table from the Excel file located at C:/data/customer_details.xlsx.\",\"Filter the customer feedback table to include only feedback with a rating of 4 or higher.\",\"Join the filtered feedback table with the customer details table on the customer_id column.\",\"Aggregate the joined table to calculate the average rating for each customer.\",\"Sort the aggregated data by average rating in descending order.\",\"Calculate a new column in the sorted table to determine if the average rating is above 4.5.\",\"Filter the sorted table to include only customers with an average rating above 4.5.\",\"Join the high-rated customers table with a promotions table to identify eligible customers for promotions.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"filtered_feedback\",\"comment\":\"Filtering feedback to include only ratings of 4 or higher.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered feedback with customer details on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating average rating for each customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting data by average rating in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_data\",\"columnName\":\"high_rating\",\"formula\":\"average_rating > 4.5\"},\"output\":\"sorted_data_with_high_rating\",\"comment\":\"Determining if the average rating is above 4.5.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_data_with_high_rating\",\"query\":\"high_rating == True\"},\"output\":\"high_rated_customers\",\"comment\":\"Filtering to include only customers with an average rating above 4.5.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_rated_customers\",\"promotions\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"eligible_customers_for_promotions\",\"comment\":\"Joining high-rated customers with promotions to identify eligible customers.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the 'local_outlier_factor' algorithm used for?","output":"Use 'local_outlier_factor' to detect outliers based on local density estimation, requiring 'n_neighbors' as a parameter."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"This table contains records of financial transactions including transaction ID, amount, currency code, and transaction type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code of the transaction amount.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of transaction, e.g., sale, refund.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/exchange_rates.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Exchange Rates\",\"description\":\"This table provides exchange rates for various currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"An identifier for the data.\"},{\"column\":\"col_2\",\"column_name\":\"value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"A value associated with the data.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at C:/data/financial_transactions.csv.\",\"Load the 'exchange_rates' table from the Excel file located at C:/data/exchange_rates.xlsx.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Join the filtered 'financial_transactions' table with the 'exchange_rates' table on the 'currency_code' column.\",\"Calculate a new column 'amount_in_usd' by multiplying 'amount' with 'exchange_rate' in the joined table.\",\"Aggregate the joined table by 'transaction_type' to calculate the total 'amount_in_usd' for each type.\",\"Sort the aggregated data by 'total_amount_in_usd' in descending order.\",\"Apply a log transformation to the 'total_amount_in_usd' column to stabilize variance.\",\"Forecast the 'total_amount_in_usd' for the next 12 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered transactions with exchange rates on currency code.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_table\",\"columnName\":\"amount_in_usd\",\"formula\":\"amount * exchange_rate\"},\"output\":\"transactions_with_usd\",\"comment\":\"Calculating amount in USD by multiplying amount with exchange rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_usd\",\"groupBy\":\"transaction_type\",\"aggregations\":[{\"column\":\"amount_in_usd\",\"name\":\"total_amount_in_usd\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by transaction type to calculate total amount in USD.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_amount_in_usd\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total amount in USD in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_data\",\"transformations\":[{\"columns\":[\"total_amount_in_usd\"],\"operation\":\"log_transform\"}]},\"output\":\"log_transformed_data\",\"comment\":\"Applying log transformation to stabilize variance in total amount in USD.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"log_transformed_data\",\"columns\":[\"total_amount_in_usd\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"transaction_type\"}},\"output\":\"forecast_results\",\"comment\":\"Forecasting total amount in USD for the next 12 months using Holt-Winters method.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/forecasted_financials.csv\",\"type\":\"csv\"},\"output\":\"forecasted_financials\",\"comment\":\"Saving the forecast results to a new table called forecasted_financials.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'sin_transform' operation do?","output":"The 'sin_transform' operation applies a sine transformation to cyclical data."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comments from the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"column_3_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.csv\",\"sheet_name\":null,\"label\":\"Product Catalog\",\"description\":\"Table containing product information, not relevant for this workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Load the customer details table from the Excel file.\",\"Filter the feedback table to include only feedback with a rating below 3.\",\"Join the filtered feedback with customer details on customer ID.\",\"Aggregate the joined table to count the number of complaints per region.\",\"Sort the aggregated data by the number of complaints in descending order.\",\"Calculate a new column in the sorted table to show the percentage of total complaints per region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating < 3\"},\"output\":\"low_rating_feedback\",\"comment\":\"Filtering feedback to include only ratings below 3.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_rating_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_feedback_details\",\"comment\":\"Joining filtered feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_feedback_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"customer_id\",\"name\":\"complaint_count\",\"function\":\"count\"}]},\"output\":\"complaints_per_region\",\"comment\":\"Aggregating data to count complaints per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"complaints_per_region\",\"sortBy\":\"complaint_count\",\"order\":\"desc\"},\"output\":\"sorted_complaints\",\"comment\":\"Sorting the data by the number of complaints in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_complaints\",\"columnName\":\"complaint_percentage\",\"formula\":\"complaint_count / sum(complaint_count) * 100\"},\"output\":\"complaints_with_percentage\",\"comment\":\"Calculating the percentage of total complaints per region.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"This table contains details of all shipments including shipment ID, customer ID, status, and weight.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer associated with the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment (e.g., delivered, in transit).\"},{\"column\":\"col_4\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/logistics/customer_data.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"This table contains customer information including customer ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/logistics/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"This table contains inventory details which are not relevant for the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"col_2\",\"column_name\":\"quantity\",\"column_type\":\"xsd:integer\",\"column_description\":\"Quantity of the inventory item available.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of all shipments.\",\"Filter the shipments to include only those with a status of 'delivered'.\",\"Join the filtered shipment data with 'customer_data' on the 'customer_id' column.\",\"Aggregate the joined data to calculate the total weight of shipments per customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"status == 'delivered'\"},\"output\":\"delivered_shipments\",\"comment\":\"Filtering shipments to include only those with a status of 'delivered'.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"delivered_shipments\",\"customer_data\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"shipments_with_customers\",\"comment\":\"Joining filtered shipment data with customer data on 'customer_id'.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"shipments_with_customers\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"total_weight_per_customer\",\"comment\":\"Aggregating data to calculate the total weight of shipments per customer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/legal_data/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases\",\"description\":\"A table containing details of various court cases, including case ID, verdict, and associated lawyer ID.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court case.\"},{\"column\":\"column_2_id\",\"column_name\":\"verdict\",\"column_type\":\"xsd:string\",\"column_description\":\"The verdict of the court case, such as 'guilty' or 'not guilty'.\"},{\"column\":\"column_3_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer associated with the case.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/legal_data/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers\",\"description\":\"A table containing information about lawyers, including their ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_sessions\",\"location\":\"C:/legal_data/court_sessions.csv\",\"sheet_name\":null,\"label\":\"Court Sessions\",\"description\":\"A table containing details of court sessions, including session ID and date.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"session_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court session.\"},{\"column\":\"column_2_id\",\"column_name\":\"session_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the court session.\"}]}},\"instructions\":[\"Load the 'court_cases' table from the CSV file located at C:/legal_data/court_cases.csv.\",\"Filter the 'court_cases' table to include only cases where the verdict is 'guilty'.\",\"Join the filtered 'court_cases' table with the 'lawyers' table on the 'lawyer_id' column.\",\"Aggregate the joined table to find the total number of guilty verdicts per lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"verdict == 'guilty'\"},\"output\":\"guilty_cases\",\"comment\":\"Filtering court cases to include only those with a guilty verdict.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyers.csv\",\"type\":\"csv\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"guilty_cases\",\"lawyers\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"guilty_cases_with_lawyers\",\"comment\":\"Joining guilty cases with lawyer information based on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"guilty_cases_with_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_guilty_verdicts\",\"function\":\"count\"}]},\"output\":\"guilty_verdicts_per_lawyer\",\"comment\":\"Aggregating data to find the total number of guilty verdicts per lawyer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_data\",\"location\":\"C:/legal_data/case_data.csv\",\"sheet_name\":null,\"label\":\"Case Data\",\"description\":\"A table containing information about various legal cases, including case ID, judge, verdict, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"judge\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the judge presiding over the case.\"},{\"column\":\"col_3\",\"column_name\":\"verdict\",\"column_type\":\"xsd:string\",\"column_description\":\"The verdict of the case, e.g., guilty, not guilty.\"},{\"column\":\"col_4\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the verdict was given.\"}]},\"table_id2\":{\"name\":\"judge_data\",\"location\":\"C:/legal_data/judge_data.csv\",\"sheet_name\":null,\"label\":\"Judge Data\",\"description\":\"A table containing information about judges, including their ID, name, and court.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"judge_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each judge.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the judge.\"},{\"column\":\"col_3\",\"column_name\":\"court\",\"column_type\":\"xsd:string\",\"column_description\":\"The court where the judge presides.\"}]}},\"instructions\":[\"Load the table 'case_data' which contains information about legal cases.\",\"Filter the 'case_data' table to include only cases where the verdict is 'guilty'.\",\"Aggregate the filtered data to count the number of guilty verdicts per judge.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_data.csv\",\"type\":\"csv\"},\"output\":\"case_data\",\"comment\":\"Loading the table containing information about legal cases.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_data\",\"query\":\"verdict == 'guilty'\"},\"output\":\"guilty_cases\",\"comment\":\"Filtering the case data to include only cases with a guilty verdict.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"guilty_cases\",\"groupBy\":\"judge\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"guilty_verdicts_count\",\"function\":\"count\"}]},\"output\":\"guilty_verdicts_per_judge\",\"comment\":\"Aggregating the filtered data to count the number of guilty verdicts per judge.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications for each product, including product ID, name, and cost.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of the product.\"}]},\"table_id2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including customer ratings and feedback.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Product ID to link with product specifications.\"},{\"column\":\"col_2\",\"column_name\":\"customer_rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating for the product.\"},{\"column\":\"col_3\",\"column_name\":\"feedback_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the customer feedback.\"}]},\"table_id3\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_data.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"Contains sales data including sales price and category for each product.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Product ID to link with product specifications.\"},{\"column\":\"col_2\",\"column_name\":\"sales_price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sales price of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel sheet.\",\"Filter the market research data to include only entries from the last year.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Calculate the average customer rating for each product.\",\"Sort the products by their average customer rating in descending order.\",\"Identify the top 10 products with the highest average customer ratings.\",\"Aggregate the sales data by product category to find total sales.\",\"Calculate the profit margin for each product by subtracting the cost from the sales price.\",\"Forecast the sales for the next quarter using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading the market research data from an Excel sheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"`feedback_date` >= '2022-01-01'\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only entries from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"customer_rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_ratings\",\"comment\":\"Calculating the average customer rating for each product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_ratings\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_products\",\"comment\":\"Sorting products by their average customer rating in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_products\",\"query\":\"index < 10\"},\"output\":\"top_10_products\",\"comment\":\"Identifying the top 10 products with the highest average customer ratings.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_data\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"sales_price\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"category_sales\",\"comment\":\"Aggregating sales data by product category to find total sales.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sales_data\",\"columnName\":\"profit_margin\",\"formula\":\"`sales_price` - `cost`\"},\"output\":\"sales_with_profit\",\"comment\":\"Calculating the profit margin for each product by subtracting the cost from the sales price.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sales_data\",\"columns\":[\"sales_price\"],\"forecastParameters\":{\"forecastPeriod\":90,\"frequency\":\"D\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":30,\"dateColumn\":\"product_id\"}},\"output\":\"sales_forecast\",\"comment\":\"Forecasting the sales for the next quarter using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid list membership check.","output":"'col1 in [1, 2, 3]'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhy is strict definition important in Tamarind workflows?","output":"Strict definition ensures that each action is unambiguous and executes deterministically, avoiding unexpected outcomes in data processing."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comments from the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"column_3_id\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"Table containing sales data for various products.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback to include only those with a rating of 4 or higher.\",\"Load the customer details table from the Excel file.\",\"Join the filtered feedback with customer details using the customer ID.\",\"Aggregate the joined data to find the average rating per region.\",\"Sort the aggregated data by average rating in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback to include only those with a rating of 4 or higher.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"feedback_with_details\",\"comment\":\"Joining filtered feedback with customer details using customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"feedback_with_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_per_region\",\"comment\":\"Aggregating data to find the average rating per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_rating_per_region\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_average_rating\",\"comment\":\"Sorting the aggregated data by average rating in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"This table contains details of shipments including shipment dates, destinations, weights, and carrier IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"Destination city for the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_4\",\"column_name\":\"carrier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the carrier handling the shipment.\"}]},\"table_id2\":{\"name\":\"carrier_info\",\"location\":\"C:/data/logistics/carrier_info.csv\",\"sheet_name\":null,\"label\":\"Carrier Information\",\"description\":\"This table contains information about carriers including their IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"carrier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each carrier.\"},{\"column\":\"col_2\",\"column_name\":\"carrier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the carrier.\"}]},\"table_id3\":{\"name\":\"warehouse_data\",\"location\":\"C:/data/logistics/warehouse_data.csv\",\"sheet_name\":null,\"label\":\"Warehouse Data\",\"description\":\"This table contains information about warehouses including their locations and capacities.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse.\"},{\"column\":\"col_3\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Capacity of the warehouse in cubic meters.\"}]}},\"instructions\":[\"Load the table 'shipment_data' containing details of shipments including dates and destinations.\",\"Filter the shipment data to include only shipments destined for 'New York'.\",\"Join the filtered shipment data with the 'carrier_info' table on the 'carrier_id' column.\",\"Calculate the total weight of shipments for each carrier.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data containing details of shipments.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"destination == 'New York'\"},\"output\":\"ny_shipments\",\"comment\":\"Filtering shipments to include only those destined for New York.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/carrier_info.csv\",\"type\":\"csv\"},\"output\":\"carrier_info\",\"comment\":\"Loading the carrier information table.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"ny_shipments\",\"carrier_info\"],\"joinOn\":[\"carrier_id\"],\"joinType\":\"inner\"},\"output\":\"ny_shipments_with_carriers\",\"comment\":\"Joining filtered shipment data with carrier information on carrier_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"ny_shipments_with_carriers\",\"groupBy\":\"carrier_id\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"total_weight_per_carrier\",\"comment\":\"Calculating the total weight of shipments for each carrier.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of each product including product ID, name, category, and technical details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]},\"table_2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx\",\"sheet_name\":\"2023_data\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including product ID, market demand, and timestamp.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"market_demand\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Market demand for the product.\"},{\"column\":\"col_3\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Timestamp of the market research entry.\"}]},\"table_3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"old_data\",\"label\":\"Old Data\",\"description\":\"Contains outdated data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"old_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Old identifier.\"},{\"column\":\"col_2\",\"column_name\":\"old_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Old value.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel sheet.\",\"Filter the market research data to include only entries from the last year.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Aggregate the joined data to calculate the average market demand per product.\",\"Sort the aggregated data by average market demand in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading the market research data from an Excel sheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"`timestamp` >= '2022-01-01'\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only entries from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"market_demand\",\"name\":\"average_market_demand\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating joined data to calculate average market demand per product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_market_demand\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by average market demand in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/hospital_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains demographic and basic health information of patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the patient.\"}]},\"table_id2\":{\"name\":\"lab_results\",\"location\":\"C:/hospital_data/lab_results.csv\",\"sheet_name\":null,\"label\":\"Lab Results\",\"description\":\"Contains lab test results for patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"cholesterol_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cholesterol level of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"test_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the lab test was conducted.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/hospital_data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Contains information about hospital staff members.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"position\",\"column_type\":\"xsd:string\",\"column_description\":\"Position of the staff member in the hospital.\"}]}},\"instructions\":[\"Load the patient records table from the hospital database.\",\"Load the lab results table from the hospital database.\",\"Filter the patient records to include only patients aged 65 and above.\",\"Join the filtered patient records with lab results on patient ID.\",\"Calculate the average cholesterol level for each patient.\",\"Identify patients with cholesterol levels above 200.\",\"Sort the identified patients by cholesterol level in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records from the hospital database.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/lab_results.csv\",\"type\":\"csv\"},\"output\":\"lab_results\",\"comment\":\"Loading the lab results from the hospital database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"age >= 65\"},\"output\":\"elderly_patients\",\"comment\":\"Filtering patient records to include only patients aged 65 and above.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"elderly_patients\",\"lab_results\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"elderly_patients_with_lab\",\"comment\":\"Joining filtered patient records with lab results on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"elderly_patients_with_lab\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"cholesterol_level\",\"name\":\"average_cholesterol\",\"function\":\"mean\"}]},\"output\":\"average_cholesterol_per_patient\",\"comment\":\"Calculating the average cholesterol level for each patient.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"average_cholesterol_per_patient\",\"query\":\"average_cholesterol > 200\"},\"output\":\"high_cholesterol_patients\",\"comment\":\"Identifying patients with cholesterol levels above 200.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"high_cholesterol_patients\",\"sortBy\":\"average_cholesterol\",\"order\":\"desc\"},\"output\":\"sorted_high_cholesterol_patients\",\"comment\":\"Sorting the identified patients by cholesterol level in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with details such as transaction amount, currency code, and transaction type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"transaction_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code of the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of transaction, such as purchase or refund.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/exchange_rates.xlsx\",\"sheet_name\":\"Rates\",\"label\":\"Exchange Rates\",\"description\":\"A table containing exchange rates for different currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Irrelevant Data\",\"description\":\"A table containing data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at C:/data/financial_transactions.csv.\",\"Load the 'exchange_rates' table from the Excel file located at C:/data/exchange_rates.xlsx.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Join the filtered transactions with the 'exchange_rates' table on the 'currency_code' column.\",\"Calculate a new column 'amount_in_usd' by multiplying 'transaction_amount' with 'exchange_rate'.\",\"Aggregate the joined table to find the total 'amount_in_usd' for each 'transaction_type'.\",\"Sort the aggregated data by 'total_amount_usd' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"transaction_amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with exchange rates on currency code.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"amount_in_usd\",\"formula\":\"transaction_amount * exchange_rate\"},\"output\":\"transactions_with_usd\",\"comment\":\"Calculating amount in USD by multiplying transaction amount with exchange rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_usd\",\"groupBy\":\"transaction_type\",\"aggregations\":[{\"column\":\"amount_in_usd\",\"name\":\"total_amount_usd\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find total amount in USD for each transaction type.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_amount_usd\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total amount in USD in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'convert_timezone' operation be used?","output":"Use 'convert_timezone' when you need to change the time zone of a datetime column, such as converting UTC timestamps to local time."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"This table contains records of financial transactions including transaction ID, customer ID, and transaction amount.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The monetary amount of the transaction.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":null,\"label\":\"Customer Details\",\"description\":\"This table contains customer information including customer ID, name, and contact details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_number\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact number of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_financial_data\",\"location\":\"C:/data/irrelevant_financial_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Financial Data\",\"description\":\"This table contains outdated financial data not relevant to the current analysis.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"old_transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for old transactions.\"},{\"column\":\"col_B\",\"column_name\":\"old_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The monetary amount of old transactions.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at C:/data/financial_transactions.csv.\",\"Filter the 'financial_transactions' table to include only transactions where the amount is greater than 1000.\",\"Load the 'customer_details' table from the Excel file located at C:/data/customer_details.xlsx.\",\"Join the filtered transactions with the customer details on the customer_id column.\",\"Aggregate the joined table by customer_id to calculate the total transaction amount for each customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading the customer details dataset from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer details on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"}]},\"output\":\"customer_transaction_totals\",\"comment\":\"Aggregating data by customer_id to calculate total transaction amount for each customer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/data/hr/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"Contains details of employees including their ID, name, department ID, and salary.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"},{\"column\":\"col_4\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Salary of the employee.\"}]},\"table_id2\":{\"name\":\"department_details\",\"location\":\"C:/data/hr/department_details.csv\",\"sheet_name\":null,\"label\":\"Department Details\",\"description\":\"Contains information about each department including department ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"employee_attendance\",\"location\":\"C:/data/hr/employee_attendance.csv\",\"sheet_name\":null,\"label\":\"Employee Attendance\",\"description\":\"Records of employee attendance including employee ID and attendance dates.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"attendance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of attendance.\"}]}},\"instructions\":[\"Load the employee records table from the CSV file.\",\"Filter the records to include only employees with a salary greater than $50,000.\",\"Join the filtered employee records with the department details table on the department ID.\",\"Calculate the average salary for each department.\",\"Sort the departments by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"`salary` > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employee records to include only those with a salary greater than $50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/department_details.csv\",\"type\":\"csv\"},\"output\":\"department_details\",\"comment\":\"Loading the department details dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_salary_employees\",\"department_details\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"employee_department_info\",\"comment\":\"Joining high salary employee records with department details on department ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"employee_department_info\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"average_salary_by_department\",\"comment\":\"Calculating the average salary for each department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_salary_by_department\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_departments_by_salary\",\"comment\":\"Sorting the departments by average salary in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"Contains customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comments from the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.csv\",\"sheet_name\":null,\"label\":\"Customer Details\",\"description\":\"Contains detailed information about customers including their region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"col_3\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.csv\",\"sheet_name\":null,\"label\":\"Product Catalog\",\"description\":\"Contains information about products available for sale.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]}},\"instructions\":[\"Load the customer feedback data from the CSV file.\",\"Filter the feedback to include only those with a rating below 3.\",\"Join the filtered feedback with customer details using customer ID.\",\"Aggregate the data to find the average rating per customer region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating < 3\"},\"output\":\"low_rating_feedback\",\"comment\":\"Filtering feedback to include only those with a rating below 3.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_rating_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"feedback_with_details\",\"comment\":\"Joining filtered feedback with customer details using customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"feedback_with_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_per_region\",\"comment\":\"Aggregating data to find the average rating per customer region.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if a column name contains spaces but is not enclosed in backticks?","output":"An error is raised because column names with spaces must be enclosed in backticks (`)."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"This table contains details of shipments including shipment date, destination, and other relevant information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"shipment_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date on which the shipment was made.\"},{\"column\":\"col_3\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"The destination city for the shipment.\"}]},\"table_id2\":{\"name\":\"inventory_data\",\"location\":\"C:/data/logistics/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"This table contains information about current inventory levels in different warehouses.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_3\",\"column_name\":\"quantity\",\"column_type\":\"xsd:integer\",\"column_description\":\"The quantity of the product available in the warehouse.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of shipments including dates and destinations.\",\"Filter the shipment data to include only shipments destined for 'New York'.\",\"Sort the filtered shipment data by the 'shipment_date' in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"destination == 'New York'\"},\"output\":\"ny_shipments\",\"comment\":\"Filtering shipments to include only those destined for New York.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"ny_shipments\",\"sortBy\":\"shipment_date\",\"order\":\"asc\"},\"output\":\"sorted_ny_shipments\",\"comment\":\"Sorting the New York shipments by shipment date in ascending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_details\",\"location\":\"C:/HR/employee_details.csv\",\"sheet_name\":null,\"label\":\"Employee Details\",\"description\":\"Contains information about employees, including their start date and department.\",\"columns\":[{\"column\":\"emp_id\",\"column_name\":\"EMP_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"start_date\",\"column_name\":\"START_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the employee started working at the company.\"},{\"column\":\"department\",\"column_name\":\"DEPARTMENT\",\"column_type\":\"xsd:string\",\"column_description\":\"The department in which the employee works.\"}]},\"table_id2\":{\"name\":\"salary_information\",\"location\":\"C:/HR/salary_information.xlsx\",\"sheet_name\":\"Salaries\",\"label\":\"Salary Information\",\"description\":\"Contains salary details for employees, including their employee ID and salary amount.\",\"columns\":[{\"column\":\"emp_id\",\"column_name\":\"EMP_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"salary\",\"column_name\":\"SALARY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary amount for the employee.\"}]},\"table_id3\":{\"name\":\"employee_attendance\",\"location\":\"C:/HR/employee_attendance.csv\",\"sheet_name\":null,\"label\":\"Employee Attendance\",\"description\":\"Contains attendance records for employees, not relevant for the current workflow.\",\"columns\":[{\"column\":\"emp_id\",\"column_name\":\"EMP_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"attendance_date\",\"column_name\":\"ATTENDANCE_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of attendance record.\"}]}},\"instructions\":[\"Load the employee details table from the HR database.\",\"Filter the employees who have been with the company for more than 5 years.\",\"Join the filtered employee data with the salary information table.\",\"Calculate the average salary for employees in each department.\",\"Sort the departments by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR/employee_details.csv\",\"type\":\"csv\"},\"output\":\"employee_details\",\"comment\":\"Loading the employee details table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_details\",\"query\":\"(`START_DATE` <= '2018-10-01')\"},\"output\":\"long_term_employees\",\"comment\":\"Filtering employees who have been with the company for more than 5 years.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR/salary_information.xlsx\",\"type\":\"xlsx\"},\"output\":\"salary_information\",\"comment\":\"Loading the salary information table from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"long_term_employees\",\"salary_information\"],\"joinOn\":[\"EMP_ID\"],\"joinType\":\"inner\"},\"output\":\"employee_salary_data\",\"comment\":\"Joining filtered employee data with salary information based on employee ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"employee_salary_data\",\"groupBy\":\"DEPARTMENT\",\"aggregations\":[{\"column\":\"SALARY\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"department_salary_averages\",\"comment\":\"Calculating the average salary for employees in each department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"department_salary_averages\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_department_salaries\",\"comment\":\"Sorting the departments by average salary in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'rolling_mean' transformation.","output":"{ \"name\": \"applyTimeSeriesTransformations\", \"parameters\": { \"table\": \"temperature_data\", \"transformations\": [ { \"columns\": [\"temperature\"], \"operation\": \"rolling_mean\", \"parameters\": { \"window\": 7 } } ] }, \"output\": \"smoothed_temperature\" }"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat parameters are required for the 'forecastData' action?","output":"'forecastData' requires 'table' (name of the input table), 'columns' (list of numeric columns to forecast), 'forecastParameters' (configuration including 'forecastPeriod', 'frequency', and 'algorithm'), and 'output' (name of the table storing forecast results)."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nAre Boolean operations applied element-wise on table columns?","output":"Yes, Boolean operations are applied element-wise when used with table columns."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid arithmetic expression due to a missing operand.","output":"'col1 + ' // Error: Missing right operand after '+' operator."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipments\",\"location\":\"C:/data/logistics/shipments.csv\",\"sheet_name\":null,\"label\":\"Shipments Data\",\"description\":\"Contains details of shipments including weight, route_id, and other relevant information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"route_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the route associated with the shipment.\"}]},\"table_id2\":{\"name\":\"routes\",\"location\":\"C:/data/logistics/routes.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Routes Data\",\"description\":\"Contains route information including distance and cost per kilometer.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"route_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each route.\"},{\"column\":\"col_2\",\"column_name\":\"distance\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Distance of the route in kilometers.\"},{\"column\":\"col_3\",\"column_name\":\"cost_per_km\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost per kilometer for the route.\"}]},\"table_id3\":{\"name\":\"drivers\",\"location\":\"C:/data/logistics/drivers.csv\",\"sheet_name\":null,\"label\":\"Drivers Data\",\"description\":\"Contains information about drivers, not relevant for this workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"driver_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each driver.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the driver.\"}]}},\"instructions\":[\"Load the 'shipments' table from the CSV file located at C:/data/logistics/shipments.csv.\",\"Load the 'routes' table from the Excel file located at C:/data/logistics/routes.xlsx.\",\"Filter the 'shipments' table to include only shipments with a weight greater than 500 kg.\",\"Join the filtered shipments data with the 'routes' table on the 'route_id' column.\",\"Calculate a new column 'total_cost' in the joined table by multiplying 'distance' by 'cost_per_km'.\",\"Aggregate the data by 'route_id' to find the total shipment cost for each route.\",\"Sort the aggregated data by 'total_shipment_cost' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipments.csv\",\"type\":\"csv\"},\"output\":\"shipments\",\"comment\":\"Loading the shipments data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/routes.xlsx\",\"type\":\"xlsx\"},\"output\":\"routes\",\"comment\":\"Loading the routes data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipments\",\"query\":\"weight > 500\"},\"output\":\"filtered_shipments\",\"comment\":\"Filtering shipments to include only those with weight greater than 500 kg.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipments\",\"routes\"],\"joinOn\":[\"route_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipments with routes data on route_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"total_cost\",\"formula\":\"distance * cost_per_km\"},\"output\":\"joined_data_with_cost\",\"comment\":\"Calculating total cost by multiplying distance by cost per kilometer.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_cost\",\"groupBy\":\"route_id\",\"aggregations\":[{\"column\":\"total_cost\",\"name\":\"total_shipment_cost\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by route_id to find total shipment cost.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_shipment_cost\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total shipment cost in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat parameters are required for the 'applyTransformations' action?","output":"'applyTransformations' requires a 'table' (name of the table) and 'transformations' (a list of transformations, each specifying 'columns', 'operation', and optional 'parameters')."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials\",\"description\":\"Table containing information about raw materials including material ID, quantity, and unit price.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the raw material available.\"},{\"column\":\"unit_price\",\"column_name\":\"UNIT_PRICE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price per unit of the raw material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"Table detailing the production schedule including product ID and required materials.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material required for production.\"},{\"column\":\"required_quantity\",\"column_name\":\"REQUIRED_QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material required for production.\"}]},\"table_id3\":{\"name\":\"inventory\",\"location\":\"C:/data/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory\",\"description\":\"Table containing current inventory levels for various products.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"stock_level\",\"column_name\":\"STOCK_LEVEL\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the product.\"}]}},\"instructions\":[\"Load the 'raw_materials' table from the CSV file.\",\"Load the 'production_schedule' table from the Excel file.\",\"Filter the 'raw_materials' table to include only materials with quantity greater than 100.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on 'material_id'.\",\"Calculate a new column 'total_cost' in the joined table by multiplying 'quantity' by 'unit_price'.\",\"Aggregate the data by 'product_id' to find the total quantity and total cost.\",\"Sort the aggregated data by 'total_cost' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"`QUANTITY` > 100\"},\"output\":\"filtered_raw_materials\",\"comment\":\"Filtering raw materials to include only those with quantity greater than 100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_raw_materials\",\"production_schedule\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered raw materials with production schedule on material_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"total_cost\",\"formula\":\"`QUANTITY` * `UNIT_PRICE`\"},\"output\":\"joined_data_with_cost\",\"comment\":\"Calculating total cost by multiplying quantity by unit price.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_cost\",\"groupBy\":\"PRODUCT_ID\",\"aggregations\":[{\"column\":\"QUANTITY\",\"name\":\"total_quantity\",\"function\":\"sum\"},{\"column\":\"total_cost\",\"name\":\"total_cost\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by product_id to find total quantity and total cost.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_cost\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total cost in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption_2023.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains daily energy consumption data for the year 2023.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy_prices_2023.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains daily energy prices for the year 2023.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kilowatt-hour.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each month.\",\"Identify peak energy usage days by filtering days with consumption above a certain threshold.\",\"Forecast the energy consumption for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption_2023.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data for 2023 from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date`.dt.year == 2023\"},\"output\":\"filtered_energy_2023\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_2023\",\"groupBy\":\"date.dt.month\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"total_monthly_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Calculating the total energy consumption for each month.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"filtered_energy_2023\",\"query\":\"consumption > 1000\"},\"output\":\"peak_usage_days\",\"comment\":\"Identifying peak energy usage days by filtering days with consumption above 1000 kWh.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"filtered_energy_2023\",\"columns\":[\"consumption\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the energy consumption for the next 30 days using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat trigonometric functions are supported in Tamarind expressions?","output":"Supported trigonometric functions include 'sin', 'cos', 'tan', 'arcsin', 'arccos', 'arctan', and 'arctan2'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'dropna' operation be used?","output":"Use 'dropna' when you want to remove rows with missing values entirely. This is useful when missing values are too significant to interpolate or fill."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with columns for transaction ID, customer ID, transaction amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"A table containing details about customers, including customer ID, name, and contact information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_info\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact information for the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_financial_data\",\"location\":\"C:/data/irrelevant_financial_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Financial Data\",\"description\":\"A table with financial data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_id\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant identifier.\"},{\"column\":\"col_B\",\"column_name\":\"irrelevant_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"An irrelevant financial value.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at C:/data/financial_transactions.csv.\",\"Filter the 'financial_transactions' table to include only transactions where the amount is greater than 1000.\",\"Load the 'customer_details' table from the Excel file located at C:/data/customer_details.xlsx.\",\"Join the filtered financial transactions with the customer details on the customer_id column using an inner join.\",\"Aggregate the joined table to calculate the total transaction amount for each customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading the customer details dataset from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer details on customer_id using an inner join.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"}]},\"output\":\"customer_transaction_totals\",\"comment\":\"Aggregating the joined data to calculate the total transaction amount for each customer.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid Boolean expression using eval syntax.","output":"'(col1 > 10) & (col2 < 5)'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid arithmetic expression using negation.","output":"'- col1 + col2'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains the planned production activities including start and end dates, and machine requirements.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"activity_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each production activity.\"},{\"column\":\"col_2\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The start date of the production activity.\"},{\"column\":\"col_3\",\"column_name\":\"end_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The end date of the production activity.\"},{\"column\":\"col_4\",\"column_name\":\"machine_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the machine required for the activity.\"}]},\"table_id2\":{\"name\":\"machine_availability\",\"location\":\"C:/data/manufacturing/machine_availability.csv\",\"sheet_name\":null,\"label\":\"Machine Availability\",\"description\":\"This table provides information on the availability of machines including machine IDs and available hours.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"machine_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the machine.\"},{\"column\":\"col_2\",\"column_name\":\"available_hours\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total available hours for the machine.\"}]},\"table_id3\":{\"name\":\"employee_schedule\",\"location\":\"C:/data/manufacturing/employee_schedule.csv\",\"sheet_name\":null,\"label\":\"Employee Schedule\",\"description\":\"This table contains the work schedules of employees, which is not relevant for the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"shift_start\",\"column_type\":\"xsd:time\",\"column_description\":\"Start time of the employee's shift.\"},{\"column\":\"col_3\",\"column_name\":\"shift_end\",\"column_type\":\"xsd:time\",\"column_description\":\"End time of the employee's shift.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the planned production activities.\",\"Filter the production schedule to include only activities scheduled for the current month.\",\"Join the filtered production schedule with the 'machine_availability' table to ensure machines are available for the scheduled activities.\",\"Aggregate the joined data to calculate the total production hours required for each machine.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the planned production activities from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"`start_date`.dt.month == current_month\"},\"output\":\"current_month_schedule\",\"comment\":\"Filtering production activities to include only those scheduled for the current month.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/machine_availability.csv\",\"type\":\"csv\"},\"output\":\"machine_availability\",\"comment\":\"Loading machine availability data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"current_month_schedule\",\"machine_availability\"],\"joinOn\":[\"machine_id\"],\"joinType\":\"inner\"},\"output\":\"schedule_with_availability\",\"comment\":\"Joining the filtered production schedule with machine availability to ensure machines are available.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"schedule_with_availability\",\"groupBy\":\"machine_id\",\"aggregations\":[{\"column\":\"available_hours\",\"name\":\"total_production_hours\",\"function\":\"sum\"}]},\"output\":\"machine_production_hours\",\"comment\":\"Calculating the total production hours required for each machine.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains demographic and health information about patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the patient.\"}]},\"table_id2\":{\"name\":\"treatment_data\",\"location\":\"C:/data/treatment_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Treatment Data\",\"description\":\"Details of treatments administered to patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"treatment_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of treatment administered.\"},{\"column\":\"col_3\",\"column_name\":\"success\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Success rate of the treatment.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Information about hospital staff members.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file.\",\"Load the treatment data from the Excel file.\",\"Filter the patient records to include only those over 50 years old.\",\"Join the filtered patient records with the treatment data on patient ID.\",\"Calculate a new column in the joined table to determine the treatment success rate.\",\"Aggregate the data to find the average treatment success rate by treatment type.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/treatment_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"treatment_data\",\"comment\":\"Loading treatment data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"age > 50\"},\"output\":\"filtered_patient_records\",\"comment\":\"Filtering patient records to include only those over 50 years old.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_patient_records\",\"treatment_data\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient records with treatment data on patient ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"treatment_success_rate\",\"formula\":\"success\"},\"output\":\"joined_data_with_success_rate\",\"comment\":\"Calculating treatment success rate in the joined table.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_success_rate\",\"groupBy\":\"treatment_type\",\"aggregations\":[{\"column\":\"treatment_success_rate\",\"name\":\"average_success_rate\",\"function\":\"mean\"}]},\"output\":\"average_success_rate_by_treatment\",\"comment\":\"Aggregating data to find the average treatment success rate by treatment type.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid while loop.","output":"'while col1 < 10: col1 + 1' // Error: While loops are not allowed in Tamarind."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid Boolean expression with scalars.","output":"'True and False' // Error: Scalar Boolean expressions are not allowed in Tamarind."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains records of energy consumption with timestamps and consumption values in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The timestamp of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The energy consumption in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy_prices.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains historical energy prices with timestamps and price values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The timestamp of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price_per_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kilowatt-hour.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Filter the data to include only records where the consumption is greater than 100 kWh.\",\"Aggregate the filtered data to calculate the total energy consumption per month.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"consumption_kWh > 100\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the data to include only records where the consumption is greater than 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_consumption\",\"groupBy\":\"timestamp\",\"aggregations\":[{\"column\":\"consumption_kWh\",\"name\":\"total_monthly_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Aggregating the filtered data to calculate the total energy consumption per month.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"This table contains customer feedback including ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Rating given by the customer, ranging from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comment\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comment provided by the customer.\"}]},\"table_2\":{\"name\":\"purchase_history\",\"location\":\"C:/data/purchase_history.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Purchase History\",\"description\":\"This table contains records of customer purchases including amounts and dates.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"purchase_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount spent by the customer in a single purchase.\"},{\"column\":\"col_3\",\"column_name\":\"purchase_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the purchase.\"}]},\"table_3\":{\"name\":\"customer_support_tickets\",\"location\":\"C:/data/customer_support_tickets.csv\",\"sheet_name\":null,\"label\":\"Customer Support Tickets\",\"description\":\"This table contains records of customer support tickets.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"ticket_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each support ticket.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"issue_description\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the issue reported by the customer.\"}]}},\"instructions\":[\"Load the customer feedback data from the CSV file.\",\"Load the customer purchase history from the Excel file.\",\"Filter the feedback data to include only feedback with a rating of 4 or higher.\",\"Join the filtered feedback data with the purchase history on customer ID.\",\"Aggregate the joined data to calculate the average purchase amount per customer.\",\"Sort the aggregated data by average purchase amount in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/purchase_history.xlsx\",\"type\":\"xlsx\"},\"output\":\"purchase_history\",\"comment\":\"Loading customer purchase history from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback data to include only feedback with a rating of 4 or higher.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"purchase_history\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered feedback data with purchase history on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"purchase_amount\",\"name\":\"average_purchase_amount\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating joined data to calculate the average purchase amount per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_purchase_amount\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by average purchase amount in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/marketing/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information of customers including age, income, and city.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"income\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual income of the customer.\"},{\"column\":\"col_4\",\"column_name\":\"city\",\"column_type\":\"xsd:string\",\"column_description\":\"City where the customer resides.\"}]},\"table_id2\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing/marketing_campaigns.csv\",\"sheet_name\":null,\"label\":\"Marketing Campaigns\",\"description\":\"Details of various marketing campaigns run by the company.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each campaign.\"},{\"column\":\"col_2\",\"column_name\":\"campaign_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the marketing campaign.\"},{\"column\":\"col_3\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Start date of the campaign.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Filter the customers who are aged between 25 and 40.\",\"Aggregate the filtered data to find the average income by city.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading the customer demographics table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_demographics\",\"query\":\"age >= 25 and age <= 40\"},\"output\":\"filtered_customers\",\"comment\":\"Filtering customers aged between 25 and 40.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_customers\",\"groupBy\":\"city\",\"aggregations\":[{\"column\":\"income\",\"name\":\"average_income\",\"function\":\"mean\"}]},\"output\":\"average_income_by_city\",\"comment\":\"Aggregating data to find the average income by city.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhy is the 'output' field important in a Tamarind workflow action?","output":"The 'output' field specifies the name of the table where the result of the action is stored, ensuring that subsequent actions can reference it."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains the manufacturing schedule for the upcoming month, including product IDs, quantities, and status.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"quantity\",\"column_type\":\"xsd:integer\",\"column_description\":\"The quantity of the product to be manufactured.\"},{\"column\":\"column_3_id\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"The current status of the production order, e.g., pending, completed.\"}]},\"table_id2\":{\"name\":\"inventory\",\"location\":\"C:/data/manufacturing/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory Details\",\"description\":\"This table contains current inventory levels for each product, identified by product ID.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"current_stock\",\"column_type\":\"xsd:integer\",\"column_description\":\"The current stock level of the product.\"}]},\"table_id3\":{\"name\":\"employee_schedule\",\"location\":\"C:/data/manufacturing/employee_schedule.csv\",\"sheet_name\":null,\"label\":\"Employee Schedule\",\"description\":\"This table contains the work schedule for employees, including shifts and assigned tasks.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"column_2_id\",\"column_name\":\"shift\",\"column_type\":\"xsd:string\",\"column_description\":\"The shift assigned to the employee.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the manufacturing schedule for the upcoming month.\",\"Filter the 'production_schedule' to include only entries where the 'status' is 'pending'.\",\"Join the filtered 'production_schedule' with the 'inventory' table on 'product_id' to get inventory details.\",\"Aggregate the joined table to calculate the total quantity required for each product.\",\"Sort the aggregated data by 'total_quantity' in descending order to prioritize production.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the manufacturing schedule for the upcoming month from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"status == 'pending'\"},\"output\":\"pending_production_schedule\",\"comment\":\"Filtering the production schedule to include only pending entries.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"pending_production_schedule\",\"inventory\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"production_with_inventory\",\"comment\":\"Joining the filtered production schedule with inventory details based on product_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"production_with_inventory\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"quantity\",\"name\":\"total_quantity\",\"function\":\"sum\"}]},\"output\":\"aggregated_production\",\"comment\":\"Aggregating the joined table to calculate the total quantity required for each product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_production\",\"sortBy\":\"total_quantity\",\"order\":\"desc\"},\"output\":\"prioritized_production\",\"comment\":\"Sorting the aggregated data by total quantity in descending order to prioritize production.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains daily production targets for each machine in the manufacturing plant.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date for which the production target is set.\"},{\"column\":\"col_2\",\"column_name\":\"machine_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each machine.\"},{\"column\":\"col_3\",\"column_name\":\"production_target\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The production target for the machine on the given date.\"}]},\"table_id2\":{\"name\":\"machine_status\",\"location\":\"C:/data/manufacturing/machine_status.csv\",\"sheet_name\":null,\"label\":\"Machine Status\",\"description\":\"This table contains the operational status of machines in the manufacturing plant.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"machine_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each machine.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"The operational status of the machine (e.g., operational, maintenance, offline).\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/manufacturing/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"This table contains inventory levels for raw materials.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"col_2\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The quantity of the raw material available in inventory.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the daily production targets.\",\"Filter the production schedule to include only entries for the current month.\",\"Load the table 'machine_status' which contains the operational status of machines.\",\"Join the filtered production schedule with machine status data on the machine ID.\",\"Aggregate the joined data to calculate the total production target for each machine.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule containing daily production targets.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"`date`.dt.month == pd.Timestamp('now').month\"},\"output\":\"current_month_schedule\",\"comment\":\"Filtering the production schedule to include only entries for the current month.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/machine_status.csv\",\"type\":\"csv\"},\"output\":\"machine_status\",\"comment\":\"Loading the machine status data to check operational status of machines.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"current_month_schedule\",\"machine_status\"],\"joinOn\":[\"machine_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered production schedule with machine status data on machine ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"machine_id\",\"aggregations\":[{\"column\":\"production_target\",\"name\":\"total_production_target\",\"function\":\"sum\"}]},\"output\":\"aggregated_production_targets\",\"comment\":\"Aggregating the joined data to calculate the total production target for each machine.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications for each product, including priority levels.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"priority\",\"column_type\":\"xsd:string\",\"column_description\":\"Priority level of the product development.\"},{\"column\":\"col_3\",\"column_name\":\"specification_details\",\"column_type\":\"xsd:string\",\"column_description\":\"Detailed specifications of the product.\"}]},\"table_id2\":{\"name\":\"supplier_details\",\"location\":\"C:/data/supplier_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Supplier Details\",\"description\":\"Contains information about suppliers, including costs associated with each product.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"},{\"column\":\"col_3\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost associated with the product from the supplier.\"}]},\"table_id3\":{\"name\":\"market_analysis\",\"location\":\"C:/data/market_analysis.xlsx\",\"sheet_name\":\"Analysis\",\"label\":\"Market Analysis\",\"description\":\"Contains market analysis data, not directly relevant to product development.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"market_trend\",\"column_type\":\"xsd:string\",\"column_description\":\"Current market trends.\"},{\"column\":\"col_2\",\"column_name\":\"forecast\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Forecasted market growth.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a high priority.\",\"Join the filtered product specifications with the supplier details table.\",\"Aggregate the joined data to calculate the total cost for each product.\",\"Sort the aggregated data by total cost in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"priority == 'High'\"},\"output\":\"high_priority_products\",\"comment\":\"Filtering the product specifications to include only high priority products.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"supplier_details\",\"comment\":\"Loading supplier details from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_priority_products\",\"supplier_details\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_product_supplier\",\"comment\":\"Joining high priority products with supplier details based on product_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_product_supplier\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"cost\",\"name\":\"total_cost\",\"function\":\"sum\"}]},\"output\":\"aggregated_costs\",\"comment\":\"Aggregating the joined data to calculate the total cost for each product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_costs\",\"sortBy\":\"total_cost\",\"order\":\"desc\"},\"output\":\"sorted_products\",\"comment\":\"Sorting the aggregated data by total cost in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid comparison expression using inequality.","output":"'col1 != col2'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials\",\"description\":\"Table containing details about raw materials including material ID, stock level, and unit cost.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"stock_level\",\"column_name\":\"STOCK_LEVEL\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the material.\"},{\"column\":\"unit_cost\",\"column_name\":\"UNIT_COST\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost per unit of the material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"Table outlining the production schedule, tasks, and required materials.\",\"columns\":[{\"column\":\"task_id\",\"column_name\":\"TASK_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each production task.\"},{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material required for the task.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material required for the task.\"},{\"column\":\"production_line\",\"column_name\":\"PRODUCTION_LINE\",\"column_type\":\"xsd:string\",\"column_description\":\"Production line where the task is scheduled.\"}]},\"table_id3\":{\"name\":\"employee_records\",\"location\":\"C:/data/manufacturing/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"Table containing employee details, not relevant to the current workflow.\",\"columns\":[{\"column\":\"employee_id\",\"column_name\":\"EMPLOYEE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"name\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"department\",\"column_name\":\"DEPARTMENT\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the employee works.\"}]}},\"instructions\":[\"Load the 'raw_materials' table containing information about raw materials used in manufacturing.\",\"Load the 'production_schedule' table which outlines the production timeline and tasks.\",\"Filter the 'raw_materials' table to include only materials with a stock level greater than 100 units.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Calculate a new column 'total_cost' in the joined table by multiplying 'quantity' by 'unit_cost'.\",\"Aggregate the data by 'production_line' to find the total cost and total quantity of materials used.\",\"Sort the aggregated data by 'total_cost' in descending order to prioritize production lines with higher costs.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"STOCK_LEVEL > 100\"},\"output\":\"filtered_raw_materials\",\"comment\":\"Filtering raw materials to include only those with stock level greater than 100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_raw_materials\",\"production_schedule\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered raw materials with production schedule on material ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"total_cost\",\"formula\":\"QUANTITY * UNIT_COST\"},\"output\":\"data_with_cost\",\"comment\":\"Calculating total cost by multiplying quantity by unit cost.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_cost\",\"groupBy\":\"PRODUCTION_LINE\",\"aggregations\":[{\"column\":\"total_cost\",\"name\":\"total_cost\",\"function\":\"sum\"},{\"column\":\"QUANTITY\",\"name\":\"total_quantity\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by production line to find total cost and quantity.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_cost\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total cost in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'sortData' action with multiple sorting columns.","output":"{ \"name\": \"sortData\", \"parameters\": { \"table\": \"sales\", \"sortBy\": [\"region\", \"revenue\"], \"order\": [\"asc\", \"desc\"] }, \"output\": \"sorted_sales\" }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of products including their development status.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"development_status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current development status of the product.\"}]},\"table_id2\":{\"name\":\"team_assignments\",\"location\":\"C:/data/team_assignments.csv\",\"sheet_name\":null,\"label\":\"Team Assignments\",\"description\":\"Lists team members assigned to each product by product ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"team_member_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each team member.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"Contains sales data for products, not relevant for current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only products with a development status of 'in progress'.\",\"Join the filtered product specifications with the team assignments table on the product ID.\",\"Aggregate the joined table to calculate the total number of team members assigned to each product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"development_status == 'in progress'\"},\"output\":\"in_progress_products\",\"comment\":\"Filtering products to include only those with a development status of 'in progress'.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/team_assignments.csv\",\"type\":\"csv\"},\"output\":\"team_assignments\",\"comment\":\"Loading the team assignments from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"in_progress_products\",\"team_assignments\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"product_team_info\",\"comment\":\"Joining filtered product specifications with team assignments on product ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"product_team_info\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"team_member_id\",\"name\":\"total_team_members\",\"function\":\"count\"}]},\"output\":\"product_team_summary\",\"comment\":\"Aggregating to calculate the total number of team members assigned to each product.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if the specified 'sortBy' columns do not exist in the table?","output":"If 'sortBy' columns do not exist, sorting is skipped."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the 'z-score' algorithm used for?","output":"Use 'z-score' for anomaly detection by identifying outliers using a standard deviation threshold."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"A table containing detailed records of employees including their salary, hire date, and other personal information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Full name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"hire_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the employee was hired.\"}]},\"table_id2\":{\"name\":\"department_info\",\"location\":\"C:/data/department_info.csv\",\"sheet_name\":null,\"label\":\"Department Information\",\"description\":\"A table containing information about different departments within the company.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]}},\"instructions\":[\"Load the employee records from the CSV file located at C:/data/employee_records.csv.\",\"Filter the employee records to include only those with a salary greater than 50000.\",\"Sort the filtered employee records by the 'hire_date' column in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employee records to include only those with a salary greater than 50000.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"high_salary_employees\",\"sortBy\":\"hire_date\",\"order\":\"asc\"},\"output\":\"sorted_high_salary_employees\",\"comment\":\"Sorting the filtered employee records by hire date in ascending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Department Data\",\"description\":\"Contains information about departments including their IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"project_data\",\"location\":\"C:/data/project_data.xlsx\",\"sheet_name\":\"Projects\",\"label\":\"Project Data\",\"description\":\"Contains information about projects including project IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"project_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the project.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/employee_data.csv.\",\"Filter the employee data to include only those with a salary greater than 50000.\",\"Load the department data from the Excel file located at C:/data/department_data.xlsx.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Aggregate the joined data to calculate the average salary per department.\",\"Sort the aggregated data by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading the employee data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employees with a salary greater than 50000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_data\",\"comment\":\"Loading the department data from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employee_data\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating the average salary per department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by average salary in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transaction records including transaction ID, customer ID, amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer associated with the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"customer_info\",\"location\":\"C:/data/customer_info.xlsx\",\"sheet_name\":null,\"label\":\"Customer Information\",\"description\":\"A table containing customer details including customer ID, name, and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Geographical region of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"A table containing data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the data entry.\"},{\"column\":\"col_2\",\"column_name\":\"info\",\"column_type\":\"xsd:string\",\"column_description\":\"Additional information.\"}]}},\"instructions\":[\"Load the table 'financial_transactions' from the CSV file located at C:/data/financial_transactions.csv.\",\"Filter the 'financial_transactions' table to include only transactions where the amount is greater than 1000.\",\"Load the 'customer_info' table from the Excel file located at C:/data/customer_info.xlsx.\",\"Join the filtered 'financial_transactions' table with the 'customer_info' table on the 'customer_id' column.\",\"Aggregate the joined table by 'region' to calculate the total transaction amount.\",\"Sort the aggregated data by total transaction amount in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_info.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_info\",\"comment\":\"Loading the customer information dataset from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_info\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer information based on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by region to calculate the total transaction amount.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_transaction_amount\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total transaction amount in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of products including their development status.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"development_status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current development status of the product.\"}]},\"table_id2\":{\"name\":\"team_assignments\",\"location\":\"C:/data/team_assignments.csv\",\"sheet_name\":null,\"label\":\"Team Assignments\",\"description\":\"Lists team members assigned to each product by product ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"team_member_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each team member.\"}]},\"table_id3\":{\"name\":\"archived_products\",\"location\":\"C:/data/archived_products.csv\",\"sheet_name\":null,\"label\":\"Archived Products\",\"description\":\"Contains information about products that have been archived.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"archive_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the product was archived.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only products with a development status of 'in progress'.\",\"Join the filtered product specifications with the team assignments table on the product ID.\",\"Aggregate the joined data to calculate the total number of team members assigned to each product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"development_status == 'in progress'\"},\"output\":\"in_progress_products\",\"comment\":\"Filtering products to include only those with a development status of 'in progress'.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/team_assignments.csv\",\"type\":\"csv\"},\"output\":\"team_assignments\",\"comment\":\"Loading the team assignments from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"in_progress_products\",\"team_assignments\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"product_team_data\",\"comment\":\"Joining filtered products with team assignments on product ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"product_team_data\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"team_member_id\",\"name\":\"total_team_members\",\"function\":\"count\"}]},\"output\":\"product_team_summary\",\"comment\":\"Aggregating data to calculate the total number of team members assigned to each product.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Manufacturing Production Schedule\",\"description\":\"This table contains the manufacturing schedule data including job IDs, status, and priority.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"job_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each manufacturing job.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the manufacturing job (e.g., pending, completed).\"},{\"column\":\"col_3\",\"column_name\":\"priority\",\"column_type\":\"xsd:integer\",\"column_description\":\"Priority level of the job, with higher numbers indicating higher priority.\"}]},\"table_id2\":{\"name\":\"machine_maintenance\",\"location\":\"C:/data/manufacturing/machine_maintenance.csv\",\"sheet_name\":null,\"label\":\"Machine Maintenance Records\",\"description\":\"This table contains records of machine maintenance activities.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"machine_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each machine.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the maintenance was performed.\"},{\"column\":\"col_3\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Status of the maintenance activity.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the manufacturing schedule data.\",\"Filter the production schedule to include only entries with a status of 'pending'.\",\"Sort the filtered production schedule by priority in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the manufacturing production schedule from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"status == 'pending'\"},\"output\":\"pending_production_schedule\",\"comment\":\"Filtering the production schedule to include only jobs with a status of 'pending'.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"pending_production_schedule\",\"sortBy\":\"priority\",\"order\":\"desc\"},\"output\":\"sorted_pending_schedule\",\"comment\":\"Sorting the filtered production schedule by priority in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"legal_cases\",\"location\":\"C:/data/legal_cases.csv\",\"sheet_name\":null,\"label\":\"Legal Cases\",\"description\":\"A table containing details of various legal cases, including case ID, lawyer ID, and case date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each legal case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"case_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"}]},\"table_2\":{\"name\":\"lawyer_info\",\"location\":\"C:/data/lawyer_info.csv\",\"sheet_name\":null,\"label\":\"Lawyer Information\",\"description\":\"A table containing information about lawyers, including their ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the lawyer.\"}]},\"table_3\":{\"name\":\"court_schedule\",\"location\":\"C:/data/court_schedule.csv\",\"sheet_name\":null,\"label\":\"Court Schedule\",\"description\":\"A table containing the schedule of court hearings, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"hearing_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court hearing.\"},{\"column\":\"col_2\",\"column_name\":\"hearing_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the court hearing.\"}]}},\"instructions\":[\"Load the legal cases table from the CSV file.\",\"Filter the cases to include only those from the year 2022.\",\"Join the filtered cases with the lawyer information table on the lawyer_id column.\",\"Aggregate the joined data to find the total number of cases handled by each lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal_cases.csv\",\"type\":\"csv\"},\"output\":\"legal_cases\",\"comment\":\"Loading the legal cases dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"legal_cases\",\"query\":\"`case_date`.year == 2022\"},\"output\":\"cases_2022\",\"comment\":\"Filtering cases to include only those from the year 2022.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/lawyer_info.csv\",\"type\":\"csv\"},\"output\":\"lawyer_info\",\"comment\":\"Loading the lawyer information dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"cases_2022\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered cases with lawyer information on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating data to find the total number of cases handled by each lawyer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"A table containing employee details including ID, name, department ID, and salary.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"},{\"column\":\"col_4\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Salary of the employee.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.csv\",\"sheet_name\":null,\"label\":\"Department Data\",\"description\":\"A table containing department details including department ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"project_assignments\",\"location\":\"C:/data/project_assignments.csv\",\"sheet_name\":null,\"label\":\"Project Assignments\",\"description\":\"A table containing project assignments for employees, not relevant for this workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"}]}},\"instructions\":[\"Load the employee records from the CSV file.\",\"Filter the records to include only employees with a salary greater than $50,000.\",\"Join the filtered records with the department data to get department names.\",\"Aggregate the data to find the average salary per department.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering records to include only employees with a salary greater than $50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.csv\",\"type\":\"csv\"},\"output\":\"department_data\",\"comment\":\"Loading the department data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_salary_employees\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"employees_with_departments\",\"comment\":\"Joining filtered employee records with department data to get department names.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"employees_with_departments\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"average_salary_per_department\",\"comment\":\"Aggregating data to find the average salary per department.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/hospital_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains detailed records of patients including their diagnoses and personal information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"admission_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of patient admission.\"}]},\"table_id2\":{\"name\":\"medication\",\"location\":\"C:/hospital_data/medication.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Contains records of medications prescribed to patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"},{\"column\":\"col_3\",\"column_name\":\"prescription_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the medication was prescribed.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/hospital_data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff Records\",\"description\":\"Contains records of hospital staff including their roles and departments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the staff member works.\"}]}},\"instructions\":[\"Load the patient records table from the hospital database.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Join the filtered patient records with the medication table on patient ID.\",\"Aggregate the joined data to calculate the total number of medications prescribed per patient.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from the hospital database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetes_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with diabetes.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/medication.csv\",\"type\":\"csv\"},\"output\":\"medication\",\"comment\":\"Loading medication records from the hospital database.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"diabetes_patients\",\"medication\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient records with medication records on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"medication_name\",\"name\":\"total_medications\",\"function\":\"count\"}]},\"output\":\"medication_summary\",\"comment\":\"Aggregating data to calculate the total number of medications prescribed per patient.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat operations are supported in 'applyTransformations'?","output":"Supported operations include: 'interpolate', 'fillna', 'dropna', 'log_transform', 'exp_transform', 'sin_transform', 'cos_transform', 'tan_transform', 'min_max_scale', 'standardize', 'normalize', 'power_transform', 'rolling_quantile', 'rolling_skew', and 'rolling_kurtosis'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of products including status, cost, and supplier ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Approval status of the product.\"},{\"column\":\"col_3\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of the product.\"},{\"column\":\"col_4\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]},\"table_id2\":{\"name\":\"supplier_details\",\"location\":\"C:/data/supplier_details.csv\",\"sheet_name\":null,\"label\":\"Supplier Details\",\"description\":\"Contains information about suppliers including their IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a status of 'approved'.\",\"Join the filtered product specifications with the supplier details table on the supplier ID.\",\"Aggregate the joined table to calculate the total cost per product category.\",\"Sort the aggregated data by total cost in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"status == 'approved'\"},\"output\":\"approved_products\",\"comment\":\"Filtering the product specifications to include only approved products.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_details.csv\",\"type\":\"csv\"},\"output\":\"supplier_details\",\"comment\":\"Loading the supplier details from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"approved_products\",\"supplier_details\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining approved products with supplier details on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_table\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"cost\",\"name\":\"total_cost\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined table to calculate total cost per product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_cost\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total cost in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of each product including product ID, name, and estimated adoption rate.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"estimated_adoption_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Estimated rate at which the product will be adopted by the market.\"}]},\"table_id2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx:sheet=2023&cols=1:10,rows=1:500\",\"sheet_name\":\"2023\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including product ID, target audience size, and date of research.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"target_audience_size\",\"column_type\":\"xsd:integer\",\"column_description\":\"Size of the target audience for the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"research_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the market research was conducted.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.xlsx:sheet=OldData&cols=1:5,rows=1:100\",\"sheet_name\":\"OldData\",\"label\":\"Irrelevant Data\",\"description\":\"Contains outdated data not relevant to the current analysis.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"old_product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Old product identifier.\"},{\"column\":\"column_2_id\",\"column_name\":\"old_data\",\"column_type\":\"xsd:string\",\"column_description\":\"Outdated information.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel sheet.\",\"Filter the market research data to include only entries from the last year.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Calculate the potential market size by multiplying the target audience size by the product's estimated adoption rate.\",\"Aggregate the joined data to find the average potential market size per product category.\",\"Sort the aggregated data by potential market size in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx:sheet=2023\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading the market research data from an Excel sheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"`research_date` >= '2022-01-01'\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only entries from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_market_size\",\"formula\":\"`target_audience_size` * `estimated_adoption_rate`\"},\"output\":\"data_with_market_size\",\"comment\":\"Calculating potential market size by multiplying target audience size by the product's estimated adoption rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_market_size\",\"groupBy\":\"product_category\",\"aggregations\":[{\"column\":\"potential_market_size\",\"name\":\"average_market_size\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find the average potential market size per product category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_market_size\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by potential market size in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'sortData' action with an unsupported sorting order.","output":"{ \"name\": \"sortData\", \"parameters\": { \"table\": \"sales\", \"sortBy\": \"revenue\", \"order\": \"unsupported_order\" }, \"output\": \"sorted_sales\" } // Error: 'unsupported_order' is not a valid sorting order."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains hourly energy consumption data for a facility, including timestamps and consumption values in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption reading.\"},{\"column\":\"col_2\",\"column_name\":\"consumption_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature, humidity, and wind speed.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather observation.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature in degrees Celsius.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Filter the data to include only entries where the consumption is greater than 100 kWh.\",\"Aggregate the filtered data by day to calculate the total daily energy consumption.\",\"Sort the aggregated data by total daily consumption in descending order.\",\"Forecast the next 30 days of energy consumption using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"consumption_kWh > 100\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the data to include only entries with consumption greater than 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_consumption\",\"groupBy\":\"timestamp\",\"aggregations\":[{\"column\":\"consumption_kWh\",\"name\":\"total_daily_consumption\",\"function\":\"sum\"}]},\"output\":\"daily_energy_consumption\",\"comment\":\"Aggregating the filtered data by day to calculate total daily energy consumption.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"daily_energy_consumption\",\"sortBy\":\"total_daily_consumption\",\"order\":\"desc\"},\"output\":\"sorted_daily_energy_consumption\",\"comment\":\"Sorting the aggregated data by total daily consumption in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sorted_daily_energy_consumption\",\"columns\":[\"total_daily_consumption\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"timestamp\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the next 30 days of energy consumption using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains the manufacturing schedule with task details and scheduled dates.\",\"columns\":[{\"column\":\"task_id\",\"column_name\":\"TASK_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each manufacturing task.\"},{\"column\":\"scheduled_date\",\"column_name\":\"SCHEDULED_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date on which the task is scheduled.\"},{\"column\":\"machine_id\",\"column_name\":\"MACHINE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the machine required for the task.\"},{\"column\":\"task_duration\",\"column_name\":\"TASK_DURATION\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Duration of the task in hours.\"}]},\"table_id2\":{\"name\":\"machine_availability\",\"location\":\"C:/data/manufacturing/machine_availability.csv\",\"sheet_name\":null,\"label\":\"Machine Availability\",\"description\":\"This table contains information about machine availability for the manufacturing tasks.\",\"columns\":[{\"column\":\"machine_id\",\"column_name\":\"MACHINE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the machine.\"},{\"column\":\"available_date\",\"column_name\":\"AVAILABLE_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date on which the machine is available.\"}]},\"table_id3\":{\"name\":\"inventory\",\"location\":\"C:/data/manufacturing/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory\",\"description\":\"This table contains inventory details for raw materials and finished goods.\",\"columns\":[{\"column\":\"item_id\",\"column_name\":\"ITEM_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the item in stock.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the manufacturing schedule.\",\"Filter the production schedule to include only tasks scheduled for the current week.\",\"Join the filtered production schedule with the 'machine_availability' table to ensure machines are available for the tasks.\",\"Aggregate the joined data to calculate the total production time required for each machine.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the manufacturing schedule from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"`SCHEDULED_DATE` >= '2023-10-23' and `SCHEDULED_DATE` <= '2023-10-29'\"},\"output\":\"current_week_schedule\",\"comment\":\"Filtering the production schedule to include only tasks scheduled for the current week.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"current_week_schedule\",\"machine_availability\"],\"joinOn\":[\"MACHINE_ID\"],\"joinType\":\"inner\"},\"output\":\"schedule_with_availability\",\"comment\":\"Joining the filtered production schedule with machine availability to ensure machines are available for the tasks.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"schedule_with_availability\",\"groupBy\":\"MACHINE_ID\",\"aggregations\":[{\"column\":\"TASK_DURATION\",\"name\":\"total_production_time\",\"function\":\"sum\"}]},\"output\":\"machine_production_time\",\"comment\":\"Aggregating the joined data to calculate the total production time required for each machine.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat forecast frequencies are supported in 'forecastData'?","output":"Supported forecast frequencies include 'B' (business day), 'C' (custom business day), 'D' (calendar day), 'W' (weekly), 'ME' (month end), 'BME' (business month end), 'MS' (month start), 'BMS' (business month start), 'QE' (quarter end), 'QS' (quarter start), 'YE' (year end), 'YS' (year start), 'h' (hourly), 'bh' (business hour), 'min' (minutely), 's' (secondly), and more."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Customer comments on the service.\"}]},\"table_2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"col_3\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.csv\",\"sheet_name\":null,\"label\":\"Product Catalog\",\"description\":\"Table containing information about products offered by the company.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback to include only entries with a rating below 3.\",\"Load the customer details table from the Excel file.\",\"Join the filtered feedback with customer details using the customer ID.\",\"Aggregate the data to find the average rating per region.\",\"Sort the aggregated data by average rating in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating < 3\"},\"output\":\"low_rating_feedback\",\"comment\":\"Filtering feedback to include only entries with a rating below 3.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_rating_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"detailed_low_rating_feedback\",\"comment\":\"Joining filtered feedback with customer details using customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"detailed_low_rating_feedback\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_per_region\",\"comment\":\"Aggregating data to find the average rating per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_rating_per_region\",\"sortBy\":\"average_rating\",\"order\":\"asc\"},\"output\":\"sorted_average_rating_per_region\",\"comment\":\"Sorting the aggregated data by average rating in ascending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"building_energy_data\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and energy usage in kilowatt-hours.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Building Information\",\"description\":\"This table contains information about the buildings, such as their location and type.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_B\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location of the building.\"},{\"column\":\"col_C\",\"column_name\":\"building_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of building, e.g., residential, commercial.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_X\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each building.\",\"Sort the buildings by total energy consumption in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"building_energy_data\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"building_energy_data\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"filtered_energy_data\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_data\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_usage_kwh\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"total_energy_per_building\",\"comment\":\"Calculating the total energy consumption for each building.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"total_energy_per_building\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_buildings\",\"comment\":\"Sorting the buildings by total energy consumption in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if a column referenced in the formula does not exist?","output":"If a column referenced in the formula does not exist, it is treated as 'null'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'decompose' operation be used?","output":"Use 'decompose' when you need to break down a time series into trend, seasonality, and residual components."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/marketing/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics Data\",\"description\":\"This table contains demographic information about customers, including age, income, and city.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"income\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual income of the customer.\"},{\"column\":\"col_4\",\"column_name\":\"city\",\"column_type\":\"xsd:string\",\"column_description\":\"City where the customer resides.\"}]},\"table_id2\":{\"name\":\"sales_data\",\"location\":\"C:/data/marketing/sales_data.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"This table contains sales records, including transaction amounts and customer IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Transaction amount.\"}]}},\"instructions\":[\"Load the customer demographics data from the CSV file.\",\"Filter the customer data to include only those aged between 25 and 40.\",\"Aggregate the filtered data to find the average income by city.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading the customer demographics data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_demographics\",\"query\":\"`age` >= 25 and `age` <= 40\"},\"output\":\"filtered_customers\",\"comment\":\"Filtering the customer data to include only those aged between 25 and 40.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_customers\",\"groupBy\":\"city\",\"aggregations\":[{\"column\":\"income\",\"name\":\"average_income\",\"function\":\"mean\"}]},\"output\":\"average_income_by_city\",\"comment\":\"Aggregating the filtered data to find the average income by city.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_details\",\"location\":\"C:/data/legal/case_details.csv\",\"sheet_name\":null,\"label\":\"Case Details\",\"description\":\"Contains details of legal cases including start and end dates, status, and associated lawyer.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case started.\"},{\"column\":\"col_4\",\"column_name\":\"end_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case ended.\"},{\"column\":\"col_5\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the case.\"}]},\"table_id2\":{\"name\":\"lawyer_info\",\"location\":\"C:/data/legal/lawyer_info.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Lawyer Information\",\"description\":\"Contains information about lawyers including their ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedule\",\"location\":\"C:/data/legal/court_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Court Schedule\",\"description\":\"Contains the schedule of court hearings, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"hearing_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court hearing.\"},{\"column\":\"col_2\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court hearing.\"}]}},\"instructions\":[\"Load the 'case_details' table from the CSV file located at C:/data/legal/case_details.csv.\",\"Load the 'lawyer_info' table from the Excel file located at C:/data/legal/lawyer_info.xlsx.\",\"Filter the 'case_details' table to include only cases with a status of 'open'.\",\"Join the filtered 'case_details' table with the 'lawyer_info' table on the 'lawyer_id' column.\",\"Aggregate the joined table to count the number of open cases per lawyer.\",\"Sort the aggregated data by the number of open cases in descending order.\",\"Calculate a new column 'case_duration' in the 'case_details' table as the difference between 'end_date' and 'start_date'.\",\"Filter the 'case_details' table to include only cases with a 'case_duration' greater than 30 days.\",\"Join the filtered 'case_details' with the 'lawyer_info' table again to get lawyer details for long-duration cases.\",\"Aggregate the final joined table to find the average 'case_duration' per lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/case_details.csv\",\"type\":\"csv\"},\"output\":\"case_details\",\"comment\":\"Loading the case details dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/lawyer_info.xlsx\",\"type\":\"xlsx\"},\"output\":\"lawyer_info\",\"comment\":\"Loading the lawyer information dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_details\",\"query\":\"status == 'open'\"},\"output\":\"open_cases\",\"comment\":\"Filtering the case details to include only open cases.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"open_cases\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"open_cases_with_lawyers\",\"comment\":\"Joining open cases with lawyer information on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"open_cases_with_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"open_case_count\",\"function\":\"count\"}]},\"output\":\"open_cases_per_lawyer\",\"comment\":\"Aggregating to count the number of open cases per lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"open_cases_per_lawyer\",\"sortBy\":\"open_case_count\",\"order\":\"desc\"},\"output\":\"sorted_open_cases_per_lawyer\",\"comment\":\"Sorting the aggregated data by the number of open cases in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"case_details\",\"columnName\":\"case_duration\",\"formula\":\"`end_date` - `start_date`\"},\"output\":\"case_details_with_duration\",\"comment\":\"Calculating the case duration as the difference between end_date and start_date.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_details_with_duration\",\"query\":\"case_duration > 30\"},\"output\":\"long_duration_cases\",\"comment\":\"Filtering cases to include only those with a duration greater than 30 days.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"long_duration_cases\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"long_duration_cases_with_lawyers\",\"comment\":\"Joining long-duration cases with lawyer information to get lawyer details.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"long_duration_cases_with_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_duration\",\"name\":\"average_case_duration\",\"function\":\"mean\"}]},\"output\":\"average_case_duration_per_lawyer\",\"comment\":\"Aggregating to find the average case duration per lawyer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/marketing/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"This table contains demographic information of customers including age, income, and city.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"income\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual income of the customer.\"},{\"column\":\"col_4\",\"column_name\":\"city\",\"column_type\":\"xsd:string\",\"column_description\":\"City where the customer resides.\"}]},\"table_id2\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing/marketing_campaigns.xlsx\",\"sheet_name\":\"Campaigns\",\"label\":\"Marketing Campaigns\",\"description\":\"Details of various marketing campaigns run by the company.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each campaign.\"},{\"column\":\"col_2\",\"column_name\":\"campaign_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the marketing campaign.\"},{\"column\":\"col_3\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Start date of the campaign.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Filter the customers who are aged between 25 and 40.\",\"Aggregate the filtered data to find the average income by city.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading the customer demographics data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_demographics\",\"query\":\"age >= 25 and age <= 40\"},\"output\":\"filtered_customers\",\"comment\":\"Filtering customers aged between 25 and 40.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_customers\",\"groupBy\":\"city\",\"aggregations\":[{\"column\":\"income\",\"name\":\"average_income\",\"function\":\"mean\"}]},\"output\":\"average_income_by_city\",\"comment\":\"Aggregating data to find the average income by city.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"building_energy_data\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and energy usage values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.xlsx\",\"sheet_name\":\"Info\",\"label\":\"Building Information\",\"description\":\"This table contains additional information about the buildings, such as location and size.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_B\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location of the building.\"},{\"column\":\"col_C\",\"column_name\":\"size\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The size of the building in square meters.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_X\",\"column_name\":\"random_id\",\"column_type\":\"xsd:string\",\"column_description\":\"A random identifier.\"},{\"column\":\"col_Y\",\"column_name\":\"random_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"A random value.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each building.\",\"Sort the buildings by total energy consumption in descending order.\",\"Identify the top 5 buildings with the highest energy consumption.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"building_energy_data\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"building_energy_data\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"filtered_energy_data\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_data\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"total_energy_by_building\",\"comment\":\"Calculating the total energy consumption for each building.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"total_energy_by_building\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_energy_data\",\"comment\":\"Sorting the buildings by total energy consumption in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_energy_data\",\"query\":\"index < 5\"},\"output\":\"top_5_buildings\",\"comment\":\"Identifying the top 5 buildings with the highest energy consumption.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/HR_database/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"Contains detailed records of all employees including their salaries, department IDs, and other personal information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_details\",\"location\":\"C:/HR_database/department_details.csv\",\"sheet_name\":null,\"label\":\"Department Details\",\"description\":\"Contains information about each department including department ID and department name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the department.\"}]},\"table_id3\":{\"name\":\"employee_attendance\",\"location\":\"C:/HR_database/employee_attendance.csv\",\"sheet_name\":null,\"label\":\"Employee Attendance\",\"description\":\"Contains attendance records of employees, not relevant for salary analysis.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"attendance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of attendance.\"}]}},\"instructions\":[\"Load the employee records table from the HR database.\",\"Filter the employee records to include only those with a salary greater than $50,000.\",\"Load the department details table from the HR database.\",\"Join the filtered employee records with the department details on the department ID.\",\"Calculate the average salary for each department.\",\"Sort the departments by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_database/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records table from the HR database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering the employee records to include only those with a salary greater than $50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR_database/department_details.csv\",\"type\":\"csv\"},\"output\":\"department_details\",\"comment\":\"Loading the department details table from the HR database.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_salary_employees\",\"department_details\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"employee_department_data\",\"comment\":\"Joining the filtered employee records with the department details on the department ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"employee_department_data\",\"groupBy\":\"department_id\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"department_salary_averages\",\"comment\":\"Calculating the average salary for each department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"department_salary_averages\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_department_salaries\",\"comment\":\"Sorting the departments by average salary in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data with columns for date, usage, and other relevant metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed on the given date.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data with columns for date, temperature, and other weather metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature recorded on the given date.\"}]},\"table_id3\":{\"name\":\"historical_energy_prices\",\"location\":\"C:/data/historical_energy_prices.csv\",\"sheet_name\":null,\"label\":\"Historical Energy Prices\",\"description\":\"This table contains historical energy prices data, which is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the price record.\"},{\"column\":\"col_2\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The energy price on the given date.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Load the weather data from the Excel file located at C:/data/weather_data.xlsx.\",\"Filter the energy consumption data to include only records from the year 2023.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column in the joined table to determine the energy efficiency by dividing energy usage by temperature.\",\"Aggregate the data by month to find the total energy consumption and average temperature.\",\"Sort the aggregated data by total energy consumption in descending order.\",\"Forecast the energy consumption for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date`.dt.year == 2023\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering energy consumption data to include only records from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered energy consumption data with weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`usage` / `temperature`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating energy efficiency by dividing energy usage by temperature.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date.dt.to_period('M')\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating data by month to find total energy consumption and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total energy consumption in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"energy_consumption\",\"columns\":[\"usage\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting energy consumption for the next 30 days using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials\",\"description\":\"This table contains information about raw materials used in the manufacturing process, including stock levels and unit prices.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"stock_level\",\"column_name\":\"STOCK_LEVEL\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the material.\"},{\"column\":\"unit_price\",\"column_name\":\"UNIT_PRICE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price per unit of the material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"This table details the production timelines and tasks, including material requirements.\",\"columns\":[{\"column\":\"task_id\",\"column_name\":\"TASK_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each production task.\"},{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material required for the task.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material required for the task.\"},{\"column\":\"production_line\",\"column_name\":\"PRODUCTION_LINE\",\"column_type\":\"xsd:string\",\"column_description\":\"The production line where the task is scheduled.\"}]},\"table_id3\":{\"name\":\"employee_data\",\"location\":\"C:/data/manufacturing/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"This table contains information about employees working in the manufacturing plant.\",\"columns\":[{\"column\":\"employee_id\",\"column_name\":\"EMPLOYEE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"name\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"role\",\"column_name\":\"ROLE\",\"column_type\":\"xsd:string\",\"column_description\":\"Role of the employee in the manufacturing plant.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains information about the materials used in manufacturing.\",\"Load the table 'production_schedule' which details the production timelines and tasks.\",\"Filter the 'raw_materials' table to include only materials with a stock level greater than 100.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Calculate a new column 'total_cost' in the joined table by multiplying 'quantity' by 'unit_price'.\",\"Aggregate the joined table by 'production_line' to find the total cost and total quantity of materials used.\",\"Sort the aggregated data by 'total_cost' in descending order.\",\"Apply a transformation to normalize the 'total_cost' column in the sorted table.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"STOCK_LEVEL > 100\"},\"output\":\"filtered_raw_materials\",\"comment\":\"Filtering raw materials to include only those with stock levels greater than 100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_raw_materials\",\"production_schedule\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_materials_schedule\",\"comment\":\"Joining filtered raw materials with the production schedule on material_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_materials_schedule\",\"columnName\":\"total_cost\",\"formula\":\"QUANTITY * UNIT_PRICE\"},\"output\":\"materials_with_cost\",\"comment\":\"Calculating total cost by multiplying quantity by unit price.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"materials_with_cost\",\"groupBy\":\"PRODUCTION_LINE\",\"aggregations\":[{\"column\":\"total_cost\",\"name\":\"total_cost\",\"function\":\"sum\"},{\"column\":\"QUANTITY\",\"name\":\"total_quantity\",\"function\":\"sum\"}]},\"output\":\"aggregated_materials\",\"comment\":\"Aggregating data by production line to find total cost and quantity of materials used.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_materials\",\"sortBy\":\"total_cost\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_materials\",\"comment\":\"Sorting aggregated data by total cost in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_aggregated_materials\",\"transformations\":[{\"columns\":[\"total_cost\"],\"operation\":\"normalize\"}]},\"output\":\"normalized_materials\",\"comment\":\"Normalizing the total cost column in the sorted table.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'set_index' operation be used?","output":"Use 'set_index' when you need to set a specific column (must be datetime) as the index for time series operations."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/healthcare_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"A table containing patient information including demographics and medical diagnoses.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_visits\",\"location\":\"C:/healthcare_data/hospital_visits.csv\",\"sheet_name\":null,\"label\":\"Hospital Visits\",\"description\":\"A table detailing patient visits to the hospital, including dates and reasons for visits.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"visit_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital visit.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the visit to a specific patient.\"},{\"column\":\"col_3\",\"column_name\":\"visit_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the hospital visit.\"}]}},\"instructions\":[\"Load the patient records table from the healthcare database.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Aggregate the filtered records to calculate the average age of diabetic patients.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetic_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with diabetes.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"diabetic_patients\",\"groupBy\":null,\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"average_age_diabetic_patients\",\"comment\":\"Calculating the average age of patients diagnosed with diabetes.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'calculateDerivedColumn' action.","output":"{ \"name\": \"calculateDerivedColumn\", \"parameters\": { \"table\": \"sales\", \"columnName\": \"profit_margin\", \"formula\": \"(revenue - cost) / revenue\" }, \"output\": \"sales_with_margin\" }"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'expanding_sum' operation be used?","output":"Use 'expanding_sum' when you need to compute a cumulative sum over time, such as tracking cumulative sales revenue."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid column name usage with double quotes.","output":"'\"a column name\" + col2' // Error: Column names must be enclosed in backticks, not double quotes."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat parameters are required for the 'aggregateData' action?","output":"'aggregateData' requires a 'table' (name of the input table), 'groupBy' (columns used for grouping), and 'aggregations' (list of aggregation instructions, specifying the column, output name, and function)."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'rolling_skew' operation be used?","output":"Use 'rolling_skew' when you need to measure skewness in a dataset over a moving window, which helps analyze asymmetry in financial or statistical time series."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if the dataset has fewer than '2 \u00c3\u2014 forecastPeriod' observations?","output":"If the dataset has fewer than '2 \u00c3\u2014 forecastPeriod' observations, an error is raised to ensure sufficient data is available for accurate forecasting."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_details\",\"location\":\"C:/legal_data/case_details.csv\",\"sheet_name\":null,\"label\":\"Case Details\",\"description\":\"A table containing detailed information about various legal cases, including start and end dates.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"column_2_id\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case started.\"},{\"column\":\"column_3_id\",\"column_name\":\"end_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case ended.\"},{\"column\":\"column_4_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"}]},\"table_id2\":{\"name\":\"lawyer_info\",\"location\":\"C:/legal_data/lawyer_info.csv\",\"sheet_name\":null,\"label\":\"Lawyer Information\",\"description\":\"A table containing information about lawyers, including their IDs and names.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedule\",\"location\":\"C:/legal_data/court_schedule.csv\",\"sheet_name\":null,\"label\":\"Court Schedule\",\"description\":\"A table containing court schedules, not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"court_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court.\"},{\"column\":\"column_2_id\",\"column_name\":\"schedule_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court schedule.\"}]}},\"instructions\":[\"Load the table 'case_details' which contains information about various legal cases.\",\"Load the table 'lawyer_info' which contains details about lawyers involved in the cases.\",\"Filter the 'case_details' table to include only cases filed after January 1, 2020.\",\"Join the filtered 'case_details' table with 'lawyer_info' on the lawyer_id column.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\",\"Sort the aggregated data by the total number of cases in descending order.\",\"Calculate a new column 'average_case_duration' in the 'case_details' table by subtracting 'start_date' from 'end_date'.\",\"Filter the 'case_details' table to include only cases with an average duration greater than 30 days.\",\"Join the filtered 'case_details' table with 'lawyer_info' to get details of lawyers handling long-duration cases.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_details.csv\",\"type\":\"csv\"},\"output\":\"case_details\",\"comment\":\"Loading the case details dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyer_info.csv\",\"type\":\"csv\"},\"output\":\"lawyer_info\",\"comment\":\"Loading the lawyer information dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_details\",\"query\":\"`start_date` > '2020-01-01'\"},\"output\":\"filtered_case_details\",\"comment\":\"Filtering cases to include only those filed after January 1, 2020.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_case_details\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_case_lawyer\",\"comment\":\"Joining filtered case details with lawyer information on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_case_lawyer\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_count\",\"comment\":\"Aggregating to find the total number of cases handled by each lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"lawyer_case_count\",\"sortBy\":\"total_cases\",\"order\":\"desc\"},\"output\":\"sorted_lawyer_case_count\",\"comment\":\"Sorting lawyers by the total number of cases in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"case_details\",\"columnName\":\"average_case_duration\",\"formula\":\"`end_date` - `start_date`\"},\"output\":\"case_details_with_duration\",\"comment\":\"Calculating average case duration by subtracting start date from end date.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_details_with_duration\",\"query\":\"`average_case_duration` > 30\"},\"output\":\"long_duration_cases\",\"comment\":\"Filtering cases to include only those with an average duration greater than 30 days.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"long_duration_cases\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"long_duration_cases_with_lawyers\",\"comment\":\"Joining long-duration cases with lawyer information to get details of lawyers handling these cases.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transaction records with details such as account ID, transaction amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"account_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each account.\"},{\"column\":\"col_2\",\"column_name\":\"transaction_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"account_details\",\"location\":\"C:/data/account_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Account Details\",\"description\":\"A table containing details of accounts such as account holder name and account type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"account_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each account.\"},{\"column\":\"col_2\",\"column_name\":\"account_holder\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the account holder.\"},{\"column\":\"col_3\",\"column_name\":\"account_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of the account, e.g., savings, checking.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at C:/data/financial_transactions.csv.\",\"Filter the 'financial_transactions' table to include only rows where the transaction amount is greater than 1000.\",\"Aggregate the filtered data by 'account_id' to calculate the total transaction amount for each account.\",\"Sort the aggregated data by total transaction amount in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"transaction_amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_transactions\",\"groupBy\":\"account_id\",\"aggregations\":[{\"column\":\"transaction_amount\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"}]},\"output\":\"aggregated_transactions\",\"comment\":\"Aggregating the filtered transactions by account ID to calculate total transaction amount.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_transactions\",\"sortBy\":\"total_transaction_amount\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_transactions\",\"comment\":\"Sorting the aggregated transactions by total transaction amount in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing all financial transactions with details such as transaction ID, customer ID, amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"customer_info\",\"location\":\"C:/data/customer_info.csv\",\"sheet_name\":null,\"label\":\"Customer Information\",\"description\":\"A table containing customer details such as customer ID, name, and contact information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_number\",\"column_type\":\"xsd:string\",\"column_description\":\"The contact number of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"A table with data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Join the filtered transactions with the 'customer_info' table on the 'customer_id' column.\",\"Aggregate the joined data to calculate the total transaction amount per customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_info.csv\",\"type\":\"csv\"},\"output\":\"customer_info\",\"comment\":\"Loading the customer information dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_info\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer information based on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"}]},\"output\":\"customer_transaction_totals\",\"comment\":\"Aggregating data to calculate total transaction amount per customer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_details\",\"location\":\"C:/legal_data/case_details.csv\",\"sheet_name\":null,\"label\":\"Legal Case Details\",\"description\":\"A table containing detailed information about various legal cases, including case ID, lawyer ID, and case date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each legal case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"case_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyer_info\",\"location\":\"C:/legal_data/lawyer_info.csv\",\"sheet_name\":null,\"label\":\"Lawyer Information\",\"description\":\"A table containing information about lawyers, including lawyer ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedule\",\"location\":\"C:/legal_data/court_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Court Schedule\",\"description\":\"A table containing the schedule of court hearings, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"hearing_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court hearing.\"},{\"column\":\"col_2\",\"column_name\":\"hearing_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the court hearing.\"}]}},\"instructions\":[\"Load the table 'case_details' which contains information about legal cases.\",\"Filter the 'case_details' table to only include cases from the year 2023.\",\"Load the table 'lawyer_info' which contains details about lawyers.\",\"Join the filtered 'case_details' table with the 'lawyer_info' table on the 'lawyer_id' column.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_details.csv\",\"type\":\"csv\"},\"output\":\"case_details\",\"comment\":\"Loading the table containing details of legal cases.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_details\",\"query\":\"`case_date`.year == 2023\"},\"output\":\"filtered_case_details\",\"comment\":\"Filtering cases to include only those from the year 2023.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyer_info.csv\",\"type\":\"csv\"},\"output\":\"lawyer_info\",\"comment\":\"Loading the table containing information about lawyers.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_case_details\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered case details with lawyer information on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_table\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_count\",\"comment\":\"Aggregating to find the total number of cases handled by each lawyer.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhy is it recommended to provide a unique name for aggregation results?","output":"A unique name avoids column name conflicts, making the workflow more readable and reducing errors in later steps."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_usage\",\"location\":\"C:/data/building_management/energy_usage.csv\",\"sheet_name\":null,\"label\":\"Energy Usage Data\",\"description\":\"This table contains daily energy consumption data for the building, including date and consumption in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/building_management/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for building equipment.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"equipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for each piece of equipment.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"}]}},\"instructions\":[\"Load the table 'energy_usage' which contains daily energy consumption data for the building.\",\"Filter the energy usage data to include only entries where the consumption is greater than 500 kWh.\",\"Aggregate the filtered data by month to calculate the total energy consumption for each month.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_management/energy_usage.csv\",\"type\":\"csv\"},\"output\":\"energy_usage\",\"comment\":\"Loading the energy usage data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_usage\",\"query\":\"consumption_kWh > 500\"},\"output\":\"high_energy_usage\",\"comment\":\"Filtering the energy usage data to include only entries with consumption greater than 500 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"high_energy_usage\",\"groupBy\":\"date\",\"aggregations\":[{\"column\":\"consumption_kWh\",\"name\":\"total_monthly_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Aggregating the filtered data by month to calculate the total energy consumption for each month.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"This table contains records of financial transactions including transaction ID, customer ID, and amount.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The monetary amount of the transaction.\"}]},\"table_id2\":{\"name\":\"customer_info\",\"location\":\"C:/data/customer_info.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Information\",\"description\":\"This table contains customer details including customer ID, name, and contact information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_info\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact information of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Load the 'customer_info' table from the Excel file.\",\"Join the filtered transactions with customer information on the customer ID.\",\"Aggregate the joined data to calculate the total transaction amount per customer.\",\"Sort the aggregated data by total transaction amount in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_info.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_info\",\"comment\":\"Loading the customer information dataset from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_info\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer information on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate the total transaction amount per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_transaction_amount\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total transaction amount in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid dictionary literal.","output":"'{'a': 1, 'b': 2}' // Error: Dictionary literals are not allowed in Tamarind expressions."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Table containing details of shipments including product IDs, quantities, and shipment status.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product being shipped.\"},{\"column\":\"col_3\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the product in the shipment.\"},{\"column\":\"col_4\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_inventory\",\"location\":\"C:/data/logistics/warehouse_inventory.xlsx\",\"sheet_name\":\"Inventory\",\"label\":\"Warehouse Inventory\",\"description\":\"Table containing current stock levels for each product in the warehouse.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product in stock.\"},{\"column\":\"col_2\",\"column_name\":\"stock_available\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current available stock for the product.\"}]},\"table_id3\":{\"name\":\"transport_routes\",\"location\":\"C:/data/logistics/transport_routes.csv\",\"sheet_name\":null,\"label\":\"Transport Routes\",\"description\":\"Table containing information about transport routes and their capacities.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"route_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transport route.\"},{\"column\":\"col_2\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Maximum capacity of the transport route.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at 'C:/data/logistics/shipment_data.csv'.\",\"Load the 'warehouse_inventory' table from the Excel file located at 'C:/data/logistics/warehouse_inventory.xlsx'.\",\"Filter the 'shipment_data' table to include only shipments with a status of 'Pending'.\",\"Join the filtered 'shipment_data' with 'warehouse_inventory' on the 'product_id' column using an inner join.\",\"Calculate a new column 'inventory_shortage' in the joined table by subtracting 'quantity' from 'stock_available'.\",\"Filter the joined table to include only rows where 'inventory_shortage' is greater than zero.\",\"Sort the resulting table by 'inventory_shortage' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouse_inventory.xlsx\",\"type\":\"xlsx\"},\"output\":\"warehouse_inventory\",\"comment\":\"Loading the warehouse inventory data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"status == 'Pending'\"},\"output\":\"pending_shipments\",\"comment\":\"Filtering shipments to include only those with a status of 'Pending'.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"pending_shipments\",\"warehouse_inventory\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining pending shipments with warehouse inventory on product_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"inventory_shortage\",\"formula\":\"quantity - stock_available\"},\"output\":\"joined_data_with_shortage\",\"comment\":\"Calculating inventory shortage by subtracting stock available from quantity.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"joined_data_with_shortage\",\"query\":\"inventory_shortage > 0\"},\"output\":\"shortage_data\",\"comment\":\"Filtering data to include only rows where inventory shortage is greater than zero.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"shortage_data\",\"sortBy\":\"inventory_shortage\",\"order\":\"desc\"},\"output\":\"sorted_shortage_data\",\"comment\":\"Sorting the data by inventory shortage in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"A table containing product specifications including product ID, name, and development status.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"development_status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current development status of the product.\"}]},\"table_id2\":{\"name\":\"team_assignments\",\"location\":\"C:/data/team_assignments.csv\",\"sheet_name\":null,\"label\":\"Team Assignments\",\"description\":\"A table containing team assignments including product ID and team member ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"team_member_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each team member.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"A table that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only products with a development status of 'In Progress'.\",\"Join the filtered product specifications with the team assignments table on the product ID.\",\"Aggregate the joined table to calculate the total number of team members assigned to each product.\",\"Sort the aggregated data by the total number of team members in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"development_status == 'In Progress'\"},\"output\":\"in_progress_products\",\"comment\":\"Filtering products to include only those with a development status of 'In Progress'.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/team_assignments.csv\",\"type\":\"csv\"},\"output\":\"team_assignments\",\"comment\":\"Loading the team assignments table from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"in_progress_products\",\"team_assignments\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_product_team\",\"comment\":\"Joining filtered product specifications with team assignments on product ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_product_team\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"team_member_id\",\"name\":\"total_team_members\",\"function\":\"count\"}]},\"output\":\"product_team_count\",\"comment\":\"Aggregating to calculate the total number of team members assigned to each product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"product_team_count\",\"sortBy\":\"total_team_members\",\"order\":\"desc\"},\"output\":\"sorted_product_team_count\",\"comment\":\"Sorting the aggregated data by the total number of team members in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"legal_cases\",\"location\":\"C:/data/legal_cases.csv\",\"sheet_name\":null,\"label\":\"Legal Cases Table\",\"description\":\"A table containing details of legal cases, including case_id, lawyer_id, and filing_date.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each legal case.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"column_3_id\",\"column_name\":\"filing_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/data/lawyers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Lawyers Table\",\"description\":\"A table containing details of lawyers, including lawyer_id and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_decisions\",\"location\":\"C:/data/court_decisions.csv\",\"sheet_name\":null,\"label\":\"Court Decisions Table\",\"description\":\"A table containing court decisions, not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"decision_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court decision.\"},{\"column\":\"column_2_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the related legal case.\"},{\"column\":\"column_3_id\",\"column_name\":\"decision_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the decision was made.\"}]}},\"instructions\":[\"Load the legal cases table from the CSV file located at C:/data/legal_cases.csv.\",\"Load the lawyers table from the Excel file located at C:/data/lawyers.xlsx.\",\"Filter the legal cases to include only those filed after January 1, 2020.\",\"Join the filtered legal cases with the lawyers table on the lawyer_id column.\",\"Aggregate the joined data to count the number of cases per lawyer.\",\"Sort the aggregated data by the number of cases in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal_cases.csv\",\"type\":\"csv\"},\"output\":\"legal_cases\",\"comment\":\"Loading the legal cases dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/lawyers.xlsx\",\"type\":\"xlsx\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"legal_cases\",\"query\":\"`filing_date` > '2020-01-01'\"},\"output\":\"filtered_legal_cases\",\"comment\":\"Filtering legal cases to include only those filed after January 1, 2020.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_legal_cases\",\"lawyers\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"cases_with_lawyers\",\"comment\":\"Joining filtered legal cases with lawyers data on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"cases_with_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"case_count\",\"function\":\"count\"}]},\"output\":\"cases_per_lawyer\",\"comment\":\"Aggregating data to count the number of cases per lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"cases_per_lawyer\",\"sortBy\":\"case_count\",\"order\":\"desc\"},\"output\":\"sorted_cases_per_lawyer\",\"comment\":\"Sorting the aggregated data by the number of cases in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.csv\",\"sheet_name\":null,\"label\":\"Department Data\",\"description\":\"Contains information about departments including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the department.\"}]},\"table_id3\":{\"name\":\"project_data\",\"location\":\"C:/data/project_data.csv\",\"sheet_name\":null,\"label\":\"Project Data\",\"description\":\"Contains information about projects including project IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"project_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the project.\"}]}},\"instructions\":[\"Load the employee data table from the CSV file.\",\"Filter the employee data to include only those with a salary greater than $50,000.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Aggregate the joined data to calculate the average salary per department.\",\"Sort the aggregated data by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading the employee data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employees with a salary greater than $50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.csv\",\"type\":\"csv\"},\"output\":\"department_data\",\"comment\":\"Loading the department data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_salary_employees\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"average_salary_per_department\",\"comment\":\"Calculating the average salary per department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_salary_per_department\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_average_salary\",\"comment\":\"Sorting the aggregated data by average salary in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"building_energy_data\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and energy usage values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time when the energy consumption was recorded.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.xlsx\",\"sheet_name\":\"Info\",\"label\":\"Building Information\",\"description\":\"Contains additional information about the buildings, such as location and size.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_B\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location of the building.\"},{\"column\":\"col_C\",\"column_name\":\"size\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The size of the building in square meters.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_X\",\"column_name\":\"random_id\",\"column_type\":\"xsd:string\",\"column_description\":\"A random identifier.\"},{\"column\":\"col_Y\",\"column_name\":\"random_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"A random value.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each building.\",\"Sort the buildings by total energy consumption in descending order.\",\"Identify the top 5 buildings with the highest energy consumption.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"building_energy_data\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"building_energy_data\",\"query\":\"`timestamp`.year == 2023\"},\"output\":\"filtered_energy_data\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_data\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"total_energy_per_building\",\"comment\":\"Calculating the total energy consumption for each building.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"total_energy_per_building\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_energy_data\",\"comment\":\"Sorting the buildings by total energy consumption in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_energy_data\",\"query\":\"index < 5\"},\"output\":\"top_5_buildings\",\"comment\":\"Identifying the top 5 buildings with the highest energy consumption.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan chained comparisons be used in Tamarind expressions?","output":"Yes, chained comparisons such as 'a < b < c' are allowed."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/HR/data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"A table containing detailed records of employees including their salary, hire date, and other personal information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"hire_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date the employee was hired.\"}]},\"table_id2\":{\"name\":\"department_info\",\"location\":\"C:/HR/data/department_info.xlsx\",\"sheet_name\":\"Departments\",\"label\":\"Department Information\",\"description\":\"Contains information about various departments within the company.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_B\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]}},\"instructions\":[\"Load the employee records table from the HR database.\",\"Filter the employee records to include only those with a salary greater than $50,000.\",\"Sort the filtered employee records by their hire date in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/HR/data/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"salary > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employee records to include only those with a salary greater than $50,000.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"high_salary_employees\",\"sortBy\":\"hire_date\",\"order\":\"asc\"},\"output\":\"sorted_high_salary_employees\",\"comment\":\"Sorting the filtered employee records by their hire date in ascending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat are some of the operations that can be performed in a Tamarind workflow?","output":"Operations include filtering, joining, aggregation, transformation, and forecasting of tables."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid arithmetic expression using modulus.","output":"'col1 % 5'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customers\",\"location\":\"C:/data/marketing/customers.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"Contains information about customers including their ID, name, and purchase history.\",\"columns\":[{\"column\":\"customer_id\",\"column_name\":\"CUSTOMER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"name\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"last_purchase_date\",\"column_name\":\"LAST_PURCHASE_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the customer's last purchase.\"}]},\"table_id2\":{\"name\":\"sales\",\"location\":\"C:/data/marketing/sales.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"Contains sales transactions including customer ID and sales amount.\",\"columns\":[{\"column\":\"customer_id\",\"column_name\":\"CUSTOMER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"sales_amount\",\"column_name\":\"SALES_AMOUNT\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of sales made by the customer.\"},{\"column\":\"transaction_date\",\"column_name\":\"TRANSACTION_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the sales transaction.\"}]},\"table_id3\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing/campaigns.csv\",\"sheet_name\":null,\"label\":\"Marketing Campaigns\",\"description\":\"Details of past marketing campaigns including campaign ID and effectiveness.\",\"columns\":[{\"column\":\"campaign_id\",\"column_name\":\"CAMPAIGN_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each campaign.\"},{\"column\":\"effectiveness\",\"column_name\":\"EFFECTIVENESS\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Effectiveness score of the campaign.\"}]}},\"instructions\":[\"Load the customer data from the CSV file located at C:/data/marketing/customers.csv.\",\"Filter the customer data to include only those who have made purchases in the last year.\",\"Join the filtered customer data with the sales data on the customer ID.\",\"Aggregate the joined data to calculate the total sales per customer.\",\"Sort the aggregated data by total sales in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customers.csv\",\"type\":\"csv\"},\"output\":\"customers\",\"comment\":\"Loading customer data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customers\",\"query\":\"`LAST_PURCHASE_DATE` >= '2022-10-01'\"},\"output\":\"recent_customers\",\"comment\":\"Filtering customers who have made purchases in the last year.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/sales.csv\",\"type\":\"csv\"},\"output\":\"sales\",\"comment\":\"Loading sales data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"recent_customers\",\"sales\"],\"joinOn\":[\"CUSTOMER_ID\"],\"joinType\":\"inner\"},\"output\":\"customer_sales\",\"comment\":\"Joining filtered customer data with sales data on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"customer_sales\",\"groupBy\":\"CUSTOMER_ID\",\"aggregations\":[{\"column\":\"SALES_AMOUNT\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"total_sales_per_customer\",\"comment\":\"Aggregating data to calculate total sales per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"total_sales_per_customer\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_sales\",\"comment\":\"Sorting aggregated data by total sales in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid list comprehension.","output":"'[x for x in col1]' // Error: List comprehensions are not allowed in Tamarind."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_records\",\"location\":\"C:/legal_data/case_records.csv\",\"sheet_name\":null,\"label\":\"Case Records\",\"description\":\"A table containing records of legal cases, including details such as case ID, status, and assigned lawyer.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the case, e.g., Open, Closed.\"},{\"column\":\"col_3\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer assigned to the case.\"}]},\"table_id2\":{\"name\":\"lawyer_details\",\"location\":\"C:/legal_data/lawyer_details.csv\",\"sheet_name\":null,\"label\":\"Lawyer Details\",\"description\":\"A table containing details of lawyers, including their ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedules\",\"location\":\"C:/legal_data/court_schedules.csv\",\"sheet_name\":null,\"label\":\"Court Schedules\",\"description\":\"A table containing court schedules, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"schedule_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court schedule.\"},{\"column\":\"col_2\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court schedule.\"}]}},\"instructions\":[\"Load the table 'case_records' from the CSV file located at C:/legal_data/case_records.csv.\",\"Filter the 'case_records' table to include only cases where the status is 'Open'.\",\"Join the filtered 'case_records' with the 'lawyer_details' table on the 'lawyer_id' column.\",\"Aggregate the joined table to count the number of open cases per lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_records.csv\",\"type\":\"csv\"},\"output\":\"case_records\",\"comment\":\"Loading the case records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_records\",\"query\":\"status == 'Open'\"},\"output\":\"open_cases\",\"comment\":\"Filtering the case records to include only open cases.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyer_details.csv\",\"type\":\"csv\"},\"output\":\"lawyer_details\",\"comment\":\"Loading the lawyer details dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"open_cases\",\"lawyer_details\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"open_cases_with_lawyers\",\"comment\":\"Joining open cases with lawyer details on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"open_cases_with_lawyers\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"open_case_count\",\"function\":\"count\"}]},\"output\":\"open_cases_per_lawyer\",\"comment\":\"Aggregating the data to count the number of open cases per lawyer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of shipments including shipment ID, weight, and destination.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"Destination of the shipment.\"}]},\"table_id2\":{\"name\":\"delivery_schedule\",\"location\":\"C:/logistics/delivery_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Delivery Schedule\",\"description\":\"Contains the delivery schedule for shipments including shipment ID and delivery route.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"delivery_route\",\"column_type\":\"xsd:string\",\"column_description\":\"Route assigned for the delivery.\"}]},\"table_id3\":{\"name\":\"warehouse_inventory\",\"location\":\"C:/logistics/warehouse_inventory.csv\",\"sheet_name\":null,\"label\":\"Warehouse Inventory\",\"description\":\"Contains inventory details of the warehouse, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"col_2\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the item in stock.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file.\",\"Load the 'delivery_schedule' table from the Excel file.\",\"Filter the 'shipment_data' to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered shipment data with the delivery schedule on the 'shipment_id' column.\",\"Aggregate the joined data to calculate the total weight of shipments per delivery route.\",\"Sort the aggregated data by total weight in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/logistics/delivery_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"delivery_schedule\",\"comment\":\"Loading delivery schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"filtered_shipments\",\"comment\":\"Filtering shipments to include only those with weight greater than 1000 kg.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipments\",\"delivery_schedule\"],\"joinOn\":[\"shipment_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipments with delivery schedule on shipment_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"delivery_route\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total weight per delivery route.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_weight\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total weight in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"This table contains details of all shipments including shipment ID, weight, and vehicle ID.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"column_2_id\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"column_3_id\",\"column_name\":\"vehicle_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the vehicle used for the shipment.\"}]},\"table_id2\":{\"name\":\"vehicle_data\",\"location\":\"C:/data/logistics/vehicle_data.csv\",\"sheet_name\":null,\"label\":\"Vehicle Data\",\"description\":\"This table contains details about vehicles including vehicle ID and type.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"vehicle_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each vehicle.\"},{\"column\":\"column_2_id\",\"column_name\":\"vehicle_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of the vehicle.\"}]},\"table_id3\":{\"name\":\"driver_data\",\"location\":\"C:/data/logistics/driver_data.csv\",\"sheet_name\":null,\"label\":\"Driver Data\",\"description\":\"This table contains information about drivers including driver ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"driver_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each driver.\"},{\"column\":\"column_2_id\",\"column_name\":\"driver_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the driver.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of all shipments.\",\"Filter the shipment data to include only shipments with a weight greater than 500 kg.\",\"Join the filtered shipment data with the 'vehicle_data' table on the vehicle_id column.\",\"Aggregate the joined data to calculate the total shipment weight for each vehicle type.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 500\"},\"output\":\"filtered_shipment_data\",\"comment\":\"Filtering shipments to include only those with weight greater than 500 kg.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/vehicle_data.csv\",\"type\":\"csv\"},\"output\":\"vehicle_data\",\"comment\":\"Loading the vehicle data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipment_data\",\"vehicle_data\"],\"joinOn\":[\"vehicle_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with vehicle data on vehicle_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"vehicle_type\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_shipment_weight\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total shipment weight for each vehicle type.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"This table contains customer feedback data including satisfaction scores and regions.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"satisfaction_score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Score indicating customer satisfaction, ranging from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Details\",\"label\":\"Customer Details\",\"description\":\"Contains detailed information about customers, such as contact information and demographics.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_B\",\"column_name\":\"email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file located in the specified directory.\",\"Filter the feedback data to include only entries with a satisfaction score below 3.\",\"Aggregate the filtered data to count the number of unsatisfied customers per region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"satisfaction_score < 3\"},\"output\":\"unsatisfied_customers\",\"comment\":\"Filtering feedback data to include only entries with a satisfaction score below 3.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"unsatisfied_customers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"customer_id\",\"name\":\"unsatisfied_count\",\"function\":\"count\"}]},\"output\":\"unsatisfied_customers_by_region\",\"comment\":\"Aggregating data to count the number of unsatisfied customers per region.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Weekly Production Schedule\",\"description\":\"This table contains the manufacturing schedule for the week, including task details and scheduled dates.\",\"columns\":[{\"column\":\"task_id\",\"column_name\":\"TASK_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each manufacturing task.\"},{\"column\":\"scheduled_date\",\"column_name\":\"SCHEDULED_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the task is scheduled to be performed.\"},{\"column\":\"machine_id\",\"column_name\":\"MACHINE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the machine required for the task.\"},{\"column\":\"production_time\",\"column_name\":\"PRODUCTION_TIME\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Estimated time required to complete the task in hours.\"}]},\"table_id2\":{\"name\":\"machine_availability\",\"location\":\"C:/data/manufacturing/machine_availability.csv\",\"sheet_name\":null,\"label\":\"Machine Availability\",\"description\":\"This table contains information about machine availability for the week.\",\"columns\":[{\"column\":\"machine_id\",\"column_name\":\"MACHINE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the machine.\"},{\"column\":\"available_date\",\"column_name\":\"AVAILABLE_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the machine is available for use.\"}]},\"table_id3\":{\"name\":\"employee_schedule\",\"location\":\"C:/data/manufacturing/employee_schedule.csv\",\"sheet_name\":null,\"label\":\"Employee Schedule\",\"description\":\"This table contains the work schedule for employees, including shifts and assigned tasks.\",\"columns\":[{\"column\":\"employee_id\",\"column_name\":\"EMPLOYEE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"shift_date\",\"column_name\":\"SHIFT_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the employee's shift.\"},{\"column\":\"task_id\",\"column_name\":\"TASK_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the task assigned to the employee.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the manufacturing schedule for the week.\",\"Filter the production schedule to include only the tasks scheduled for today.\",\"Join the filtered production schedule with the 'machine_availability' table to ensure machines are available for today's tasks.\",\"Aggregate the joined table to calculate the total production time required for each machine.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the weekly production schedule from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"SCHEDULED_DATE == '2023-10-15'\"},\"output\":\"today_schedule\",\"comment\":\"Filtering the production schedule to include only tasks scheduled for today.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/machine_availability.csv\",\"type\":\"csv\"},\"output\":\"machine_availability\",\"comment\":\"Loading the machine availability data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"today_schedule\",\"machine_availability\"],\"joinOn\":[\"MACHINE_ID\"],\"joinType\":\"inner\"},\"output\":\"available_tasks\",\"comment\":\"Joining today's schedule with machine availability to ensure machines are available for today's tasks.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"available_tasks\",\"groupBy\":\"MACHINE_ID\",\"aggregations\":[{\"column\":\"PRODUCTION_TIME\",\"name\":\"total_production_time\",\"function\":\"sum\"}]},\"output\":\"machine_production_time\",\"comment\":\"Aggregating the joined table to calculate the total production time required for each machine.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information of customers including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"marketing_campaign_results\",\"location\":\"C:/data/marketing_campaigns.xlsx\",\"sheet_name\":\"Results\",\"label\":\"Marketing Campaign Results\",\"description\":\"Results of various marketing campaigns including success metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"success\",\"column_type\":\"xsd:boolean\",\"column_description\":\"Indicates if the campaign was successful.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Irrelevant Data\",\"description\":\"Contains unrelated data not used in the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the data entry.\"},{\"column\":\"col_2\",\"column_name\":\"value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Some value associated with the data entry.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the marketing campaign results table from the Excel file.\",\"Filter the marketing campaign results to include only successful campaigns.\",\"Join the filtered campaign results with customer demographics on customer ID.\",\"Aggregate the joined data to find the total number of successful campaigns per age group.\",\"Sort the aggregated data by the total number of successful campaigns in descending order.\",\"Calculate the percentage of successful campaigns for each age group.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing_campaigns.xlsx\",\"type\":\"xlsx\"},\"output\":\"marketing_campaign_results\",\"comment\":\"Loading marketing campaign results from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"marketing_campaign_results\",\"query\":\"success == True\"},\"output\":\"successful_campaigns\",\"comment\":\"Filtering to include only successful marketing campaigns.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"successful_campaigns\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining successful campaigns with customer demographics on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"age\",\"aggregations\":[{\"column\":\"campaign_id\",\"name\":\"total_successful_campaigns\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find the total number of successful campaigns per age group.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_successful_campaigns\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by the total number of successful campaigns in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_data\",\"columnName\":\"success_percentage\",\"formula\":\"total_successful_campaigns / sum(total_successful_campaigns) * 100\"},\"output\":\"final_data\",\"comment\":\"Calculating the percentage of successful campaigns for each age group.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid expression using logarithmic and root functions.","output":"'log(col1) + sqrt(col2)'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees, including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Department Data\",\"description\":\"Contains information about departments, including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"project_data\",\"location\":\"C:/data/project_data.xlsx\",\"sheet_name\":\"Projects\",\"label\":\"Project Data\",\"description\":\"Contains information about projects, including project IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"project_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the project.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/employee_data.csv.\",\"Load the department data from the Excel file located at C:/data/department_data.xlsx.\",\"Filter the employee data to include only those with a salary greater than 50,000.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Calculate the average salary for each department.\",\"Sort the departments by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading employee data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"department_data\",\"comment\":\"Loading department data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employees with a salary greater than 50,000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employee_data\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"employee_department_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"employee_department_data\",\"groupBy\":\"department_id\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"department_salary_averages\",\"comment\":\"Calculating the average salary for each department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"department_salary_averages\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_department_salaries\",\"comment\":\"Sorting departments by average salary in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and energy usage in kilowatt-hours.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Timestamp of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Energy consumption in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.xlsx\",\"sheet_name\":\"Info\",\"label\":\"Building Information\",\"description\":\"This table contains information about the buildings, such as their location and type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the building.\"},{\"column\":\"col_3\",\"column_name\":\"building_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of the building (e.g., residential, commercial).\"}]},\"table_id3\":{\"name\":\"maintenance_records\",\"location\":\"C:/data/maintenance_records.csv\",\"sheet_name\":null,\"label\":\"Maintenance Records\",\"description\":\"Records of maintenance activities for the buildings, including dates and types of maintenance.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Date of the maintenance activity.\"},{\"column\":\"col_3\",\"column_name\":\"maintenance_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of maintenance performed.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each building.\",\"Sort the buildings by total energy consumption in descending order.\",\"Identify the top 5 buildings with the highest energy consumption.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"filtered_energy_2023\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_2023\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_usage_kwh\",\"name\":\"total_energy\",\"function\":\"sum\"}]},\"output\":\"total_energy_per_building\",\"comment\":\"Calculating the total energy consumption for each building.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"total_energy_per_building\",\"sortBy\":\"total_energy\",\"order\":\"desc\"},\"output\":\"sorted_energy_consumption\",\"comment\":\"Sorting the buildings by total energy consumption in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_energy_consumption\",\"query\":\"index < 5\"},\"output\":\"top_5_buildings\",\"comment\":\"Identifying the top 5 buildings with the highest energy consumption.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_details\",\"location\":\"C:/legal_data/case_details.csv\",\"sheet_name\":null,\"label\":\"Case Details\",\"description\":\"Contains details of legal cases including case ID, lawyer ID, and case year.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"case_year\",\"column_type\":\"xsd:integer\",\"column_description\":\"Year the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyer_info\",\"location\":\"C:/legal_data/lawyer_info.csv\",\"sheet_name\":null,\"label\":\"Lawyer Information\",\"description\":\"Contains information about lawyers including lawyer ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedule\",\"location\":\"C:/legal_data/court_schedule.csv\",\"sheet_name\":null,\"label\":\"Court Schedule\",\"description\":\"Contains the schedule of court hearings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"hearing_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hearing.\"},{\"column\":\"col_2\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the court hearing.\"}]}},\"instructions\":[\"Load the 'case_details' table from the legal database.\",\"Filter the 'case_details' table to include only cases from the year 2023.\",\"Load the 'lawyer_info' table from the legal database.\",\"Join the filtered 'case_details' table with the 'lawyer_info' table on 'lawyer_id'.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_details.csv\",\"type\":\"csv\"},\"output\":\"case_details\",\"comment\":\"Loading the case details dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_details\",\"query\":\"case_year == 2023\"},\"output\":\"filtered_case_details\",\"comment\":\"Filtering cases to include only those from the year 2023.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyer_info.csv\",\"type\":\"csv\"},\"output\":\"lawyer_info\",\"comment\":\"Loading the lawyer information dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_case_details\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered case details with lawyer information on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_table\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_count\",\"comment\":\"Aggregating to find the total number of cases handled by each lawyer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of products including priority levels.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"priority\",\"column_type\":\"xsd:string\",\"column_description\":\"Priority level of the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost associated with the product.\"}]},\"table_id2\":{\"name\":\"supplier_information\",\"location\":\"C:/data/supplier_information.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains information about suppliers including their IDs.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"},{\"column\":\"column_3_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Product ID associated with the supplier.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant data column.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a high priority.\",\"Join the filtered product specifications with the supplier information table.\",\"Aggregate the joined data to calculate the total cost per supplier.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"priority == 'high'\"},\"output\":\"high_priority_products\",\"comment\":\"Filtering products to include only those with high priority.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_information.csv\",\"type\":\"csv\"},\"output\":\"supplier_information\",\"comment\":\"Loading supplier information from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_priority_products\",\"supplier_information\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_product_supplier\",\"comment\":\"Joining high priority products with supplier information based on product_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_product_supplier\",\"groupBy\":\"supplier_id\",\"aggregations\":[{\"column\":\"cost\",\"name\":\"total_cost\",\"function\":\"sum\"}]},\"output\":\"total_cost_per_supplier\",\"comment\":\"Aggregating data to calculate total cost per supplier.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Contains information about employees, including their salaries and department IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the department the employee belongs to.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.csv\",\"sheet_name\":null,\"label\":\"Department Data\",\"description\":\"Contains information about departments, including department IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]},\"table_id3\":{\"name\":\"project_data\",\"location\":\"C:/data/project_data.csv\",\"sheet_name\":null,\"label\":\"Project Data\",\"description\":\"Contains information about projects, including project IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"project_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the project.\"}]}},\"instructions\":[\"Load the employee data from the CSV file located at C:/data/employee_data.csv.\",\"Filter the employee data to include only those with a salary greater than 50,000.\",\"Join the filtered employee data with the department data on the department_id column.\",\"Aggregate the joined data to calculate the average salary per department.\",\"Sort the aggregated data by average salary in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading the employee data from the CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employees with a salary greater than 50,000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/department_data.csv\",\"type\":\"csv\"},\"output\":\"department_data\",\"comment\":\"Loading the department data from the CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_employee_data\",\"department_data\"],\"joinOn\":[\"department_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered employee data with department data on department_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"department_name\",\"aggregations\":[{\"column\":\"salary\",\"name\":\"average_salary\",\"function\":\"mean\"}]},\"output\":\"average_salary_per_department\",\"comment\":\"Calculating the average salary per department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_salary_per_department\",\"sortBy\":\"average_salary\",\"order\":\"desc\"},\"output\":\"sorted_average_salary_per_department\",\"comment\":\"Sorting the aggregated data by average salary in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid expression using trigonometric functions.","output":"'sin(col1) + cos(col2)'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/healthcare_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains detailed records of patients including demographics and diagnoses.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"medication\",\"location\":\"C:/healthcare_data/medication.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Details of medications prescribed to patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"}]},\"table_id3\":{\"name\":\"hospital_visits\",\"location\":\"C:/healthcare_data/hospital_visits.csv\",\"sheet_name\":null,\"label\":\"Hospital Visits\",\"description\":\"Records of hospital visits by patients, not relevant for this workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"visit_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital visit.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_3\",\"column_name\":\"visit_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the hospital visit.\"}]}},\"instructions\":[\"Load the patient records table from the healthcare database.\",\"Filter the records to include only patients diagnosed with diabetes.\",\"Join the filtered patient records with the medication table to get prescribed medications.\",\"Aggregate the data to find the average age of diabetic patients for each medication.\",\"Sort the aggregated data by average age in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from the healthcare database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetic_patients\",\"comment\":\"Filtering records to include only patients diagnosed with diabetes.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/medication.csv\",\"type\":\"csv\"},\"output\":\"medication\",\"comment\":\"Loading medication records from the healthcare database.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"diabetic_patients\",\"medication\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"diabetic_patients_medications\",\"comment\":\"Joining diabetic patient records with medication data to get prescribed medications.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"diabetic_patients_medications\",\"groupBy\":\"medication_name\",\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"average_age_per_medication\",\"comment\":\"Aggregating data to find the average age of diabetic patients for each medication.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_age_per_medication\",\"sortBy\":\"average_age\",\"order\":\"desc\"},\"output\":\"sorted_average_age_per_medication\",\"comment\":\"Sorting the aggregated data by average age in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data with columns for date, region, and consumption in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region where the energy consumption was recorded.\"},{\"column\":\"col_3\",\"column_name\":\"consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data with columns for date, region, and temperature.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region where the weather data was recorded.\"},{\"column\":\"col_3\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature recorded in degrees Celsius.\"}]},\"table_id3\":{\"name\":\"historical_energy_prices\",\"location\":\"C:/data/historical_energy_prices.csv\",\"sheet_name\":null,\"label\":\"Historical Energy Prices\",\"description\":\"This table contains historical energy prices data with columns for date, region, and price per kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the price record.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region where the price was recorded.\"},{\"column\":\"col_3\",\"column_name\":\"price_per_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kWh.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at 'C:/data/energy_consumption.csv'.\",\"Load the weather data from the Excel file located at 'C:/data/weather_data.xlsx'.\",\"Filter the energy consumption data to include only records where the consumption is greater than 100 kWh.\",\"Join the filtered energy consumption data with the weather data on the 'date' column.\",\"Calculate a new column 'consumption_per_degree' by dividing 'consumption' by 'temperature'.\",\"Aggregate the joined data by 'region' to calculate the total consumption and average temperature.\",\"Sort the aggregated data by 'total_consumption' in descending order.\",\"Apply a rolling mean transformation with a window of 7 days to the 'consumption' column.\",\"Forecast the energy consumption for the next 30 days using the Holt-Winters method.\",\"Identify outliers in the energy consumption data using the Z-score method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"consumption > 100\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering energy consumption data to include only records with consumption greater than 100 kWh.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered energy consumption data with weather data on the 'date' column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"consumption_per_degree\",\"formula\":\"consumption / temperature\"},\"output\":\"joined_data_with_derived\",\"comment\":\"Calculating 'consumption_per_degree' by dividing 'consumption' by 'temperature'.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_derived\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"total_consumption\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by 'region' to calculate total consumption and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_consumption\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by 'total_consumption' in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"energy_consumption\",\"transformations\":[{\"columns\":[\"consumption\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":7}}]},\"output\":\"smoothed_energy_consumption\",\"comment\":\"Applying a rolling mean transformation with a window of 7 days to the 'consumption' column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"energy_consumption\",\"columns\":[\"consumption\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"forecast_results\",\"comment\":\"Forecasting the energy consumption for the next 30 days using the Holt-Winters method.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"energy_consumption\",\"columns\":[\"consumption\"],\"forecastParameters\":{\"forecastPeriod\":1,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"z-score\",\"confidenceInterval\":95}},\"output\":\"outliers\",\"comment\":\"Identifying outliers in the energy consumption data using the Z-score method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/finance/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"This table contains detailed records of financial transactions including amounts, dates, and account types.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The monetary amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"account_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of account involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"account_details\",\"location\":\"C:/data/finance/account_details.csv\",\"sheet_name\":null,\"label\":\"Account Details\",\"description\":\"Contains information about account holders and their account types.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"account_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each account.\"},{\"column\":\"col_2\",\"column_name\":\"account_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of account.\"},{\"column\":\"col_3\",\"column_name\":\"holder_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the account holder.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/finance/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Aggregate the filtered transactions by account type to calculate the total amount.\",\"Sort the aggregated data by total amount in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"`amount` > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_transactions\",\"groupBy\":\"account_type\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_amount\",\"function\":\"sum\"}]},\"output\":\"aggregated_transactions\",\"comment\":\"Aggregating filtered transactions by account type to calculate the total amount.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_transactions\",\"sortBy\":\"total_amount\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_transactions\",\"comment\":\"Sorting the aggregated data by total amount in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the 'gmm' algorithm used for?","output":"Use 'gmm' (Gaussian Mixture Model) for probabilistic clustering-based anomaly detection."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/finance/transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with columns for transaction ID, customer ID, amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_2\":{\"name\":\"customer_details\",\"location\":\"C:/data/finance/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"A table containing customer details with columns for customer ID, name, and contact information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_info\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact information for the customer.\"}]},\"table_3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/finance/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"A table with irrelevant data not used in the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the financial transactions table from the CSV file.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Join the filtered transactions with the customer details table on the customer ID.\",\"Aggregate the joined data to calculate the total transaction amount per customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading the customer details table from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_transaction_amount\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating joined data to calculate the total transaction amount per customer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_records\",\"location\":\"C:/data/hr/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"A table containing employee details including salary, hire date, and other personal information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"hire_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date the employee was hired.\"}]},\"table_id2\":{\"name\":\"department_info\",\"location\":\"C:/data/hr/department_info.csv\",\"sheet_name\":null,\"label\":\"Department Information\",\"description\":\"Information about the different departments within the company.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]}},\"instructions\":[\"Load the employee records from the CSV file located at C:/data/hr/employee_records.csv.\",\"Filter the employee records to include only those with a salary greater than 50,000.\",\"Sort the filtered employee records by the hire date in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/hr/employee_records.csv\",\"type\":\"csv\"},\"output\":\"employee_records\",\"comment\":\"Loading the employee records from the specified CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_records\",\"query\":\"`salary` > 50000\"},\"output\":\"high_salary_employees\",\"comment\":\"Filtering employee records to include only those with a salary greater than 50,000.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"high_salary_employees\",\"sortBy\":\"hire_date\",\"order\":\"asc\"},\"output\":\"sorted_high_salary_employees\",\"comment\":\"Sorting the filtered employee records by hire date in ascending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'interpolate' operation be used?","output":"Use 'interpolate' when you need to fill missing values using interpolation methods like 'linear' or 'spline'. This is useful for time series data with occasional missing values."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_data\",\"location\":\"C:/data/customer_data.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"Contains customer information including customer_id, name, and contact details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"contact\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact details of the customer.\"}]},\"table_id2\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Sales Data\",\"description\":\"Contains sales transactions including transaction_id, customer_id, total_revenue, and total_cost.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the transaction to a customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"},{\"column\":\"column_4_id\",\"column_name\":\"total_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total cost incurred for the transaction.\"}]},\"table_id3\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing_campaigns.csv\",\"sheet_name\":null,\"label\":\"Marketing Campaigns\",\"description\":\"Contains data on various marketing campaigns including campaign_id, start_date, and end_date.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"column_2_id\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Start date of the marketing campaign.\"},{\"column\":\"column_3_id\",\"column_name\":\"end_date\",\"column_type\":\"xsd:date\",\"column_description\":\"End date of the marketing campaign.\"}]}},\"instructions\":[\"Load the customer data from the CSV file located at C:/data/customer_data.csv.\",\"Load the sales data from the Excel file located at C:/data/sales_data.xlsx.\",\"Filter the sales data to include only transactions with a total revenue greater than 1000.\",\"Join the filtered sales data with the customer data on the customer_id column.\",\"Calculate a new column 'profit_margin' by subtracting total_cost from total_revenue in the joined table.\",\"Aggregate the joined table by customer_id to calculate the total sales and average profit margin.\",\"Sort the aggregated data by total sales in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_data.csv\",\"type\":\"csv\"},\"output\":\"customer_data\",\"comment\":\"Loading customer data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"sales_data\",\"comment\":\"Loading sales data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"total_revenue > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customer_data\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered sales data with customer data on customer_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"profit_margin\",\"formula\":\"total_revenue - total_cost\"},\"output\":\"data_with_profit\",\"comment\":\"Calculating profit margin by subtracting total cost from total revenue.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_profit\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"},{\"column\":\"profit_margin\",\"name\":\"average_profit_margin\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by customer_id to calculate total sales and average profit margin.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total sales in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'min_max_scale' operation be used?","output":"Use 'min_max_scale' when you need to scale data to a fixed range, such as [0,1]. This is commonly used for normalizing inputs in machine learning models."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"product_category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product being reviewed.\"},{\"column\":\"col_3\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating for the product, ranging from 1 to 5.\"},{\"column\":\"col_4\",\"column_name\":\"feedback\",\"column_type\":\"xsd:string\",\"column_description\":\"Textual feedback provided by the customer.\"}]},\"table_id2\":{\"name\":\"customer_profiles\",\"location\":\"C:/data/customer_profiles.csv\",\"sheet_name\":null,\"label\":\"Customer Profiles\",\"description\":\"Table containing customer demographic information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback to include only those with a rating of 4 or 5.\",\"Aggregate the filtered feedback by product category to find the average rating.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback to include only those with a rating of 4 or 5.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"positive_feedback\",\"groupBy\":\"product_category\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_ratings_by_category\",\"comment\":\"Aggregating feedback by product category to find the average rating.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid Boolean expression using 'and'.","output":"'col1 > 10 and col2 < 5'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Contains customer feedback including ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:integer\",\"column_description\":\"Customer rating for the product, ranging from 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"comment\",\"column_type\":\"xsd:string\",\"column_description\":\"Customer's feedback comment.\"}]},\"table_id2\":{\"name\":\"purchase_history\",\"location\":\"C:/data/purchase_history.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Purchase History\",\"description\":\"Records of customer purchases including product details and amounts.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_3_id\",\"column_name\":\"purchase_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount spent by the customer on the purchase.\"}]},\"table_id3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.xlsx\",\"sheet_name\":\"Products\",\"label\":\"Product Catalog\",\"description\":\"List of products available for purchase with details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]}},\"instructions\":[\"Load the customer feedback data from the CSV file.\",\"Load the customer purchase history from the Excel file.\",\"Filter the feedback data to include only feedback with a rating of 4 or 5.\",\"Join the filtered feedback data with the purchase history on customer ID.\",\"Calculate the total purchase amount for each customer.\",\"Sort the customers by total purchase amount in descending order.\",\"Identify the top 10 customers based on total purchase amount.\",\"Aggregate the feedback data to find the average rating for each product.\",\"Filter the aggregated data to include only products with an average rating above 4.5.\",\"Generate a report of the top 10 customers and high-rated products.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/purchase_history.xlsx\",\"type\":\"xlsx\"},\"output\":\"purchase_history\",\"comment\":\"Loading customer purchase history from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback to include only ratings of 4 or 5.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"purchase_history\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"feedback_with_purchases\",\"comment\":\"Joining filtered feedback with purchase history on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"feedback_with_purchases\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"purchase_amount\",\"name\":\"total_purchase\",\"function\":\"sum\"}]},\"output\":\"customer_total_purchases\",\"comment\":\"Calculating total purchase amount for each customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"customer_total_purchases\",\"sortBy\":\"total_purchase\",\"order\":\"desc\"},\"output\":\"sorted_customers\",\"comment\":\"Sorting customers by total purchase amount in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_customers\",\"query\":\"index < 10\"},\"output\":\"top_10_customers\",\"comment\":\"Identifying the top 10 customers based on total purchase amount.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"positive_feedback\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"product_average_ratings\",\"comment\":\"Aggregating feedback to find average rating for each product.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_average_ratings\",\"query\":\"average_rating > 4.5\"},\"output\":\"high_rated_products\",\"comment\":\"Filtering products with an average rating above 4.5.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"top_10_customers\",\"high_rated_products\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"customer_product_report\",\"comment\":\"Generating a report of the top 10 customers and high-rated products.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customers\",\"location\":\"C:/data/marketing/customers.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"Contains customer information including customer_id, name, and contact details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact details of the customer.\"}]},\"table_id2\":{\"name\":\"sales\",\"location\":\"C:/data/marketing/sales.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Sales Data\",\"description\":\"Contains sales transactions including transaction_id, customer_id, product_id, date, and amount.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking to the customer who made the purchase.\"},{\"column\":\"col_3\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product sold.\"},{\"column\":\"col_4\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the transaction.\"},{\"column\":\"col_5\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total amount of the transaction.\"}]},\"table_id3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/marketing/product_catalog.xlsx\",\"sheet_name\":\"Products\",\"label\":\"Product Catalog\",\"description\":\"Contains product details including product_id, name, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]}},\"instructions\":[\"Load the customer data from the CSV file located at C:/data/marketing/customers.csv.\",\"Load the sales data from the Excel file located at C:/data/marketing/sales.xlsx.\",\"Filter the sales data to include only transactions from the last quarter.\",\"Join the filtered sales data with the customer data on the customer_id column.\",\"Calculate the total purchase amount for each customer.\",\"Sort the customers by their total purchase amount in descending order.\",\"Identify the top 10% of customers based on their total purchase amount.\",\"Aggregate the sales data by product category to find the total sales for each category.\",\"Forecast the next quarter's sales for each product category using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customers.csv\",\"type\":\"csv\"},\"output\":\"customers\",\"comment\":\"Loading customer data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/sales.xlsx\",\"type\":\"xlsx\"},\"output\":\"sales\",\"comment\":\"Loading sales data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales\",\"query\":\"`date` >= '2023-07-01' and `date` <= '2023-09-30'\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions from the last quarter.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customers\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer data on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_customers\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_purchase\",\"function\":\"sum\"}]},\"output\":\"customer_purchases\",\"comment\":\"Calculating the total purchase amount for each customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"customer_purchases\",\"sortBy\":\"total_purchase\",\"order\":\"desc\"},\"output\":\"sorted_customers\",\"comment\":\"Sorting customers by their total purchase amount in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_customers\",\"query\":\"index < len(sorted_customers) * 0.1\"},\"output\":\"top_customers\",\"comment\":\"Identifying the top 10% of customers based on their total purchase amount.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"sales_by_product\",\"comment\":\"Aggregating sales data by product category to find the total sales for each category.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sales_by_product\",\"columns\":[\"total_sales\"],\"forecastParameters\":{\"forecastPeriod\":90,\"frequency\":\"D\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":30,\"dateColumn\":\"date\"}},\"output\":\"forecasted_sales\",\"comment\":\"Forecasting the next quarter's sales for each product category using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'log_transform' transformation.","output":"{ \"name\": \"applyTransformations\", \"parameters\": { \"table\": \"financial_data\", \"transformations\": [ { \"columns\": [\"market_cap\"], \"operation\": \"log_transform\" } ] }, \"output\": \"log_market_cap\" }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of shipments including weight, destination, and cost per kg.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"cost_per_kg\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost per kilogram for shipping.\"},{\"column\":\"col_4\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse associated with the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_data\",\"location\":\"C:/data/warehouse_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Warehouse Data\",\"description\":\"Details about warehouses including their locations.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"warehouse_location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse.\"}]},\"table_id3\":{\"name\":\"fleet_data\",\"location\":\"C:/data/fleet_data.csv\",\"sheet_name\":null,\"label\":\"Fleet Data\",\"description\":\"Information about the fleet vehicles used for shipments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"vehicle_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each vehicle.\"},{\"column\":\"col_2\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Capacity of the vehicle in kilograms.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from a CSV file.\",\"Load the 'warehouse_data' table from an Excel file.\",\"Filter the 'shipment_data' to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered 'shipment_data' with 'warehouse_data' on the 'warehouse_id' column.\",\"Calculate a new column 'shipping_cost' in the joined table by multiplying 'weight' by 'cost_per_kg'.\",\"Aggregate the data by 'warehouse_location' to find the total shipping cost per location.\",\"Sort the aggregated data by 'total_shipping_cost' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/warehouse_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"warehouse_data\",\"comment\":\"Loading warehouse data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"filtered_shipment_data\",\"comment\":\"Filtering shipments to include only those with weight greater than 1000 kg.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipment_data\",\"warehouse_data\"],\"joinOn\":[\"warehouse_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with warehouse data on warehouse_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"shipping_cost\",\"formula\":\"weight * cost_per_kg\"},\"output\":\"data_with_shipping_cost\",\"comment\":\"Calculating shipping cost by multiplying weight by cost per kg.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_shipping_cost\",\"groupBy\":\"warehouse_location\",\"aggregations\":[{\"column\":\"shipping_cost\",\"name\":\"total_shipping_cost\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by warehouse location to find total shipping cost per location.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_shipping_cost\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total shipping cost in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of each product including product ID, name, and price.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]},\"table_id2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx\",\"sheet_name\":\"2023_data\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including product ID, estimated sales volume, and date of research.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"estimated_sales_volume\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Estimated sales volume for the product.\"},{\"column\":\"col_3\",\"column_name\":\"research_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the market research was conducted.\"}]},\"table_id3\":{\"name\":\"historical_sales\",\"location\":\"C:/data/historical_sales.csv\",\"sheet_name\":null,\"label\":\"Historical Sales Data\",\"description\":\"Contains historical sales data for various products, not relevant for this workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales_volume\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Recorded sales volume for the product.\"},{\"column\":\"col_3\",\"column_name\":\"sales_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the sales record.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel sheet.\",\"Filter the market research data to include only entries from the last year.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Calculate the potential revenue by multiplying the estimated sales volume by the product price.\",\"Sort the resulting table by potential revenue in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading the market research data from an Excel sheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"`research_date` >= '2022-01-01'\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only entries from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_revenue\",\"formula\":\"`estimated_sales_volume` * `price`\"},\"output\":\"data_with_revenue\",\"comment\":\"Calculating potential revenue by multiplying estimated sales volume by product price.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"data_with_revenue\",\"sortBy\":\"potential_revenue\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the resulting table by potential revenue in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of all shipments including their status, dispatch time, and expected delivery time.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"dispatch_time\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Time when the shipment was dispatched.\"},{\"column\":\"col_4\",\"column_name\":\"expected_delivery_time\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Expected time of delivery for the shipment.\"},{\"column\":\"col_5\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse associated with the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_data\",\"location\":\"C:/data/logistics/warehouse_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Warehouse Data\",\"description\":\"Contains information about warehouses including their IDs and locations.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/logistics/inventory_data.xlsx\",\"sheet_name\":\"Inventory\",\"label\":\"Inventory Data\",\"description\":\"Contains inventory details for each warehouse.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"col_2\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse where the item is stored.\"},{\"column\":\"col_3\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the item in stock.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at C:/data/logistics/shipment_data.csv.\",\"Load the 'warehouse_data' table from the Excel file located at C:/data/logistics/warehouse_data.xlsx.\",\"Filter the 'shipment_data' to include only shipments with a status of 'In Transit'.\",\"Join the filtered shipment data with the 'warehouse_data' on the 'warehouse_id' column using an inner join.\",\"Calculate a new column 'delivery_time' in the joined table by subtracting 'dispatch_time' from 'expected_delivery_time'.\",\"Aggregate the joined table to find the total number of shipments per warehouse.\",\"Sort the aggregated data by the total number of shipments in descending order.\",\"Apply a log transformation to the 'total_shipments' column in the sorted data.\",\"Forecast the future shipment count for each warehouse using the 'total_shipments' column with a forecast period of 30 days.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouse_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"warehouse_data\",\"comment\":\"Loading warehouse data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"status == 'In Transit'\"},\"output\":\"in_transit_shipments\",\"comment\":\"Filtering shipments to include only those that are in transit.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"in_transit_shipments\",\"warehouse_data\"],\"joinOn\":[\"warehouse_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with warehouse data on warehouse_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"delivery_time\",\"formula\":\"`expected_delivery_time` - `dispatch_time`\"},\"output\":\"joined_data_with_delivery_time\",\"comment\":\"Calculating delivery time by subtracting dispatch time from expected delivery time.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_delivery_time\",\"groupBy\":\"warehouse_id\",\"aggregations\":[{\"column\":\"shipment_id\",\"name\":\"total_shipments\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find the total number of shipments per warehouse.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_shipments\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total number of shipments in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_data\",\"transformations\":[{\"columns\":[\"total_shipments\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_data\",\"comment\":\"Applying log transformation to the total_shipments column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_data\",\"columns\":[\"total_shipments\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"dateColumn\":\"warehouse_id\"}},\"output\":\"forecast_results\",\"comment\":\"Forecasting future shipment count for each warehouse using a 30-day period.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid column name usage.","output":"'col1 + col2'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/finance/transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing all financial transactions including date, amount, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"The category of the transaction, such as groceries, utilities, etc.\"}]},\"table_id2\":{\"name\":\"budget_plan\",\"location\":\"C:/data/finance/budget_plan.xlsx\",\"sheet_name\":\"Plan\",\"label\":\"Budget Plan\",\"description\":\"A table outlining the planned budget for different categories.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"The category for which the budget is planned.\"},{\"column\":\"col_2\",\"column_name\":\"planned_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The planned budget amount for the category.\"}]}},\"instructions\":[\"Load the financial transactions table from the CSV file.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Calculate the total amount for each category.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"`amount` > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_transactions\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_amount\",\"function\":\"sum\"}]},\"output\":\"category_totals\",\"comment\":\"Calculating the total amount for each category.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_engagement\",\"location\":\"C:/data/marketing/customer_engagement.csv\",\"sheet_name\":null,\"label\":\"Customer Engagement Data\",\"description\":\"This table contains customer engagement data including interactions with marketing campaigns.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"interaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of customer interaction.\"},{\"column\":\"col_3\",\"column_name\":\"interaction_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of interaction with the marketing campaign.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/marketing/customer_details.csv\",\"sheet_name\":null,\"label\":\"Customer Details\",\"description\":\"Contains detailed information about customers, such as demographics and preferences.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]}},\"instructions\":[\"Load the customer engagement data from the CSV file.\",\"Filter the data to include only customers who have interacted with the marketing campaign in the last 30 days.\",\"Aggregate the filtered data to calculate the total number of interactions per customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_engagement.csv\",\"type\":\"csv\"},\"output\":\"customer_engagement\",\"comment\":\"Loading the customer engagement data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_engagement\",\"query\":\"`interaction_date` >= '2023-09-01'\"},\"output\":\"recent_engagements\",\"comment\":\"Filtering data to include only interactions from the last 30 days.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"recent_engagements\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"interaction_type\",\"name\":\"total_interactions\",\"function\":\"count\"}]},\"output\":\"customer_interaction_summary\",\"comment\":\"Aggregating the data to calculate the total number of interactions per customer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains daily energy consumption data with columns for date and consumption.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed on the given date.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Weather Data\",\"description\":\"This table contains daily weather data with columns for date and temperature.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature recorded on the given date.\"}]},\"table_id3\":{\"name\":\"historical_energy_prices\",\"location\":\"C:/data/historical_energy_prices.csv\",\"sheet_name\":null,\"label\":\"Historical Energy Prices\",\"description\":\"This table contains historical energy prices data, which is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the price record.\"},{\"column\":\"col_2\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The energy price on the given date.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Load the weather data from the Excel file located at C:/data/weather_data.xlsx.\",\"Filter the energy consumption data to include only records from the year 2023.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column 'energy_efficiency' by dividing energy consumption by temperature.\",\"Aggregate the joined data by month to calculate the total energy consumption and average temperature.\",\"Sort the aggregated data by total energy consumption in descending order.\",\"Apply a rolling mean transformation with a window of 7 days to the energy consumption column.\",\"Forecast the energy consumption for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering energy consumption data to include only records from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered energy consumption data with weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`consumption` / `temperature`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating energy efficiency by dividing energy consumption by temperature.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date.dt.to_period('M')\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating data by month to calculate total energy consumption and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total energy consumption in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"transformations\":[{\"columns\":[\"consumption\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":7}}]},\"output\":\"smoothed_data\",\"comment\":\"Applying a rolling mean transformation with a window of 7 days to the energy consumption column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"smoothed_data\",\"columns\":[\"consumption\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"forecast_results\",\"comment\":\"Forecasting the energy consumption for the next 30 days using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"A table containing detailed specifications of products, including status and supplier ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Approval status of the product.\"},{\"column\":\"col_4\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"},{\"column\":\"col_5\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of the product.\"}]},\"table_id2\":{\"name\":\"supplier_information\",\"location\":\"C:/data/supplier_information.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"A table containing information about suppliers, including supplier ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"A table containing data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant data column.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a status of 'approved'.\",\"Join the approved product specifications with the supplier information table on the supplier ID.\",\"Aggregate the joined data to calculate the total cost per supplier.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"status == 'approved'\"},\"output\":\"approved_products\",\"comment\":\"Filtering the product specifications to include only approved products.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_information.csv\",\"type\":\"csv\"},\"output\":\"supplier_information\",\"comment\":\"Loading the supplier information from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"approved_products\",\"supplier_information\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining approved product specifications with supplier information on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"supplier_id\",\"aggregations\":[{\"column\":\"cost\",\"name\":\"total_cost\",\"function\":\"sum\"}]},\"output\":\"total_cost_per_supplier\",\"comment\":\"Aggregating the joined data to calculate the total cost per supplier.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"This table contains customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:integer\",\"column_description\":\"Customer rating on a scale of 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"comment\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comment provided by the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"This table contains detailed information about customers, including their region.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"column_3_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.xlsx\",\"sheet_name\":\"Products\",\"label\":\"Product Catalog\",\"description\":\"This table contains information about the products offered by the company.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]}},\"instructions\":[\"Load the customer feedback data from the CSV file.\",\"Filter the feedback to include only those with a rating of 3 or below.\",\"Load the customer details from the Excel file.\",\"Join the filtered feedback with customer details using the customer ID.\",\"Aggregate the joined data to count the number of low ratings per region.\",\"Sort the aggregated data by the count of low ratings in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating <= 3\"},\"output\":\"low_rating_feedback\",\"comment\":\"Filtering feedback to include only those with a rating of 3 or below.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_rating_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_feedback_details\",\"comment\":\"Joining filtered feedback with customer details using customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_feedback_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"low_rating_count\",\"function\":\"count\"}]},\"output\":\"low_ratings_per_region\",\"comment\":\"Aggregating data to count the number of low ratings per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"low_ratings_per_region\",\"sortBy\":\"low_rating_count\",\"order\":\"desc\"},\"output\":\"sorted_low_ratings\",\"comment\":\"Sorting aggregated data by the count of low ratings in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials\",\"description\":\"Table containing information about raw materials including stock levels and prices.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"stock_level\",\"column_name\":\"STOCK_LEVEL\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the material.\"},{\"column\":\"unit_price\",\"column_name\":\"UNIT_PRICE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price per unit of the material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"Table detailing the planned production activities including material requirements.\",\"columns\":[{\"column\":\"production_line\",\"column_name\":\"PRODUCTION_LINE\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the production line.\"},{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material required.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material required.\"}]},\"table_id3\":{\"name\":\"machine_maintenance\",\"location\":\"C:/data/manufacturing/machine_maintenance.csv\",\"sheet_name\":null,\"label\":\"Machine Maintenance\",\"description\":\"Table containing maintenance schedules for machines, not relevant to the current workflow.\",\"columns\":[{\"column\":\"machine_id\",\"column_name\":\"MACHINE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each machine.\"},{\"column\":\"maintenance_date\",\"column_name\":\"MAINTENANCE_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"Scheduled date for maintenance.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains information about the materials used in manufacturing.\",\"Load the table 'production_schedule' which details the planned production activities.\",\"Filter the 'raw_materials' table to include only materials with a stock level greater than 100.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Calculate a new column 'material_cost' in the joined table by multiplying 'quantity' by 'unit_price'.\",\"Aggregate the joined table by 'production_line' to calculate the total 'material_cost' for each line.\",\"Sort the aggregated data by 'total_material_cost' in descending order.\",\"Identify the top 5 production lines with the highest material costs.\",\"Apply a transformation to normalize the 'total_material_cost' column in the top 5 production lines.\",\"Forecast the material requirements for the next month using the 'production_schedule' data.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"`STOCK_LEVEL` > 100\"},\"output\":\"filtered_materials\",\"comment\":\"Filtering raw materials to include only those with stock levels greater than 100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_materials\",\"production_schedule\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered materials with production schedule on material_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"material_cost\",\"formula\":\"`QUANTITY` * `UNIT_PRICE`\"},\"output\":\"joined_data_with_cost\",\"comment\":\"Calculating material cost by multiplying quantity by unit price.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_cost\",\"groupBy\":\"PRODUCTION_LINE\",\"aggregations\":[{\"column\":\"material_cost\",\"name\":\"total_material_cost\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by production line to calculate total material cost.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_material_cost\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total material cost in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"query\":\"index < 5\"},\"output\":\"top_5_production_lines\",\"comment\":\"Identifying the top 5 production lines with the highest material costs.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"top_5_production_lines\",\"transformations\":[{\"columns\":[\"total_material_cost\"],\"operation\":\"normalize\"}]},\"output\":\"normalized_top_5_lines\",\"comment\":\"Normalizing the total material cost column in the top 5 production lines.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"production_schedule\",\"columns\":[\"QUANTITY\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"dateColumn\":\"PRODUCTION_LINE\"}},\"output\":\"material_forecast\",\"comment\":\"Forecasting material requirements for the next month using the production schedule data.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"A table containing customer feedback including ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"product_category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product being reviewed.\"},{\"column\":\"col_3\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating for the product, ranging from 1 to 5.\"},{\"column\":\"col_4\",\"column_name\":\"feedback\",\"column_type\":\"xsd:string\",\"column_description\":\"Textual feedback provided by the customer.\"}]},\"table_id2\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.xlsx\",\"sheet_name\":\"Products\",\"label\":\"Product Catalog\",\"description\":\"A table listing all available products and their details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback to include only those with a rating of 4 or 5.\",\"Aggregate the filtered feedback by product category to find the average rating.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading the customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback to include only those with a rating of 4 or 5.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"positive_feedback\",\"groupBy\":\"product_category\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_by_category\",\"comment\":\"Aggregating feedback by product category to find the average rating.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'detect_outliers' operation be used?","output":"Use 'detect_outliers' when you need to identify anomalies in a time series using methods like z-score thresholding."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy/consumption_2023.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data recorded daily for the year 2023.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed on the given date.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather/weather_2023.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains daily weather data for the year 2023, including temperature and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The recorded temperature on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The recorded humidity on the given date.\"}]},\"table_id3\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy/prices_2023.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains daily energy prices for the year 2023.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The energy price on the given date.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each month.\",\"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy/consumption_2023.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_consumption\",\"groupBy\":\"date.dt.to_period('M')\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"total_monthly_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Calculating the total energy consumption for each month.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"monthly_energy_consumption\",\"columns\":[\"total_monthly_consumption\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"confidenceInterval\":95}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'normalize' operation do?","output":"The 'normalize' operation normalizes data by dividing by its maximum absolute value."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications of each product including product ID, name, category, and price.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"},{\"column\":\"column_4_id\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]},\"table_id2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including product ID, estimated demand, and date of research.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"estimated_demand\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Estimated demand for the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"research_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the market research was conducted.\"}]},\"table_id3\":{\"name\":\"historical_sales\",\"location\":\"C:/data/historical_sales.csv\",\"sheet_name\":null,\"label\":\"Historical Sales Data\",\"description\":\"Contains historical sales data which is not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"sale_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each sale.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_3_id\",\"column_name\":\"sale_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the sale.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel file.\",\"Filter the market research data to include only entries from the last year.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Calculate the potential revenue for each product by multiplying the estimated demand by the product price.\",\"Aggregate the data to find the total potential revenue by product category.\",\"Sort the aggregated data by total potential revenue in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading the market research data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"`research_date` >= '2022-01-01'\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only entries from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_revenue\",\"formula\":\"`estimated_demand` * `price`\"},\"output\":\"data_with_revenue\",\"comment\":\"Calculating potential revenue for each product by multiplying estimated demand by product price.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_revenue\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"potential_revenue\",\"name\":\"total_potential_revenue\",\"function\":\"sum\"}]},\"output\":\"aggregated_revenue\",\"comment\":\"Aggregating data to find total potential revenue by product category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_revenue\",\"sortBy\":\"total_potential_revenue\",\"order\":\"desc\"},\"output\":\"sorted_revenue\",\"comment\":\"Sorting aggregated data by total potential revenue in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data with timestamps and usage values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature and humidity readings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature reading in degrees Celsius.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The humidity percentage.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Aggregate the data to calculate the total energy consumption per month.\",\"Calculate the average daily energy consumption for each month.\",\"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"energy_2023\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_2023\",\"groupBy\":\"timestamp.dt.month\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_monthly_usage\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_usage\",\"comment\":\"Aggregating the data to calculate the total energy consumption per month.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"monthly_energy_usage\",\"columnName\":\"average_daily_usage\",\"formula\":\"total_monthly_usage / 30\"},\"output\":\"monthly_energy_with_avg\",\"comment\":\"Calculating the average daily energy consumption for each month.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"monthly_energy_with_avg\",\"columns\":[\"total_monthly_usage\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"timestamp\"}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid comparison expression due to incorrect syntax.","output":"'col1 >' // Error: Missing right operand in comparison."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid function call using unsupported functions.","output":"'len(col1)' // Error: 'len' is not a supported function in Tamarind expressions."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information of customers including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"marketing_campaign_results\",\"location\":\"C:/data/marketing_campaigns.xlsx\",\"sheet_name\":\"Results\",\"label\":\"Marketing Campaign Results\",\"description\":\"Details of marketing campaigns including their success status.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each campaign.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"success\",\"column_type\":\"xsd:boolean\",\"column_description\":\"Indicates whether the campaign was successful.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the marketing workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant data column.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the marketing campaign results table from the Excel file.\",\"Filter the marketing campaign results to include only successful campaigns.\",\"Join the filtered campaign results with the customer demographics table on customer ID.\",\"Aggregate the joined data to find the total number of successful campaigns per age group.\",\"Sort the aggregated data by the number of successful campaigns in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing_campaigns.xlsx\",\"type\":\"xlsx\"},\"output\":\"marketing_campaign_results\",\"comment\":\"Loading marketing campaign results from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"marketing_campaign_results\",\"query\":\"success == True\"},\"output\":\"successful_campaigns\",\"comment\":\"Filtering campaign results to include only successful campaigns.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"successful_campaigns\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining successful campaign results with customer demographics on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"age\",\"aggregations\":[{\"column\":\"campaign_id\",\"name\":\"total_successful_campaigns\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find the total number of successful campaigns per age group.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_successful_campaigns\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by the number of successful campaigns in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"Contains customer feedback including ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer feedback rating on a scale of 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Additional comments provided by the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details Data\",\"description\":\"Contains detailed information about customers including their contact details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales Data\",\"description\":\"Contains sales data for various products, not relevant to customer feedback analysis.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"quantity_sold\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Number of units sold.\"},{\"column\":\"column_3_id\",\"column_name\":\"sale_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the sale.\"}]}},\"instructions\":[\"Load the customer feedback data from the CSV file.\",\"Load the customer details data from the Excel file.\",\"Filter the feedback data to include only feedback with a rating of 4 or higher.\",\"Join the filtered feedback data with the customer details data on the customer ID.\",\"Aggregate the joined data to calculate the average feedback rating per customer.\",\"Sort the aggregated data by average feedback rating in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback data to include only ratings of 4 or higher.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating average feedback rating per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by average feedback rating in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid column name usage due to missing backticks.","output":"'column name + col2' // Error: Column names with spaces must be enclosed in backticks (`column name`)."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_records\",\"location\":\"C:/data/legal/case_records.csv\",\"sheet_name\":null,\"label\":\"Case Records\",\"description\":\"A table containing details of legal cases including case ID, filing date, and lawyer ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"filing_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"},{\"column\":\"col_3\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"}]},\"table_id2\":{\"name\":\"lawyer_info\",\"location\":\"C:/data/legal/lawyer_info.csv\",\"sheet_name\":null,\"label\":\"Lawyer Information\",\"description\":\"A table containing information about lawyers including lawyer ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_schedule\",\"location\":\"C:/data/legal/court_schedule.csv\",\"sheet_name\":null,\"label\":\"Court Schedule\",\"description\":\"A table containing court schedules, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"schedule_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each schedule.\"},{\"column\":\"col_2\",\"column_name\":\"court_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the court session.\"}]}},\"instructions\":[\"Load the table 'case_records' which contains details of legal cases.\",\"Filter the 'case_records' table to include only cases filed after January 1, 2020.\",\"Join the filtered 'case_records' with the 'lawyer_info' table on the lawyer_id column.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/case_records.csv\",\"type\":\"csv\"},\"output\":\"case_records\",\"comment\":\"Loading the table containing details of legal cases.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_records\",\"query\":\"`filing_date` > '2020-01-01'\"},\"output\":\"filtered_case_records\",\"comment\":\"Filtering cases to include only those filed after January 1, 2020.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/lawyer_info.csv\",\"type\":\"csv\"},\"output\":\"lawyer_info\",\"comment\":\"Loading the table containing information about lawyers.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_case_records\",\"lawyer_info\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered case records with lawyer information based on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_table\",\"groupBy\":\"lawyer_id\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_count\",\"comment\":\"Aggregating the joined table to find the total number of cases handled by each lawyer.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'sortData' action with a missing required parameter.","output":"{ \"name\": \"sortData\", \"parameters\": { \"table\": \"sales\", \"order\": \"asc\" }, \"output\": \"sorted_sales\" } // Error: 'sortBy' parameter is missing."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with details such as account_id, transaction_date, and amount.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"account_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each account.\"},{\"column\":\"column_2_id\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"},{\"column\":\"column_3_id\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"A table containing details about customers such as customer_id, name, and address.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"address\",\"column_type\":\"xsd:string\",\"column_description\":\"The address of the customer.\"}]}},\"instructions\":[\"Load the table 'financial_transactions' from the CSV file located at C:/data/financial_transactions.csv.\",\"Filter the transactions to include only those with an amount greater than 1000.\",\"Aggregate the filtered transactions by account_id to calculate the total amount for each account.\",\"Sort the aggregated data by total amount in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_transactions\",\"groupBy\":\"account_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_amount\",\"function\":\"sum\"}]},\"output\":\"aggregated_transactions\",\"comment\":\"Aggregating filtered transactions by account_id to calculate the total amount for each account.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_transactions\",\"sortBy\":\"total_amount\",\"order\":\"desc\"},\"output\":\"sorted_transactions\",\"comment\":\"Sorting the aggregated data by total amount in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/manufacturing_data/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Inventory\",\"description\":\"Contains information about the raw materials available in the inventory, including material ID, name, and quantity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"col_2\",\"column_name\":\"material_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the material.\"},{\"column\":\"col_3\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the material available.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/manufacturing_data/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"Details the production schedule, including product ID, material ID, and quantity required.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material required for production.\"},{\"column\":\"col_3\",\"column_name\":\"quantity_required\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material required for production.\"}]},\"table_id3\":{\"name\":\"employee_records\",\"location\":\"C:/manufacturing_data/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"Contains records of employees, including employee ID, name, and department.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"employee_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the employee works.\"}]}},\"instructions\":[\"Load the 'raw_materials' table from the CSV file.\",\"Load the 'production_schedule' table from the Excel file.\",\"Filter the 'raw_materials' table to include only materials with a quantity greater than 100.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Aggregate the joined table to calculate the total quantity required for each product.\",\"Sort the aggregated data by 'total_quantity' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/manufacturing_data/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials inventory from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/manufacturing_data/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"quantity > 100\"},\"output\":\"filtered_raw_materials\",\"comment\":\"Filtering raw materials to include only those with a quantity greater than 100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_raw_materials\",\"production_schedule\"],\"joinOn\":[\"material_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered raw materials with the production schedule on material_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"quantity_required\",\"name\":\"total_quantity\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined data to calculate the total quantity required for each product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_quantity\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total quantity in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/hospital_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains detailed records of patients including diagnoses, treatments, and personal information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"blood_pressure\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Recorded blood pressure of the patient.\"}]},\"table_id2\":{\"name\":\"medication\",\"location\":\"C:/hospital_data/medication.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Details of medications prescribed to patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the prescribed medication.\"}]},\"table_id3\":{\"name\":\"staff_records\",\"location\":\"C:/hospital_data/staff_records.csv\",\"sheet_name\":null,\"label\":\"Staff Records\",\"description\":\"Contains records of hospital staff including roles and departments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"role\",\"column_type\":\"xsd:string\",\"column_description\":\"Role of the staff member in the hospital.\"}]}},\"instructions\":[\"Load the patient records table from the hospital database.\",\"Filter the records to include only patients diagnosed with hypertension.\",\"Join the filtered patient records with the medication table to get prescribed medications.\",\"Calculate the average blood pressure for each patient.\",\"Sort the patients by their average blood pressure in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from the hospital database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'hypertension'\"},\"output\":\"hypertension_patients\",\"comment\":\"Filtering records to include only patients diagnosed with hypertension.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/medication.csv\",\"type\":\"csv\"},\"output\":\"medication\",\"comment\":\"Loading medication records to join with patient data.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"hypertension_patients\",\"medication\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"patients_with_medication\",\"comment\":\"Joining filtered patient records with medication data.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"patients_with_medication\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"blood_pressure\",\"name\":\"average_blood_pressure\",\"function\":\"mean\"}]},\"output\":\"patients_with_avg_bp\",\"comment\":\"Calculating the average blood pressure for each patient.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"patients_with_avg_bp\",\"sortBy\":\"average_blood_pressure\",\"order\":\"desc\"},\"output\":\"sorted_patients\",\"comment\":\"Sorting patients by their average blood pressure in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information of customers including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"marketing_campaign_results\",\"location\":\"C:/data/marketing_campaign_results.xlsx\",\"sheet_name\":\"Results\",\"label\":\"Marketing Campaign Results\",\"description\":\"Contains results of various marketing campaigns including customer ID and campaign success.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"campaign_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of the marketing campaign.\"},{\"column\":\"col_3\",\"column_name\":\"success\",\"column_type\":\"xsd:boolean\",\"column_description\":\"Indicates whether the campaign was successful.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the customer demographics data from the CSV file.\",\"Load the marketing campaign results from the Excel file.\",\"Filter the customer demographics to include only those aged 18-35.\",\"Filter the marketing campaign results to include only successful campaigns.\",\"Join the filtered customer demographics with the successful campaign results on customer ID.\",\"Calculate the total spend for each customer by summing up the campaign costs.\",\"Aggregate the data to find the average spend per age group.\",\"Sort the aggregated data by average spend in descending order.\",\"Apply a log transformation to the average spend to stabilize variance.\",\"Forecast future marketing spend using the Holt-Winters method for the next 12 months.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing_campaign_results.xlsx\",\"type\":\"xlsx\"},\"output\":\"marketing_campaign_results\",\"comment\":\"Loading marketing campaign results from Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_demographics\",\"query\":\"age >= 18 and age <= 35\"},\"output\":\"young_customers\",\"comment\":\"Filtering customers aged 18-35.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"marketing_campaign_results\",\"query\":\"success == True\"},\"output\":\"successful_campaigns\",\"comment\":\"Filtering successful marketing campaigns.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"young_customers\",\"successful_campaigns\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered customer demographics with successful campaign results.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"total_spend\",\"formula\":\"`campaign_cost`\"},\"output\":\"customer_spend\",\"comment\":\"Calculating total spend for each customer.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"customer_spend\",\"groupBy\":\"age\",\"aggregations\":[{\"column\":\"total_spend\",\"name\":\"average_spend\",\"function\":\"mean\"}]},\"output\":\"age_group_spend\",\"comment\":\"Aggregating data to find average spend per age group.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"age_group_spend\",\"sortBy\":\"average_spend\",\"order\":\"desc\"},\"output\":\"sorted_age_group_spend\",\"comment\":\"Sorting aggregated data by average spend in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_age_group_spend\",\"transformations\":[{\"columns\":[\"average_spend\"],\"operation\":\"log_transform\"}]},\"output\":\"log_transformed_spend\",\"comment\":\"Applying log transformation to average spend.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"log_transformed_spend\",\"columns\":[\"average_spend\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"age\"}},\"output\":\"forecasted_spend\",\"comment\":\"Forecasting future marketing spend using Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains the energy consumption data for the building, including date, energy consumption, and number of occupants.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"occupants\",\"column_type\":\"xsd:integer\",\"column_description\":\"The number of occupants in the building.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including date, temperature, and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity on the given date.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for the building, including dates and tasks.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"},{\"column\":\"col_2\",\"column_name\":\"task\",\"column_type\":\"xsd:string\",\"column_description\":\"The maintenance task to be performed.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only records from the year 2023.\",\"Filter the weather data to include only records from the year 2023.\",\"Join the filtered energy consumption data with the filtered weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing energy consumption by the number of occupants.\",\"Aggregate the joined data by month to calculate the total energy consumption and average temperature.\",\"Sort the aggregated data by total energy consumption in descending order.\",\"Apply a rolling mean transformation to the energy consumption column with a window size of 3.\",\"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only records from the year 2023.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"weather_data\",\"query\":\"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"},\"output\":\"filtered_weather_data\",\"comment\":\"Filtering the weather data to include only records from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"filtered_weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered energy consumption data with the filtered weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`energy_consumption` / `occupants`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating a new column for energy efficiency by dividing energy consumption by the number of occupants.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date.dt.month\",\"aggregations\":[{\"column\":\"energy_consumption\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating the joined data by month to calculate the total energy consumption and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total energy consumption in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_data\",\"transformations\":[{\"columns\":[\"energy_consumption\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":3}}]},\"output\":\"smoothed_data\",\"comment\":\"Applying a rolling mean transformation to the energy consumption column with a window size of 3.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"smoothed_data\",\"columns\":[\"energy_consumption\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow can a column be accessed using dictionary-style indexing?","output":"A column can be accessed using dictionary-style indexing, such as 'df[\"col1\"] + df[\"col2\"]'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_data\",\"location\":\"C:/hospital_data/patient_data.csv\",\"sheet_name\":null,\"label\":\"Patient Data\",\"description\":\"Contains demographic and personal information of patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the patient in kilograms.\"},{\"column\":\"col_4\",\"column_name\":\"height\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Height of the patient in meters.\"}]},\"table_id2\":{\"name\":\"medical_test_results\",\"location\":\"C:/lab_data/medical_test_results.xlsx\",\"sheet_name\":\"Results\",\"label\":\"Medical Test Results\",\"description\":\"Contains results of various medical tests conducted on patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"test_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medical test.\"},{\"column\":\"col_3\",\"column_name\":\"test_result\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Result of the medical test.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/hospital_data/staff_data.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Contains information about hospital staff members.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the staff member works.\"}]}},\"instructions\":[\"Load the patient data table from the hospital database.\",\"Load the medical test results table from the laboratory system.\",\"Filter the patient data to include only patients over the age of 50.\",\"Join the filtered patient data with the medical test results on patient ID.\",\"Calculate the Body Mass Index (BMI) for each patient using weight and height columns.\",\"Aggregate the joined data to find the average BMI per age group.\",\"Sort the aggregated data by average BMI in descending order.\",\"Identify patients with a BMI greater than 30 as potential candidates for a weight management program.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/patient_data.csv\",\"type\":\"csv\"},\"output\":\"patient_data\",\"comment\":\"Loading patient data from the hospital database.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/lab_data/medical_test_results.xlsx\",\"type\":\"xlsx\"},\"output\":\"medical_test_results\",\"comment\":\"Loading medical test results from the laboratory system.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_data\",\"query\":\"age > 50\"},\"output\":\"filtered_patient_data\",\"comment\":\"Filtering patient data to include only patients over the age of 50.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_patient_data\",\"medical_test_results\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient data with medical test results on patient ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"BMI\",\"formula\":\"weight / (height ** 2)\"},\"output\":\"data_with_bmi\",\"comment\":\"Calculating BMI for each patient using weight and height columns.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_bmi\",\"groupBy\":\"age\",\"aggregations\":[{\"column\":\"BMI\",\"name\":\"average_bmi\",\"function\":\"mean\"}]},\"output\":\"average_bmi_per_age_group\",\"comment\":\"Aggregating data to find the average BMI per age group.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_bmi_per_age_group\",\"sortBy\":\"average_bmi\",\"order\":\"desc\"},\"output\":\"sorted_bmi_data\",\"comment\":\"Sorting the aggregated data by average BMI in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"data_with_bmi\",\"query\":\"BMI > 30\"},\"output\":\"potential_weight_management_candidates\",\"comment\":\"Identifying patients with a BMI greater than 30 as potential candidates for a weight management program.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of shipments including shipment ID, weight, and destination.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"Destination of the shipment.\"}]},\"table_id2\":{\"name\":\"delivery_schedule\",\"location\":\"C:/data/logistics/delivery_schedule.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Delivery Schedule\",\"description\":\"Contains delivery schedules including shipment ID, departure time, and arrival time.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"departure_time\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Scheduled departure time of the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"arrival_time\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"Scheduled arrival time of the shipment.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/logistics/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"Contains inventory details including item ID, quantity, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"col_2\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the inventory item.\"},{\"column\":\"col_3\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the inventory item.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at C:/data/logistics/shipment_data.csv.\",\"Load the 'delivery_schedule' table from the Excel file located at C:/data/logistics/delivery_schedule.xlsx.\",\"Filter the 'shipment_data' table to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered 'shipment_data' with the 'delivery_schedule' on the 'shipment_id' column using an inner join.\",\"Calculate a new column 'delivery_time_hours' in the joined table by subtracting 'departure_time' from 'arrival_time' and converting the result to hours.\",\"Aggregate the joined table by 'destination' to calculate the total weight of shipments for each destination.\",\"Sort the aggregated data by 'total_weight' in descending order.\",\"Apply a log transformation to the 'total_weight' column to stabilize variance.\",\"Forecast the 'total_weight' for the next 7 days using the Holt-Winters method, considering daily frequency and a seasonal period of 7.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/delivery_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"delivery_schedule\",\"comment\":\"Loading delivery schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"filtered_shipment_data\",\"comment\":\"Filtering shipments with weight greater than 1000 kg.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipment_data\",\"delivery_schedule\"],\"joinOn\":[\"shipment_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with delivery schedule on shipment_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"delivery_time_hours\",\"formula\":\"(`arrival_time` - `departure_time`).total_seconds() / 3600\"},\"output\":\"joined_data_with_delivery_time\",\"comment\":\"Calculating delivery time in hours by subtracting departure time from arrival time.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_delivery_time\",\"groupBy\":\"destination\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by destination to calculate total weight of shipments.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_weight\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total weight in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_data\",\"transformations\":[{\"columns\":[\"total_weight\"],\"operation\":\"log_transform\",\"parameters\":{}}]},\"output\":\"transformed_data\",\"comment\":\"Applying log transformation to the total weight column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_data\",\"columns\":[\"total_weight\"],\"forecastParameters\":{\"forecastPeriod\":7,\"frequency\":\"D\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"dateColumn\":\"destination\"}},\"output\":\"forecast_results\",\"comment\":\"Forecasting total weight for the next 7 days using Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials_inventory\",\"location\":\"C:/manufacturing_data/raw_materials_inventory.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Inventory\",\"description\":\"Contains information about available raw materials including material ID, name, and current stock levels.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"Material ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"material_name\",\"column_name\":\"Material Name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the raw material.\"},{\"column\":\"current_stock\",\"column_name\":\"Current Stock\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/manufacturing_data/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"Details the production schedule including order ID, material ID, and required quantity.\",\"columns\":[{\"column\":\"order_id\",\"column_name\":\"Order ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each production order.\"},{\"column\":\"material_id\",\"column_name\":\"Material ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material required for the order.\"},{\"column\":\"required_quantity\",\"column_name\":\"Required Quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material required for the order.\"},{\"column\":\"status\",\"column_name\":\"Status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the production order.\"}]},\"table_id3\":{\"name\":\"supplier_information\",\"location\":\"C:/manufacturing_data/supplier_information.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains details about suppliers including supplier ID, name, and material costs.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"Supplier ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"supplier_name\",\"column_name\":\"Supplier Name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"},{\"column\":\"material_id\",\"column_name\":\"Material ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material supplied.\"},{\"column\":\"material_cost\",\"column_name\":\"Material Cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of the material supplied.\"}]}},\"instructions\":[\"Load the raw materials inventory table from the CSV file.\",\"Load the production schedule from the Excel file.\",\"Filter the production schedule to include only active orders.\",\"Join the filtered production schedule with the raw materials inventory on material ID.\",\"Calculate the total required quantity for each material by summing up the quantities from the joined table.\",\"Load the supplier information table from the CSV file.\",\"Join the total required materials with supplier information on supplier ID.\",\"Aggregate the data to find the total cost of materials by supplier.\",\"Sort the aggregated data by total cost in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/manufacturing_data/raw_materials_inventory.csv\",\"type\":\"csv\"},\"output\":\"raw_materials_inventory\",\"comment\":\"Loading the raw materials inventory from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/manufacturing_data/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"`status` == 'active'\"},\"output\":\"active_production_schedule\",\"comment\":\"Filtering the production schedule to include only active orders.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"active_production_schedule\",\"raw_materials_inventory\"],\"joinOn\":[\"material_id\"],\"joinType\":\"inner\"},\"output\":\"joined_materials_schedule\",\"comment\":\"Joining active production schedule with raw materials inventory on material ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_materials_schedule\",\"groupBy\":\"material_id\",\"aggregations\":[{\"column\":\"required_quantity\",\"name\":\"total_required_quantity\",\"function\":\"sum\"}]},\"output\":\"total_required_materials\",\"comment\":\"Calculating the total required quantity for each material.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/manufacturing_data/supplier_information.csv\",\"type\":\"csv\"},\"output\":\"supplier_information\",\"comment\":\"Loading the supplier information from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"total_required_materials\",\"supplier_information\"],\"joinOn\":[\"material_id\"],\"joinType\":\"inner\"},\"output\":\"materials_with_suppliers\",\"comment\":\"Joining total required materials with supplier information on material ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"materials_with_suppliers\",\"groupBy\":\"supplier_id\",\"aggregations\":[{\"column\":\"material_cost\",\"name\":\"total_material_cost\",\"function\":\"sum\"}]},\"output\":\"total_cost_by_supplier\",\"comment\":\"Aggregating data to find the total cost of materials by supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"total_cost_by_supplier\",\"sortBy\":\"total_material_cost\",\"order\":\"desc\"},\"output\":\"sorted_cost_by_supplier\",\"comment\":\"Sorting the aggregated data by total cost in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Inventory\",\"description\":\"Contains information about raw materials, including stock levels and reorder thresholds.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"Material ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"stock_level\",\"column_name\":\"Stock Level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the material.\"},{\"column\":\"reorder_threshold\",\"column_name\":\"Reorder Threshold\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Stock level at which the material should be reordered.\"},{\"column\":\"unit_price\",\"column_name\":\"Unit Price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price per unit of the material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"Details the production schedule, including material requirements and priorities.\",\"columns\":[{\"column\":\"schedule_id\",\"column_name\":\"Schedule ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each production schedule entry.\"},{\"column\":\"material_id\",\"column_name\":\"Material ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material required in the schedule.\"},{\"column\":\"quantity_needed\",\"column_name\":\"Quantity Needed\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount of material required for production.\"},{\"column\":\"priority\",\"column_name\":\"Priority\",\"column_type\":\"xsd:integer\",\"column_description\":\"Priority level of the production schedule.\"}]},\"table_id3\":{\"name\":\"historical_consumption\",\"location\":\"C:/data/manufacturing/historical_consumption.csv\",\"sheet_name\":null,\"label\":\"Historical Material Consumption\",\"description\":\"Records past consumption of materials over time.\",\"columns\":[{\"column\":\"date\",\"column_name\":\"Date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of material consumption.\"},{\"column\":\"material_id\",\"column_name\":\"Material ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the consumed material.\"},{\"column\":\"quantity_consumed\",\"column_name\":\"Quantity Consumed\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount of material consumed on the given date.\"}]}},\"instructions\":[\"Load the raw materials data from the CSV file.\",\"Load the production schedule from the Excel file.\",\"Filter the raw materials data to include only materials with a stock level below the reorder threshold.\",\"Join the filtered raw materials data with the production schedule to identify urgent material needs.\",\"Aggregate the joined data to calculate the total quantity needed for each material.\",\"Sort the aggregated data by material priority in descending order.\",\"Calculate the cost of ordering additional materials based on the unit price and quantity needed.\",\"Apply a transformation to standardize the cost data for comparison.\",\"Forecast the future material needs using the production schedule and historical consumption data.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading raw materials data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading production schedule data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"`Stock Level` < `Reorder Threshold`\"},\"output\":\"low_stock_materials\",\"comment\":\"Filtering raw materials to include only those with stock levels below the reorder threshold.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_stock_materials\",\"production_schedule\"],\"joinOn\":[\"Material ID\"],\"joinType\":\"inner\"},\"output\":\"urgent_material_needs\",\"comment\":\"Joining filtered raw materials with the production schedule to identify urgent material needs.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"urgent_material_needs\",\"groupBy\":\"Material ID\",\"aggregations\":[{\"column\":\"Quantity Needed\",\"name\":\"Total Quantity Needed\",\"function\":\"sum\"}]},\"output\":\"aggregated_material_needs\",\"comment\":\"Aggregating data to calculate the total quantity needed for each material.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_material_needs\",\"sortBy\":\"Priority\",\"order\":\"desc\"},\"output\":\"sorted_material_needs\",\"comment\":\"Sorting aggregated data by material priority in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_material_needs\",\"columnName\":\"Order Cost\",\"formula\":\"`Total Quantity Needed` * `Unit Price`\"},\"output\":\"material_costs\",\"comment\":\"Calculating the cost of ordering additional materials based on unit price and quantity needed.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"material_costs\",\"transformations\":[{\"columns\":[\"`Order Cost`\"],\"operation\":\"standardize\",\"parameters\":{}}]},\"output\":\"standardized_costs\",\"comment\":\"Standardizing the cost data for comparison.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"historical_consumption\",\"columns\":[\"Quantity Consumed\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"Date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"future_material_needs\",\"comment\":\"Forecasting future material needs using the production schedule and historical consumption data.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials\",\"description\":\"A table containing details of raw materials used in the manufacturing process, including material ID, name, and quantity.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"material_name\",\"column_name\":\"MATERIAL_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the material.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the material available.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"A table containing the schedule for manufacturing processes, including material ID and production line.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"production_line\",\"column_name\":\"PRODUCTION_LINE\",\"column_type\":\"xsd:string\",\"column_description\":\"The production line where the material is used.\"},{\"column\":\"scheduled_date\",\"column_name\":\"SCHEDULED_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the material is scheduled for use.\"}]},\"table_id3\":{\"name\":\"inventory\",\"location\":\"C:/data/manufacturing/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory\",\"description\":\"A table containing inventory details, not relevant to the current workflow.\",\"columns\":[{\"column\":\"inventory_id\",\"column_name\":\"INVENTORY_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"item_name\",\"column_name\":\"ITEM_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the inventory item.\"},{\"column\":\"stock_level\",\"column_name\":\"STOCK_LEVEL\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the item.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains details of materials used in manufacturing.\",\"Filter the 'raw_materials' table to include only materials with a quantity greater than 100.\",\"Load the table 'production_schedule' which contains the schedule for manufacturing processes.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Aggregate the joined table to calculate the total quantity of materials required for each production line.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"QUANTITY > 100\"},\"output\":\"filtered_raw_materials\",\"comment\":\"Filtering raw materials to include only those with a quantity greater than 100.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule dataset from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_raw_materials\",\"production_schedule\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered raw materials with the production schedule on material ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_table\",\"groupBy\":\"PRODUCTION_LINE\",\"aggregations\":[{\"column\":\"QUANTITY\",\"name\":\"total_quantity\",\"function\":\"sum\"}]},\"output\":\"aggregated_materials\",\"comment\":\"Aggregating the joined table to calculate the total quantity of materials required for each production line.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'rolling_skew' operation do?","output":"The 'rolling_skew' operation computes rolling skewness over a given window. It requires 'parameters.window'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'lag' transformation.","output":"{ \"name\": \"applyTimeSeriesTransformations\", \"parameters\": { \"table\": \"sales_data\", \"transformations\": [ { \"columns\": [\"sales\"], \"operation\": \"lag\", \"parameters\": { \"periods\": 1 } } ] }, \"output\": \"lagged_sales\" }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"This table contains customer feedback including ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Customer feedback comments.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.csv\",\"sheet_name\":null,\"label\":\"Customer Details Data\",\"description\":\"This table contains detailed information about customers including their region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales Data\",\"description\":\"This table contains sales data for various products.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file located at C:/data/customer_feedback.csv.\",\"Filter the feedback data to include only entries with a rating of 4 or higher.\",\"Join the filtered feedback data with the customer details table on the customer_id column.\",\"Aggregate the joined data by region to calculate the average rating.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback to include only entries with a rating of 4 or higher.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.csv\",\"type\":\"csv\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"feedback_with_details\",\"comment\":\"Joining filtered feedback with customer details on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"feedback_with_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_by_region\",\"comment\":\"Aggregating data by region to calculate the average rating.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhen should the 'lag' operation be used?","output":"Use 'lag' when you need to shift time series values by a certain period, such as comparing today's stock price with the previous day's price."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains demographic information of patients including age, gender, weight, and height.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the patient.\"},{\"column\":\"col_4\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the patient in kilograms.\"},{\"column\":\"col_5\",\"column_name\":\"height\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Height of the patient in meters.\"}]},\"table_id2\":{\"name\":\"medical_test_results\",\"location\":\"C:/data/medical_test_results.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Medical Test Results\",\"description\":\"Contains results of various medical tests conducted on patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"test_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medical test.\"},{\"column\":\"col_3\",\"column_name\":\"test_result\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Result of the medical test.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Contains information about hospital staff members.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"position\",\"column_type\":\"xsd:string\",\"column_description\":\"Position of the staff member in the hospital.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file.\",\"Load the medical test results table from the Excel file.\",\"Filter the patient records to include only those over 60 years old.\",\"Join the filtered patient records with the medical test results on patient ID.\",\"Calculate the BMI for each patient using weight and height columns.\",\"Aggregate the data to find the average BMI by gender.\",\"Sort the aggregated data by average BMI in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient demographic information from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/medical_test_results.xlsx\",\"type\":\"xlsx\"},\"output\":\"medical_test_results\",\"comment\":\"Loading medical test results from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"age > 60\"},\"output\":\"filtered_patient_records\",\"comment\":\"Filtering patient records to include only those over 60 years old.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_patient_records\",\"medical_test_results\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered patient records with medical test results on patient ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"BMI\",\"formula\":\"weight / (height ** 2)\"},\"output\":\"data_with_bmi\",\"comment\":\"Calculating BMI for each patient using weight and height columns.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_bmi\",\"groupBy\":\"gender\",\"aggregations\":[{\"column\":\"BMI\",\"name\":\"average_BMI\",\"function\":\"mean\"}]},\"output\":\"average_bmi_by_gender\",\"comment\":\"Aggregating data to find the average BMI by gender.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_bmi_by_gender\",\"sortBy\":\"average_BMI\",\"order\":\"desc\"},\"output\":\"sorted_bmi_data\",\"comment\":\"Sorting the aggregated data by average BMI in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains information about shipments including weight, route, and destination.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"route_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the route taken by the shipment.\"},{\"column\":\"col_4\",\"column_name\":\"destination\",\"column_type\":\"xsd:string\",\"column_description\":\"Destination of the shipment.\"}]},\"table_id2\":{\"name\":\"route_data\",\"location\":\"C:/data/logistics/route_data.csv\",\"sheet_name\":null,\"label\":\"Route Data\",\"description\":\"Contains information about routes including route ID and details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"route_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each route.\"},{\"column\":\"col_2\",\"column_name\":\"route_details\",\"column_type\":\"xsd:string\",\"column_description\":\"Details about the route.\"}]},\"table_id3\":{\"name\":\"vehicle_data\",\"location\":\"C:/data/logistics/vehicle_data.csv\",\"sheet_name\":null,\"label\":\"Vehicle Data\",\"description\":\"Contains information about vehicles used for shipments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"vehicle_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each vehicle.\"},{\"column\":\"col_2\",\"column_name\":\"capacity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Capacity of the vehicle in kilograms.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at 'C:/data/logistics/shipment_data.csv'.\",\"Filter the 'shipment_data' table to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered 'shipment_data' with the 'route_data' table on the 'route_id' column using an inner join.\",\"Aggregate the joined table by 'destination' to calculate the total weight of shipments for each destination.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"filtered_shipment_data\",\"comment\":\"Filtering shipments to include only those with a weight greater than 1000 kg.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/route_data.csv\",\"type\":\"csv\"},\"output\":\"route_data\",\"comment\":\"Loading the route data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipment_data\",\"route_data\"],\"joinOn\":[\"route_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with route data on route_id using an inner join.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"destination\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined data by destination to calculate the total weight of shipments.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customers\",\"location\":\"C:/data/marketing/customers.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"This table contains customer information including their lifetime value and purchase history.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"lifetime_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total value of purchases made by the customer.\"},{\"column\":\"col_3\",\"column_name\":\"last_purchase_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the customer's most recent purchase.\"}]},\"table_id2\":{\"name\":\"campaigns\",\"location\":\"C:/data/marketing/campaigns.xlsx\",\"sheet_name\":\"Campaigns\",\"label\":\"Marketing Campaigns\",\"description\":\"Contains details of various marketing campaigns.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_2\",\"column_name\":\"campaign_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the marketing campaign.\"}]}},\"instructions\":[\"Load the customer data from the CSV file located at C:/data/marketing/customers.csv.\",\"Filter the customer data to include only those with a lifetime value greater than 1000.\",\"Sort the filtered customer data by last purchase date in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customers.csv\",\"type\":\"csv\"},\"output\":\"customers\",\"comment\":\"Loading the customer data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customers\",\"query\":\"`lifetime_value` > 1000\"},\"output\":\"high_value_customers\",\"comment\":\"Filtering customers with a lifetime value greater than 1000.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"high_value_customers\",\"sortBy\":\"last_purchase_date\",\"order\":\"desc\"},\"output\":\"sorted_customers\",\"comment\":\"Sorting the filtered customers by last purchase date in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains the manufacturing schedule with details on tasks, products, and scheduled times.\",\"columns\":[{\"column\":\"task_id\",\"column_name\":\"TASK_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each manufacturing task.\"},{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product being manufactured.\"},{\"column\":\"scheduled_date\",\"column_name\":\"SCHEDULED_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date on which the task is scheduled.\"},{\"column\":\"production_time\",\"column_name\":\"PRODUCTION_TIME\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Time required to complete the task in hours.\"}]},\"table_id2\":{\"name\":\"inventory_status\",\"location\":\"C:/data/manufacturing/inventory_status.xlsx\",\"sheet_name\":\"Current\",\"label\":\"Inventory Status\",\"description\":\"This table provides the current status of inventory items.\",\"columns\":[{\"column\":\"item_id\",\"column_name\":\"ITEM_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:integer\",\"column_description\":\"Current quantity of the inventory item.\"}]}},\"instructions\":[\"Load the table 'production_schedule' which contains the manufacturing schedule.\",\"Filter the production schedule to include only tasks scheduled for the current week.\",\"Aggregate the filtered schedule to calculate the total production time required for each product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the manufacturing schedule from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"`SCHEDULED_DATE` >= '2023-10-02' and `SCHEDULED_DATE` <= '2023-10-08'\"},\"output\":\"current_week_schedule\",\"comment\":\"Filtering the production schedule to include only tasks scheduled for the current week.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"current_week_schedule\",\"groupBy\":\"PRODUCT_ID\",\"aggregations\":[{\"column\":\"PRODUCTION_TIME\",\"name\":\"total_production_time\",\"function\":\"sum\"}]},\"output\":\"weekly_production_summary\",\"comment\":\"Aggregating the filtered schedule to calculate the total production time required for each product.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"Table containing financial transactions with details like transaction amount, currency code, and account ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"transaction_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"Currency code of the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"account_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the account associated with the transaction.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/exchange_rates.xlsx\",\"sheet_name\":\"Rates\",\"label\":\"Exchange Rates\",\"description\":\"Table containing exchange rates for various currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"Currency code.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at C:/data/financial_transactions.csv.\",\"Load the 'exchange_rates' table from the Excel file located at C:/data/exchange_rates.xlsx.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Join the filtered transactions with the 'exchange_rates' table on the 'currency_code' column using an inner join.\",\"Calculate a new column 'amount_in_usd' by multiplying 'transaction_amount' with 'exchange_rate'.\",\"Aggregate the joined table by 'account_id' to calculate the total 'amount_in_usd' for each account.\",\"Sort the aggregated data by 'total_amount_usd' in descending order.\",\"Apply a log transformation to the 'total_amount_usd' column for variance stabilization.\",\"Forecast the 'total_amount_usd' for the next 12 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"transaction_amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered transactions with exchange rates on currency code.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_table\",\"columnName\":\"amount_in_usd\",\"formula\":\"transaction_amount * exchange_rate\"},\"output\":\"transactions_with_usd\",\"comment\":\"Calculating amount in USD by multiplying transaction amount with exchange rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_usd\",\"groupBy\":\"account_id\",\"aggregations\":[{\"column\":\"amount_in_usd\",\"name\":\"total_amount_usd\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by account ID to calculate total amount in USD.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_amount_usd\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total amount in USD in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_data\",\"transformations\":[{\"columns\":[\"total_amount_usd\"],\"operation\":\"log_transform\"}]},\"output\":\"log_transformed_data\",\"comment\":\"Applying log transformation to total amount in USD for variance stabilization.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"log_transformed_data\",\"columns\":[\"total_amount_usd\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"account_id\"}},\"output\":\"forecast_results\",\"comment\":\"Forecasting total amount in USD for the next 12 months using Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'interpolate' transformation.","output":"{ \"name\": \"applyTransformations\", \"parameters\": { \"table\": \"temperature_data\", \"transformations\": [ { \"columns\": [\"temperature\"], \"operation\": \"interpolate\", \"parameters\": { \"method\": \"linear\" } } ] }, \"output\": \"filled_temperature\" }"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid function call using string conversion.","output":"'str(col1)' // Error: 'str' is not a supported function in Tamarind expressions."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid attribute access expression.","output":"'df.123column' // Error: Column names must be valid Python identifiers."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/marketing/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"This table contains demographic information about customers, including age, income, and city.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"income\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual income of the customer.\"},{\"column\":\"col_4\",\"column_name\":\"city\",\"column_type\":\"xsd:string\",\"column_description\":\"City where the customer resides.\"}]},\"table_id2\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing/marketing_campaigns.xlsx\",\"sheet_name\":\"Campaigns\",\"label\":\"Marketing Campaigns\",\"description\":\"Details of various marketing campaigns including budget and target audience.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_B\",\"column_name\":\"budget\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Budget allocated for the campaign.\"},{\"column\":\"col_C\",\"column_name\":\"target_audience\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the target audience for the campaign.\"}]}},\"instructions\":[\"Load the customer demographics data from the CSV file.\",\"Filter the data to include only customers aged between 25 and 40.\",\"Aggregate the filtered data to find the average income by city.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading the customer demographics data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_demographics\",\"query\":\"age >= 25 and age <= 40\"},\"output\":\"filtered_customers\",\"comment\":\"Filtering the data to include only customers aged between 25 and 40.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_customers\",\"groupBy\":\"city\",\"aggregations\":[{\"column\":\"income\",\"name\":\"average_income\",\"function\":\"mean\"}]},\"output\":\"average_income_by_city\",\"comment\":\"Aggregating the filtered data to find the average income by city.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid Boolean expression due to incorrect syntax.","output":"'col1 and > 5' // Error: Incorrect syntax, 'and' must be between two valid expressions."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information of customers including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"marketing_campaign_results\",\"location\":\"C:/data/marketing_campaign_results.xlsx\",\"sheet_name\":\"Results\",\"label\":\"Marketing Campaign Results\",\"description\":\"Contains results of various marketing campaigns including success status and customer ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"success\",\"column_type\":\"xsd:boolean\",\"column_description\":\"Indicates whether the campaign was successful.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not related to the marketing workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"data_point\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant data point.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the marketing campaign results table from the Excel file.\",\"Filter the marketing campaign results to include only successful campaigns.\",\"Join the filtered marketing campaign results with the customer demographics table on customer ID.\",\"Aggregate the joined table to calculate the total number of successful campaigns per age group.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing_campaign_results.xlsx\",\"type\":\"xlsx\"},\"output\":\"marketing_campaign_results\",\"comment\":\"Loading marketing campaign results from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"marketing_campaign_results\",\"query\":\"success == True\"},\"output\":\"successful_campaigns\",\"comment\":\"Filtering marketing campaign results to include only successful campaigns.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"successful_campaigns\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining successful campaigns with customer demographics on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"age\",\"aggregations\":[{\"column\":\"campaign_id\",\"name\":\"total_successful_campaigns\",\"function\":\"count\"}]},\"output\":\"age_group_campaigns\",\"comment\":\"Aggregating data to calculate the total number of successful campaigns per age group.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/hospital_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains demographic and medical information for patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"medication\",\"location\":\"C:/hospital_data/medication.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Contains information about medications prescribed to patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/hospital_data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Contains information about hospital staff members.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"}]}},\"instructions\":[\"Load the patient records table from the hospital database.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Join the filtered patient records with the medication table on patient ID.\",\"Calculate the average age of patients with diabetes.\",\"Sort the resulting table by patient age in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetes_patients\",\"comment\":\"Filtering patient records to include only those with a diagnosis of diabetes.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/medication.csv\",\"type\":\"csv\"},\"output\":\"medication\",\"comment\":\"Loading the medication table from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"diabetes_patients\",\"medication\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"diabetes_patients_with_medication\",\"comment\":\"Joining filtered patient records with medication table on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"diabetes_patients_with_medication\",\"groupBy\":null,\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"average_age_diabetes_patients\",\"comment\":\"Calculating the average age of patients with diabetes.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_age_diabetes_patients\",\"sortBy\":\"age\",\"order\":\"desc\"},\"output\":\"sorted_diabetes_patients\",\"comment\":\"Sorting the resulting table by patient age in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'decompose' transformation.","output":"{ \"name\": \"applyTimeSeriesTransformations\", \"parameters\": { \"table\": \"energy_consumption\", \"transformations\": [ { \"columns\": [\"usage\"], \"operation\": \"decompose\", \"parameters\": { \"period\": 12 } } ] }, \"output\": \"decomposed_energy\" }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with details such as amount, currency, and type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code of the transaction amount.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of transaction, e.g., 'purchase', 'refund'.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/exchange_rates.xlsx\",\"sheet_name\":\"Rates\",\"label\":\"Exchange Rates\",\"description\":\"A table containing exchange rates for various currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Irrelevant Data\",\"description\":\"A table containing data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"An identifier for the data.\"},{\"column\":\"col_B\",\"column_name\":\"value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Some value associated with the data.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at 'C:/data/financial_transactions.csv'.\",\"Load the 'exchange_rates' table from the Excel file located at 'C:/data/exchange_rates.xlsx'.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Join the filtered 'financial_transactions' table with the 'exchange_rates' table on the 'currency_code' column.\",\"Calculate a new column 'amount_in_usd' by multiplying 'amount' with 'exchange_rate' in the joined table.\",\"Aggregate the joined table by 'transaction_type' to calculate the total 'amount_in_usd' for each type.\",\"Sort the aggregated data by 'total_amount_usd' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with exchange rates on currency code.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"amount_in_usd\",\"formula\":\"amount * exchange_rate\"},\"output\":\"transactions_with_usd\",\"comment\":\"Calculating the amount in USD by multiplying amount with exchange rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_usd\",\"groupBy\":\"transaction_type\",\"aggregations\":[{\"column\":\"amount_in_usd\",\"name\":\"total_amount_usd\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by transaction type to calculate total amount in USD.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_amount_usd\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total amount in USD in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comment\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comment from the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers including their region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"col_3\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.csv\",\"sheet_name\":null,\"label\":\"Product Catalog\",\"description\":\"Table containing information about products offered by the company.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback to include only entries with a rating below 3.\",\"Load the customer details table from the Excel file.\",\"Join the filtered feedback with customer details using the customer ID.\",\"Aggregate the data to find the average rating per region.\",\"Sort the aggregated data by average rating in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating < 3\"},\"output\":\"low_rating_feedback\",\"comment\":\"Filtering feedback to include only entries with a rating below 3.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_rating_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"feedback_with_details\",\"comment\":\"Joining filtered feedback with customer details using the customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"feedback_with_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_by_region\",\"comment\":\"Aggregating data to find the average rating per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_rating_by_region\",\"sortBy\":\"average_rating\",\"order\":\"asc\"},\"output\":\"sorted_average_rating_by_region\",\"comment\":\"Sorting the aggregated data by average rating in ascending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/finance/transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing details of financial transactions including amount, currency, and type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code of the transaction amount.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of transaction, e.g., purchase, refund.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/finance/exchange_rates.xlsx\",\"sheet_name\":\"Rates\",\"label\":\"Exchange Rates\",\"description\":\"A table containing exchange rates for various currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_financial_data\",\"location\":\"C:/data/finance/irrelevant_data.xlsx\",\"sheet_name\":\"Irrelevant\",\"label\":\"Irrelevant Financial Data\",\"description\":\"A table containing financial data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"irrelevant_column_1\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"},{\"column\":\"col_B\",\"column_name\":\"irrelevant_column_2\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Another irrelevant column.\"}]}},\"instructions\":[\"Load the table 'financial_transactions' from the CSV file located at C:/data/finance/transactions.csv.\",\"Load the table 'exchange_rates' from the Excel file located at C:/data/finance/exchange_rates.xlsx.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Join the filtered transactions with the 'exchange_rates' table on the 'currency_code' column.\",\"Calculate a new column 'amount_in_usd' by multiplying 'amount' with 'exchange_rate'.\",\"Aggregate the joined table by 'transaction_type' to calculate the total 'amount_in_usd' for each type.\",\"Sort the aggregated data by 'total_amount_in_usd' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"joined_transactions\",\"comment\":\"Joining filtered transactions with exchange rates on currency code.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_transactions\",\"columnName\":\"amount_in_usd\",\"formula\":\"amount * exchange_rate\"},\"output\":\"transactions_with_usd\",\"comment\":\"Calculating the amount in USD by multiplying amount with exchange rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_usd\",\"groupBy\":\"transaction_type\",\"aggregations\":[{\"column\":\"amount_in_usd\",\"name\":\"total_amount_in_usd\",\"function\":\"sum\"}]},\"output\":\"aggregated_transactions\",\"comment\":\"Aggregating data by transaction type to calculate total amount in USD.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_transactions\",\"sortBy\":\"total_amount_in_usd\",\"order\":\"desc\"},\"output\":\"sorted_transactions\",\"comment\":\"Sorting the aggregated data by total amount in USD in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating on a scale of 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comments provided by the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers including their region.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"column_3_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.csv\",\"sheet_name\":null,\"label\":\"Product Catalog\",\"description\":\"Table containing information about products, not relevant for this workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback to include only those with a rating below 3.\",\"Join the filtered feedback with the customer details table on customer ID.\",\"Aggregate the joined table to find the average rating per region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading the customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating < 3\"},\"output\":\"low_rating_feedback\",\"comment\":\"Filtering feedback to include only those with a rating below 3.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading the customer details data from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_rating_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_feedback_details\",\"comment\":\"Joining filtered feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_feedback_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_per_region\",\"comment\":\"Aggregating the joined table to find the average rating per region.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"A table containing patient information including age, diagnosis, and other medical details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"Patient_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"Age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"Diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_staff\",\"location\":\"C:/data/hospital_staff.xlsx\",\"sheet_name\":\"Staff\",\"label\":\"Hospital Staff\",\"description\":\"A table containing information about hospital staff members.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"Staff_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"Name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"Position\",\"column_type\":\"xsd:string\",\"column_description\":\"Job position of the staff member.\"}]}},\"instructions\":[\"Load the patient records table from the CSV file located at C:/data/patient_records.csv.\",\"Filter the patient records to include only those with a diagnosis of 'Hypertension'.\",\"Aggregate the filtered records to find the average age of patients with Hypertension.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"`Diagnosis` == 'Hypertension'\"},\"output\":\"hypertension_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with Hypertension.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"hypertension_patients\",\"groupBy\":null,\"aggregations\":[{\"column\":\"Age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"average_age_hypertension\",\"comment\":\"Calculating the average age of patients with Hypertension.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid column name usage.","output":"'a column name + col2' // Error: Column names with spaces must be enclosed in backticks."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid column name using single quotes.","output":"'a column name' + col2' // Error: Column names must be enclosed in backticks (`), not single quotes."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains daily energy consumption data with timestamps and usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"}]},\"table_2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains daily weather data including temperature and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature of the day in Celsius.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity of the day in percentage.\"}]},\"table_3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data not relevant to the energy management workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column for this workflow.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the daily average energy consumption.\",\"Identify peak consumption days by filtering days with energy usage above 1000 kWh.\",\"Join the peak consumption data with weather data to analyze the impact of weather on energy usage.\",\"Aggregate the joined data to find the average temperature on peak consumption days.\",\"Sort the aggregated data by average temperature in descending order.\",\"Forecast future energy consumption for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"energy_2023\",\"comment\":\"Filtering the energy consumption data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_2023\",\"groupBy\":\"timestamp.dt.date\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"daily_avg_usage\",\"function\":\"mean\"}]},\"output\":\"daily_avg_energy\",\"comment\":\"Calculating the daily average energy consumption.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"daily_avg_energy\",\"query\":\"daily_avg_usage > 1000\"},\"output\":\"peak_days\",\"comment\":\"Identifying peak consumption days with energy usage above 1000 kWh.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.csv\",\"type\":\"csv\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"peak_days\",\"weather_data\"],\"joinOn\":[\"timestamp.dt.date\",\"date\"],\"joinType\":\"inner\"},\"output\":\"peak_weather\",\"comment\":\"Joining peak consumption data with weather data to analyze the impact of weather on energy usage.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"peak_weather\",\"groupBy\":null,\"aggregations\":[{\"column\":\"temperature\",\"name\":\"avg_temp_on_peak_days\",\"function\":\"mean\"}]},\"output\":\"avg_temp_peak_days\",\"comment\":\"Aggregating the joined data to find the average temperature on peak consumption days.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"avg_temp_peak_days\",\"sortBy\":\"avg_temp_on_peak_days\",\"order\":\"desc\"},\"output\":\"sorted_avg_temp\",\"comment\":\"Sorting the aggregated data by average temperature in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"energy_2023\",\"columns\":[\"usage\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"timestamp\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting future energy consumption for the next 30 days using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid math function using exponentiation.","output":"'exp(col1) - log10(col2)'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid chained comparison expression.","output":"'10 < col1 <= 50'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains hourly energy consumption data for various locations.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location where the energy consumption was recorded.\"},{\"column\":\"col_3\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains weather information such as temperature, humidity, and wind speed.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature recorded on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The humidity level recorded on the given date.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each month.\",\"Identify peak energy consumption days by filtering days with usage above a threshold.\",\"Forecast the next month's energy consumption using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"energy_2023\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_2023\",\"groupBy\":\"timestamp.dt.month\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_monthly_usage\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_usage\",\"comment\":\"Calculating the total energy consumption for each month.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_2023\",\"query\":\"`usage` > 1000\"},\"output\":\"peak_days\",\"comment\":\"Identifying peak energy consumption days by filtering days with usage above a threshold.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"monthly_energy_usage\",\"columns\":[\"total_monthly_usage\"],\"forecastParameters\":{\"forecastPeriod\":1,\"frequency\":\"MS\",\"dateColumn\":\"timestamp\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"confidenceInterval\":95}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the next month's energy consumption using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nAre math functions applied element-wise in Tamarind expressions?","output":"Yes, math functions are applied element-wise to table columns."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of shipments including weight, distance, and warehouse ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_3\",\"column_name\":\"distance\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Distance the shipment needs to travel in kilometers.\"},{\"column\":\"col_4\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse associated with the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_locations\",\"location\":\"C:/data/logistics/warehouse_locations.csv\",\"sheet_name\":null,\"label\":\"Warehouse Locations\",\"description\":\"Contains details of warehouse locations including warehouse ID and address.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"col_2\",\"column_name\":\"address\",\"column_type\":\"xsd:string\",\"column_description\":\"Physical address of the warehouse.\"}]},\"table_id3\":{\"name\":\"driver_data\",\"location\":\"C:/data/logistics/driver_data.csv\",\"sheet_name\":null,\"label\":\"Driver Data\",\"description\":\"Contains details of drivers including driver ID and assigned shipments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"driver_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each driver.\"},{\"column\":\"col_2\",\"column_name\":\"assigned_shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the shipment assigned to the driver.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains shipment details.\",\"Load the table 'warehouse_locations' which contains warehouse location details.\",\"Filter the 'shipment_data' table to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered shipment data with the 'warehouse_locations' table on the 'warehouse_id' column.\",\"Calculate a new column 'shipping_cost' in the joined table using the formula 'distance * weight * 0.05'.\",\"Aggregate the data by 'warehouse_id' to find the total shipping cost for each warehouse.\",\"Sort the aggregated data by 'total_shipping_cost' in descending order.\",\"Apply a log transformation to the 'total_shipping_cost' column to stabilize variance.\",\"Forecast the future shipping costs for the next 12 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouse_locations.csv\",\"type\":\"csv\"},\"output\":\"warehouse_locations\",\"comment\":\"Loading the warehouse location data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"filtered_shipment_data\",\"comment\":\"Filtering shipments to include only those with a weight greater than 1000 kg.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipment_data\",\"warehouse_locations\"],\"joinOn\":[\"warehouse_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with warehouse locations on warehouse_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"shipping_cost\",\"formula\":\"distance * weight * 0.05\"},\"output\":\"data_with_shipping_cost\",\"comment\":\"Calculating shipping cost using the formula 'distance * weight * 0.05'.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_shipping_cost\",\"groupBy\":\"warehouse_id\",\"aggregations\":[{\"column\":\"shipping_cost\",\"name\":\"total_shipping_cost\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by warehouse_id to find total shipping cost for each warehouse.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_shipping_cost\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total shipping cost in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"transformations\":[{\"columns\":[\"total_shipping_cost\"],\"operation\":\"log_transform\"}]},\"output\":\"log_transformed_data\",\"comment\":\"Applying a log transformation to the total shipping cost column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"log_transformed_data\",\"columns\":[\"total_shipping_cost\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"warehouse_id\"}},\"output\":\"forecasted_shipping_costs\",\"comment\":\"Forecasting future shipping costs for the next 12 months using Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/finance/transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transactions with details such as amount, currency, and transaction type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code of the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"transaction_type\",\"column_type\":\"xsd:string\",\"column_description\":\"The type of transaction, e.g., purchase, refund.\"}]},\"table_id2\":{\"name\":\"exchange_rates\",\"location\":\"C:/data/finance/exchange_rates.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Exchange Rates\",\"description\":\"A table containing exchange rates for various currencies against USD.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"currency_code\",\"column_type\":\"xsd:string\",\"column_description\":\"The currency code.\"},{\"column\":\"col_2\",\"column_name\":\"exchange_rate\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The exchange rate of the currency against USD.\"}]},\"table_id3\":{\"name\":\"irrelevant_financial_data\",\"location\":\"C:/data/finance/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Financial Data\",\"description\":\"A table containing financial data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the data.\"},{\"column\":\"col_B\",\"column_name\":\"value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Some financial value.\"}]}},\"instructions\":[\"Load the table 'financial_transactions' from the CSV file located at C:/data/finance/transactions.csv.\",\"Load the table 'exchange_rates' from the Excel file located at C:/data/finance/exchange_rates.xlsx.\",\"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\",\"Join the filtered transactions with the 'exchange_rates' table on the 'currency_code' column.\",\"Calculate a new column 'amount_in_usd' by multiplying 'amount' with 'exchange_rate' in the joined table.\",\"Aggregate the data by 'transaction_type' to calculate the total 'amount_in_usd' for each type.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/finance/exchange_rates.xlsx\",\"type\":\"xlsx\"},\"output\":\"exchange_rates\",\"comment\":\"Loading the exchange rates dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"`amount` > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"exchange_rates\"],\"joinOn\":[\"currency_code\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with exchange rates on currency code.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"amount_in_usd\",\"formula\":\"`amount` * `exchange_rate`\"},\"output\":\"transactions_with_usd\",\"comment\":\"Calculating amount in USD by multiplying amount with exchange rate.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"transactions_with_usd\",\"groupBy\":\"transaction_type\",\"aggregations\":[{\"column\":\"amount_in_usd\",\"name\":\"total_amount_usd\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by transaction type to calculate total amount in USD.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption records with timestamps and consumption values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy_prices.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains historical energy prices with timestamps.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The date and time of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kilowatt-hour.\"}]},\"table_id3\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains weather information with timestamps.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The date and time of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature recorded at the given time.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each month.\",\"Sort the monthly energy consumption data in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"energy_consumption_2023\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_consumption_2023\",\"groupBy\":\"timestamp.dt.month\",\"aggregations\":[{\"column\":\"consumption\",\"name\":\"total_monthly_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Calculating the total energy consumption for each month.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_energy_consumption\",\"sortBy\":\"total_monthly_consumption\",\"order\":\"desc\"},\"output\":\"sorted_monthly_energy_consumption\",\"comment\":\"Sorting the monthly energy consumption data in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption records with timestamps and usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location where the energy consumption was recorded.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.csv\",\"sheet_name\":null,\"label\":\"Weather Data\",\"description\":\"This table contains weather information including temperature and humidity.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature in degrees Celsius.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The humidity percentage.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at 'C:/data/energy_consumption.csv'.\",\"Filter the energy consumption data to include only records where the usage is greater than 100 kWh.\",\"Aggregate the filtered data to calculate the total energy usage per day.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"usage > 100\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the data to include only records where usage is greater than 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_consumption\",\"groupBy\":\"timestamp\",\"aggregations\":[{\"column\":\"usage\",\"name\":\"total_daily_usage\",\"function\":\"sum\"}]},\"output\":\"daily_energy_usage\",\"comment\":\"Aggregating the filtered data to calculate total energy usage per day.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"financial_transactions\",\"location\":\"C:/data/financial_transactions.csv\",\"sheet_name\":null,\"label\":\"Financial Transactions\",\"description\":\"A table containing financial transaction records including transaction ID, customer ID, amount, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer associated with the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"}]},\"table_id2\":{\"name\":\"customer_info\",\"location\":\"C:/data/customer_info.csv\",\"sheet_name\":null,\"label\":\"Customer Information\",\"description\":\"A table containing customer details including customer ID, name, and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Geographical region of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"A table containing data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_id\",\"column_type\":\"xsd:string\",\"column_description\":\"An identifier for irrelevant data.\"},{\"column\":\"col_2\",\"column_name\":\"irrelevant_info\",\"column_type\":\"xsd:string\",\"column_description\":\"Some irrelevant information.\"}]}},\"instructions\":[\"Load the 'financial_transactions' table from the CSV file located at 'C:/data/financial_transactions.csv'.\",\"Filter the 'financial_transactions' table to include only rows where the 'amount' is greater than 1000.\",\"Join the filtered 'financial_transactions' table with the 'customer_info' table on the 'customer_id' column.\",\"Aggregate the joined table by 'region', calculating the total 'amount' for each region.\",\"Sort the aggregated data by 'total_amount' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/financial_transactions.csv\",\"type\":\"csv\"},\"output\":\"financial_transactions\",\"comment\":\"Loading the financial transactions dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"financial_transactions\",\"query\":\"amount > 1000\"},\"output\":\"filtered_transactions\",\"comment\":\"Filtering transactions to include only those with an amount greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_info.csv\",\"type\":\"csv\"},\"output\":\"customer_info\",\"comment\":\"Loading the customer information dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_transactions\",\"customer_info\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered transactions with customer information on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_amount\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by region to calculate total transaction amounts.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_amount\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total amount in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'applyTransformations' action with an unsupported transformation.","output":"{ \"name\": \"applyTransformations\", \"parameters\": { \"table\": \"sales_data\", \"transformations\": [ { \"columns\": [\"revenue\"], \"operation\": \"unsupported_transformation\" } ] }, \"output\": \"transformed_sales\" } // Error: 'unsupported_transformation' is not a valid transformation operation."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"building_energy\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and energy usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"Timestamp of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Energy consumption in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Building Information\",\"description\":\"Contains information about buildings such as address, owner, and type.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"address\",\"column_type\":\"xsd:string\",\"column_description\":\"Address of the building.\"},{\"column\":\"col_3\",\"column_name\":\"owner\",\"column_type\":\"xsd:string\",\"column_description\":\"Owner of the building.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"data_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the data entry.\"},{\"column\":\"col_2\",\"column_name\":\"value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Some irrelevant value.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each building.\",\"Sort the buildings by total energy consumption in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"building_energy\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"building_energy\",\"query\":\"`timestamp`.dt.year == 2023\"},\"output\":\"filtered_energy_2023\",\"comment\":\"Filtering the data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_2023\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_kwh\",\"name\":\"total_energy\",\"function\":\"sum\"}]},\"output\":\"total_energy_per_building\",\"comment\":\"Calculating the total energy consumption for each building.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"total_energy_per_building\",\"sortBy\":\"total_energy\",\"order\":\"desc\"},\"output\":\"sorted_buildings\",\"comment\":\"Sorting the buildings by total energy consumption in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'exp_transform' operation do?","output":"The 'exp_transform' operation applies an exponential function to the specified columns."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of shipments including shipment ID, status, weight, and warehouse ID.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"column_2_id\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment.\"},{\"column\":\"column_3_id\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"column_4_id\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the warehouse associated with the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_data\",\"location\":\"C:/data/logistics/warehouse_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Warehouse Data\",\"description\":\"Contains information about warehouses including warehouse ID and location.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"column_2_id\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse.\"}]},\"table_id3\":{\"name\":\"transport_data\",\"location\":\"C:/data/logistics/transport_data.csv\",\"sheet_name\":null,\"label\":\"Transport Data\",\"description\":\"Contains data about transportation vehicles and routes.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"vehicle_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each vehicle.\"},{\"column\":\"column_2_id\",\"column_name\":\"route\",\"column_type\":\"xsd:string\",\"column_description\":\"Route taken by the vehicle.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at C:/data/logistics/shipment_data.csv.\",\"Load the 'warehouse_data' table from the Excel file located at C:/data/logistics/warehouse_data.xlsx.\",\"Filter the 'shipment_data' table to include only shipments with a status of 'delivered'.\",\"Join the filtered 'shipment_data' with 'warehouse_data' on the 'warehouse_id' column using an inner join.\",\"Aggregate the joined table to calculate the total weight of shipments for each warehouse.\",\"Sort the aggregated data by total shipment weight in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouse_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"warehouse_data\",\"comment\":\"Loading warehouse data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"status == 'delivered'\"},\"output\":\"delivered_shipments\",\"comment\":\"Filtering shipment data to include only delivered shipments.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"delivered_shipments\",\"warehouse_data\"],\"joinOn\":[\"warehouse_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining delivered shipments with warehouse data on warehouse_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"warehouse_id\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total shipment weight for each warehouse.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_weight\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total shipment weight in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"building_energy_data\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption Data\",\"description\":\"This table contains energy consumption data for a building, including timestamps and energy usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_usage_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Building Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for the building, including dates and tasks.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"},{\"column\":\"col_B\",\"column_name\":\"task\",\"column_type\":\"xsd:string\",\"column_description\":\"The maintenance task to be performed.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records from the year 2023.\",\"Calculate the total energy consumption for each month.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"building_energy_data\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"building_energy_data\",\"query\":\"`timestamp`.year == 2023\"},\"output\":\"energy_data_2023\",\"comment\":\"Filtering the energy consumption data to include only records from the year 2023.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"energy_data_2023\",\"groupBy\":\"timestamp.month\",\"aggregations\":[{\"column\":\"energy_usage_kwh\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Calculating the total energy consumption for each month.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"Contains demographic information about customers, including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"sales_transactions\",\"location\":\"C:/data/sales_transactions.xlsx\",\"sheet_name\":\"Transactions\",\"label\":\"Sales Transactions\",\"description\":\"Contains records of sales transactions including transaction date, amount, and customer ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount of the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"}]},\"table_id3\":{\"name\":\"marketing_campaigns\",\"location\":\"C:/data/marketing_campaigns.csv\",\"sheet_name\":null,\"label\":\"Marketing Campaigns\",\"description\":\"Details of past marketing campaigns including campaign ID, start date, and end date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_2\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Start date of the campaign.\"},{\"column\":\"col_3\",\"column_name\":\"end_date\",\"column_type\":\"xsd:date\",\"column_description\":\"End date of the campaign.\"}]}},\"instructions\":[\"Load the customer demographics data from the CSV file.\",\"Load the sales transactions data from the Excel file.\",\"Filter the sales data to include only transactions from the last quarter.\",\"Join the filtered sales data with customer demographics data on customer ID.\",\"Aggregate the joined data to calculate total sales per customer.\",\"Sort the aggregated data by total sales in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_transactions.xlsx\",\"type\":\"xlsx\"},\"output\":\"sales_transactions\",\"comment\":\"Loading sales transactions data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_transactions\",\"query\":\"`transaction_date` >= '2023-07-01' and `transaction_date` <= '2023-09-30'\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions from the last quarter.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered sales data with customer demographics on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"amount\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total sales per customer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total sales in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data with columns for date, energy usage, and other relevant metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_usage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy used on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"other_metric\",\"column_type\":\"xsd:string\",\"column_description\":\"Other relevant metrics for energy consumption.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data with columns for date, temperature, and other weather metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature recorded on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The humidity level recorded on the given date.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains data that is not relevant to the energy management workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column for this workflow.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Load the weather data from the Excel file located at C:/data/weather_data.xlsx.\",\"Filter the energy consumption data to include only records from the year 2023.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column 'energy_efficiency' by dividing 'energy_usage' by 'temperature'.\",\"Aggregate the joined data by month to calculate the total energy usage and average temperature.\",\"Sort the aggregated data by total energy usage in descending order.\",\"Forecast the next 30 days of energy usage using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date`.dt.year == 2023\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering energy consumption data to include only records from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered energy consumption data with weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`energy_usage` / `temperature`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating energy efficiency by dividing energy usage by temperature.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date.dt.to_period('M')\",\"aggregations\":[{\"column\":\"energy_usage\",\"name\":\"total_energy_usage\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating data by month to calculate total energy usage and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy_usage\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total energy usage in descending order.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"energy_consumption\",\"columns\":[\"energy_usage\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7}},\"output\":\"energy_forecast\",\"comment\":\"Forecasting the next 30 days of energy usage using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials_inventory\",\"location\":\"C:/manufacturing_data/raw_materials_inventory.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Inventory\",\"description\":\"Contains details of raw materials including quantity, reorder level, and unit price.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"col_2\",\"column_name\":\"material_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the raw material.\"},{\"column\":\"col_3\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current quantity of the material in stock.\"},{\"column\":\"col_4\",\"column_name\":\"reorder_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The quantity level at which new stock should be ordered.\"},{\"column\":\"col_5\",\"column_name\":\"unit_price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price per unit of the material.\"}]},\"table_id2\":{\"name\":\"supplier_details\",\"location\":\"C:/manufacturing_data/supplier_details.csv\",\"sheet_name\":null,\"label\":\"Supplier Details\",\"description\":\"Contains information about suppliers including contact details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"},{\"column\":\"col_3\",\"column_name\":\"contact_info\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact information for the supplier.\"},{\"column\":\"col_4\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Material ID that the supplier provides.\"}]},\"table_id3\":{\"name\":\"production_schedule\",\"location\":\"C:/manufacturing_data/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"Details the production schedule for the manufacturing plant.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"schedule_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each production schedule entry.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product being manufactured.\"},{\"column\":\"col_3\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Start date of the production schedule.\"},{\"column\":\"col_4\",\"column_name\":\"end_date\",\"column_type\":\"xsd:date\",\"column_description\":\"End date of the production schedule.\"}]}},\"instructions\":[\"Load the raw materials inventory table from the CSV file.\",\"Filter the inventory to include only materials with quantity below the reorder level.\",\"Join the filtered inventory with supplier details to get supplier contact information.\",\"Calculate the total cost for each material by multiplying quantity by unit price.\",\"Aggregate the data to find the total cost per supplier.\",\"Sort the aggregated data by total cost in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/manufacturing_data/raw_materials_inventory.csv\",\"type\":\"csv\"},\"output\":\"raw_materials_inventory\",\"comment\":\"Loading the raw materials inventory from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials_inventory\",\"query\":\"quantity < reorder_level\"},\"output\":\"low_stock_materials\",\"comment\":\"Filtering materials with quantity below the reorder level.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"low_stock_materials\",\"supplier_details\"],\"joinOn\":[\"material_id\"],\"joinType\":\"inner\"},\"output\":\"materials_with_suppliers\",\"comment\":\"Joining filtered materials with supplier details to get contact information.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"materials_with_suppliers\",\"columnName\":\"total_cost\",\"formula\":\"quantity * unit_price\"},\"output\":\"materials_with_cost\",\"comment\":\"Calculating total cost for each material by multiplying quantity by unit price.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"materials_with_cost\",\"groupBy\":\"supplier_id\",\"aggregations\":[{\"column\":\"total_cost\",\"name\":\"total_supplier_cost\",\"function\":\"sum\"}]},\"output\":\"total_cost_per_supplier\",\"comment\":\"Aggregating data to find total cost per supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"total_cost_per_supplier\",\"sortBy\":\"total_supplier_cost\",\"order\":\"desc\"},\"output\":\"sorted_supplier_costs\",\"comment\":\"Sorting the aggregated data by total cost in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains energy consumption data for the building, including timestamps and consumption values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"energy_consumed\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"occupants\",\"column_type\":\"xsd:integer\",\"column_description\":\"The number of occupants in the building.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/building/weather_data.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature and humidity readings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The temperature reading in Celsius.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The humidity percentage.\"}]},\"table_id3\":{\"name\":\"maintenance_logs\",\"location\":\"C:/data/building/maintenance_logs.csv\",\"sheet_name\":null,\"label\":\"Maintenance Logs\",\"description\":\"This table contains logs of maintenance activities in the building.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"log_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the maintenance activity.\"},{\"column\":\"col_2\",\"column_name\":\"activity\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the maintenance activity.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only entries from the year 2023.\",\"Filter the weather data to include only entries from the year 2023.\",\"Join the filtered energy consumption data with the filtered weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing energy consumption by the number of occupants.\",\"Aggregate the joined data by month to calculate the total energy consumption and average temperature.\",\"Sort the aggregated data by total energy consumption in descending order.\",\"Apply a rolling mean transformation to the energy efficiency column with a window of 3 months.\",\"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"`date`.dt.year == 2023\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering energy consumption data to include only entries from the year 2023.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"weather_data\",\"query\":\"`date`.dt.year == 2023\"},\"output\":\"filtered_weather_data\",\"comment\":\"Filtering weather data to include only entries from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"filtered_weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered energy consumption data with filtered weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"`energy_consumed` / `occupants`\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating energy efficiency by dividing energy consumption by the number of occupants.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date.dt.to_period('M')\",\"aggregations\":[{\"column\":\"energy_consumed\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"},{\"column\":\"temperature\",\"name\":\"average_temperature\",\"function\":\"mean\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating data by month to calculate total energy consumption and average temperature.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total energy consumption in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_data\",\"transformations\":[{\"columns\":[\"energy_efficiency\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":3}}]},\"output\":\"transformed_data\",\"comment\":\"Applying a rolling mean transformation to the energy efficiency column with a window of 3 months.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_data\",\"columns\":[\"total_energy_consumption\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"forecast_results\",\"comment\":\"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid list membership check.","output":"'col1 in [1, 2, 3]'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/marketing/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"This table contains demographic information about customers, including age, income, and city.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"income\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Annual income of the customer.\"},{\"column\":\"col_4\",\"column_name\":\"city\",\"column_type\":\"xsd:string\",\"column_description\":\"City where the customer resides.\"}]},\"table_id2\":{\"name\":\"customer_purchases\",\"location\":\"C:/data/marketing/customer_purchases.csv\",\"sheet_name\":null,\"label\":\"Customer Purchases\",\"description\":\"This table contains purchase history of customers, including purchase amount and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"purchase_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each purchase.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking purchase to a customer.\"},{\"column\":\"col_3\",\"column_name\":\"purchase_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount spent on the purchase.\"},{\"column\":\"col_4\",\"column_name\":\"purchase_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the purchase.\"}]}},\"instructions\":[\"Load the customer demographics data from the CSV file.\",\"Filter the data to include only customers aged between 25 and 40.\",\"Aggregate the filtered data to find the average income by city.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading customer demographics data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_demographics\",\"query\":\"`age` >= 25 and `age` <= 40\"},\"output\":\"filtered_customers\",\"comment\":\"Filtering customers to include only those aged between 25 and 40.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_customers\",\"groupBy\":\"city\",\"aggregations\":[{\"column\":\"income\",\"name\":\"average_income\",\"function\":\"mean\"}]},\"output\":\"average_income_by_city\",\"comment\":\"Aggregating data to find the average income by city for the filtered customers.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Inventory\",\"description\":\"Contains information about raw materials including their stock levels and unit prices.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"stock_level\",\"column_name\":\"STOCK_LEVEL\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the material.\"},{\"column\":\"unit_price\",\"column_name\":\"UNIT_PRICE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price per unit of the material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"Contains the schedule of production activities including product IDs and required materials.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material required for production.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material required for production.\"}]},\"table_id3\":{\"name\":\"machine_maintenance\",\"location\":\"C:/data/manufacturing/machine_maintenance.csv\",\"sheet_name\":null,\"label\":\"Machine Maintenance Records\",\"description\":\"Contains records of machine maintenance activities, not relevant to the current workflow.\",\"columns\":[{\"column\":\"machine_id\",\"column_name\":\"MACHINE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each machine.\"},{\"column\":\"maintenance_date\",\"column_name\":\"MAINTENANCE_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the maintenance activity.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains information about the materials used in manufacturing.\",\"Load the table 'production_schedule' which contains the schedule of production activities.\",\"Filter the 'raw_materials' table to include only materials with a stock level greater than 100.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Calculate a new column 'total_cost' in the joined table by multiplying 'quantity' by 'unit_price'.\",\"Aggregate the joined table by 'product_id' to find the total quantity and total cost for each product.\",\"Sort the aggregated data by 'total_cost' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials inventory from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"STOCK_LEVEL > 100\"},\"output\":\"filtered_materials\",\"comment\":\"Filtering raw materials to include only those with stock levels greater than 100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_materials\",\"production_schedule\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered materials with production schedule on material ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"total_cost\",\"formula\":\"QUANTITY * UNIT_PRICE\"},\"output\":\"joined_data_with_cost\",\"comment\":\"Calculating total cost by multiplying quantity by unit price.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_cost\",\"groupBy\":\"PRODUCT_ID\",\"aggregations\":[{\"column\":\"QUANTITY\",\"name\":\"total_quantity\",\"function\":\"sum\"},{\"column\":\"total_cost\",\"name\":\"total_cost\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by product ID to find total quantity and cost.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_cost\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total cost in descending order.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/final_production_plan.csv\",\"type\":\"csv\"},\"output\":\"final_production_plan\",\"comment\":\"Saving the sorted data to a new CSV file as the final production plan.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains the production schedule with details about order priorities and product IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"order_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each order.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product to be manufactured.\"},{\"column\":\"col_3\",\"column_name\":\"priority\",\"column_type\":\"xsd:string\",\"column_description\":\"Priority level of the order.\"}]},\"table_id2\":{\"name\":\"inventory\",\"location\":\"C:/data/manufacturing/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory\",\"description\":\"This table contains current inventory levels for each product.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product in inventory.\"},{\"column\":\"col_2\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current quantity of the product in inventory.\"}]},\"table_id3\":{\"name\":\"employee_records\",\"location\":\"C:/data/manufacturing/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"Contains records of employees working in the manufacturing unit.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"}]}},\"instructions\":[\"Load the production schedule table from the CSV file.\",\"Filter the production schedule to include only high-priority orders.\",\"Join the filtered production schedule with the inventory table on the product ID.\",\"Aggregate the joined table to calculate the total quantity needed for each product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"priority == 'high'\"},\"output\":\"high_priority_orders\",\"comment\":\"Filtering the production schedule to include only high-priority orders.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/inventory.csv\",\"type\":\"csv\"},\"output\":\"inventory\",\"comment\":\"Loading the inventory data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_priority_orders\",\"inventory\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered production schedule with the inventory table on product ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"quantity\",\"name\":\"total_quantity_needed\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined table to calculate the total quantity needed for each product.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback\",\"description\":\"Table containing customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"col_3\",\"column_name\":\"comments\",\"column_type\":\"xsd:string\",\"column_description\":\"Feedback comments from the customer.\"}]},\"table_2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Table containing detailed information about customers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"col_3\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"Table containing sales data for various products.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Filter the feedback to include only those with a rating of 4 or higher.\",\"Load the customer details table from the Excel file.\",\"Join the filtered feedback with customer details on the customer ID.\",\"Aggregate the data to find the average rating per region.\",\"Sort the aggregated data by average rating in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback to include only those with a rating of 4 or higher.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"feedback_with_details\",\"comment\":\"Joining filtered feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"feedback_with_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_per_region\",\"comment\":\"Aggregating data to find the average rating per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_rating_per_region\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_average_rating\",\"comment\":\"Sorting the aggregated data by average rating in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan floating-point and integer operations be mixed?","output":"Yes, floating-point and integer operations can be mixed, such as 'col1 + 2.5' or 'col2 / 3'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains the energy consumption data for the building, including total energy usage and number of occupants.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"total_energy\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total energy consumed by the building in kWh.\"},{\"column\":\"col_3\",\"column_name\":\"occupants\",\"column_type\":\"xsd:integer\",\"column_description\":\"The number of occupants in the building.\"}]},\"table_id2\":{\"name\":\"weather_data\",\"location\":\"C:/data/weather_data.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Weather Data\",\"description\":\"This table contains weather data including temperature and humidity for the year 2023.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the weather record.\"},{\"column\":\"col_2\",\"column_name\":\"temperature\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average temperature on the given date.\"},{\"column\":\"col_3\",\"column_name\":\"humidity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The average humidity on the given date.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.csv\",\"sheet_name\":null,\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for the building, including dates and tasks.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the scheduled maintenance.\"},{\"column\":\"col_2\",\"column_name\":\"task\",\"column_type\":\"xsd:string\",\"column_description\":\"The maintenance task to be performed.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Load the weather data from the Excel file.\",\"Filter the energy consumption data to include only entries from the year 2023.\",\"Join the filtered energy consumption data with the weather data on the date column.\",\"Calculate a new column for energy efficiency by dividing total energy consumption by the number of occupants.\",\"Aggregate the joined data by month to calculate the average temperature and total energy consumption.\",\"Sort the aggregated data by total energy consumption in descending order.\",\"Apply a rolling mean transformation to the energy consumption column with a window of 3 months.\",\"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/weather_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"weather_data\",\"comment\":\"Loading the weather data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"date >= '2023-01-01' and date <= '2023-12-31'\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the energy consumption data to include only entries from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_energy_consumption\",\"weather_data\"],\"joinOn\":[\"date\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered energy consumption data with the weather data on the date column.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"energy_efficiency\",\"formula\":\"total_energy / occupants\"},\"output\":\"data_with_efficiency\",\"comment\":\"Calculating a new column for energy efficiency by dividing total energy consumption by the number of occupants.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_efficiency\",\"groupBy\":\"date.dt.to_period('M')\",\"aggregations\":[{\"column\":\"temperature\",\"name\":\"avg_temperature\",\"function\":\"mean\"},{\"column\":\"total_energy\",\"name\":\"total_energy_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_aggregated_data\",\"comment\":\"Aggregating the joined data by month to calculate the average temperature and total energy consumption.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_aggregated_data\",\"sortBy\":\"total_energy_consumption\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total energy consumption in descending order.\"},{\"action\":\"applyTimeSeriesTransformations\",\"parameters\":{\"table\":\"sorted_data\",\"transformations\":[{\"columns\":[\"total_energy_consumption\"],\"operation\":\"rolling_mean\",\"parameters\":{\"window\":3}}]},\"output\":\"data_with_rolling_mean\",\"comment\":\"Applying a rolling mean transformation to the energy consumption column with a window of 3 months.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"data_with_rolling_mean\",\"columns\":[\"total_energy_consumption\"],\"forecastParameters\":{\"forecastPeriod\":6,\"frequency\":\"MS\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"confidenceInterval\":95}},\"output\":\"forecast_results\",\"comment\":\"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Inventory\",\"description\":\"This table contains information about the raw materials available for manufacturing, including their quantities and unit prices.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each raw material.\"},{\"column\":\"quantity\",\"column_name\":\"QUANTITY\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The quantity of the raw material available.\"},{\"column\":\"unit_price\",\"column_name\":\"UNIT_PRICE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price per unit of the raw material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Production Schedule\",\"description\":\"This table contains the schedule of production activities, including the materials required for each activity.\",\"columns\":[{\"column\":\"activity_id\",\"column_name\":\"ACTIVITY_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each production activity.\"},{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the raw material required for the activity.\"},{\"column\":\"activity_date\",\"column_name\":\"ACTIVITY_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date on which the production activity is scheduled.\"}]},\"table_id3\":{\"name\":\"employee_records\",\"location\":\"C:/data/manufacturing/employee_records.csv\",\"sheet_name\":null,\"label\":\"Employee Records\",\"description\":\"This table contains records of employees, including their roles and departments.\",\"columns\":[{\"column\":\"employee_id\",\"column_name\":\"EMPLOYEE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"name\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the employee.\"},{\"column\":\"role\",\"column_name\":\"ROLE\",\"column_type\":\"xsd:string\",\"column_description\":\"The role of the employee within the company.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains information about the available raw materials.\",\"Load the table 'production_schedule' which contains the planned production activities.\",\"Filter the 'raw_materials' table to include only materials with a quantity greater than 100.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Calculate a new column 'material_cost' in the joined table by multiplying 'quantity' by 'unit_price'.\",\"Aggregate the joined table to calculate the total cost of materials for each production activity.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials inventory from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.xlsx\",\"type\":\"xlsx\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"`QUANTITY` > 100\"},\"output\":\"filtered_raw_materials\",\"comment\":\"Filtering raw materials to include only those with a quantity greater than 100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_raw_materials\",\"production_schedule\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered raw materials with the production schedule on material_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_table\",\"columnName\":\"material_cost\",\"formula\":\"`QUANTITY` * `UNIT_PRICE`\"},\"output\":\"joined_table_with_cost\",\"comment\":\"Calculating material cost by multiplying quantity by unit price.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_table_with_cost\",\"groupBy\":\"ACTIVITY_ID\",\"aggregations\":[{\"column\":\"material_cost\",\"name\":\"total_material_cost\",\"function\":\"sum\"}]},\"output\":\"activity_material_costs\",\"comment\":\"Aggregating to calculate the total cost of materials for each production activity.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/hospital_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains detailed records of patients including admission dates and departments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"admission_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date when the patient was admitted.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the patient is admitted.\"}]},\"table_id2\":{\"name\":\"medication_prescriptions\",\"location\":\"C:/hospital_data/medication_prescriptions.csv\",\"sheet_name\":null,\"label\":\"Medication Prescriptions\",\"description\":\"Contains records of medication prescribed to patients including costs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost of the prescribed medication.\"}]},\"table_id3\":{\"name\":\"staff_records\",\"location\":\"C:/hospital_data/staff_records.csv\",\"sheet_name\":null,\"label\":\"Staff Records\",\"description\":\"Contains records of hospital staff including roles and departments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"role\",\"column_type\":\"xsd:string\",\"column_description\":\"Role of the staff member in the hospital.\"}]}},\"instructions\":[\"Load the patient records table from the hospital database.\",\"Load the medication prescriptions table from the hospital database.\",\"Filter the patient records to include only those admitted in the last 30 days.\",\"Join the filtered patient records with the medication prescriptions on patient ID.\",\"Calculate the total medication cost for each patient.\",\"Aggregate the data to find the average medication cost per department.\",\"Sort the aggregated data by average medication cost in descending order.\",\"Identify any outliers in the medication costs using a z-score threshold.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading patient records from the hospital database.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/hospital_data/medication_prescriptions.csv\",\"type\":\"csv\"},\"output\":\"medication_prescriptions\",\"comment\":\"Loading medication prescriptions from the hospital database.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"`admission_date` >= '2023-09-01'\"},\"output\":\"recent_patients\",\"comment\":\"Filtering patient records to include only those admitted in the last 30 days.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"recent_patients\",\"medication_prescriptions\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"patients_with_medications\",\"comment\":\"Joining filtered patient records with medication prescriptions on patient ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"patients_with_medications\",\"columnName\":\"total_medication_cost\",\"formula\":\"`medication_cost`\"},\"output\":\"patients_with_total_cost\",\"comment\":\"Calculating the total medication cost for each patient.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"patients_with_total_cost\",\"groupBy\":\"department\",\"aggregations\":[{\"column\":\"total_medication_cost\",\"name\":\"average_medication_cost\",\"function\":\"mean\"}]},\"output\":\"average_cost_per_department\",\"comment\":\"Aggregating data to find the average medication cost per department.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_cost_per_department\",\"sortBy\":\"average_medication_cost\",\"order\":\"desc\"},\"output\":\"sorted_average_cost\",\"comment\":\"Sorting the aggregated data by average medication cost in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_average_cost\",\"transformations\":[{\"columns\":[\"average_medication_cost\"],\"operation\":\"normalize\"}]},\"output\":\"outliers_identified\",\"comment\":\"Identifying any outliers in the medication costs using a z-score threshold.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/healthcare_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains detailed records of patients including demographics and diagnoses.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"medication_records\",\"location\":\"C:/healthcare_data/medication_records.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Contains records of medications prescribed to patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the medication prescribed.\"},{\"column\":\"col_3\",\"column_name\":\"dosage\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Dosage of the medication prescribed.\"}]},\"table_id3\":{\"name\":\"hospital_staff\",\"location\":\"C:/healthcare_data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"Contains records of hospital staff including their roles and departments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the staff member works.\"}]}},\"instructions\":[\"Load the patient records table from the healthcare database.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Load the medication records table from the healthcare database.\",\"Join the filtered patient records with the medication records on patient ID.\",\"Calculate the average medication dosage for diabetic patients.\",\"Sort the resulting table by patient age in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetic_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with diabetes.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/medication_records.csv\",\"type\":\"csv\"},\"output\":\"medication_records\",\"comment\":\"Loading the medication records dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"diabetic_patients\",\"medication_records\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"diabetic_medication_records\",\"comment\":\"Joining diabetic patient records with medication records on patient ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"diabetic_medication_records\",\"groupBy\":\"patient_id\",\"aggregations\":[{\"column\":\"dosage\",\"name\":\"average_dosage\",\"function\":\"mean\"}]},\"output\":\"average_dosage_per_patient\",\"comment\":\"Calculating the average medication dosage for each diabetic patient.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_dosage_per_patient\",\"sortBy\":\"age\",\"order\":\"desc\"},\"output\":\"sorted_average_dosage\",\"comment\":\"Sorting the table by patient age in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the purpose of the 'sortData' action?","output":"The 'sortData' action sorts a table by one or more specified columns in either ascending or descending order."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Table containing specifications and development status of products.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"development_status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current development status of the product.\"}]},\"table_id2\":{\"name\":\"team_assignments\",\"location\":\"C:/data/team_assignments.csv\",\"sheet_name\":null,\"label\":\"Team Assignments\",\"description\":\"Table containing team assignments for each product.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"team_member_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each team member.\"}]},\"table_id3\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"Table containing sales data for each product.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total sales amount for the product.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only products with a development status of 'in progress'.\",\"Join the filtered product specifications with the team assignments table on the product ID.\",\"Aggregate the joined table to calculate the total number of team members assigned to each product.\",\"Sort the aggregated data by the total number of team members in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"development_status == 'in progress'\"},\"output\":\"in_progress_products\",\"comment\":\"Filtering products to include only those with a development status of 'in progress'.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/team_assignments.csv\",\"type\":\"csv\"},\"output\":\"team_assignments\",\"comment\":\"Loading the team assignments from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"in_progress_products\",\"team_assignments\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_product_team\",\"comment\":\"Joining filtered products with team assignments on product ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_product_team\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"team_member_id\",\"name\":\"total_team_members\",\"function\":\"count\"}]},\"output\":\"product_team_count\",\"comment\":\"Aggregating to calculate the total number of team members assigned to each product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"product_team_count\",\"sortBy\":\"total_team_members\",\"order\":\"desc\"},\"output\":\"sorted_product_team_count\",\"comment\":\"Sorting the aggregated data by the total number of team members in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow does the Tamarind workflow language ensure consistency in execution?","output":"By enforcing strict definitions for each action, Tamarind workflows eliminate ambiguity and ensure that data transformations are applied in a deterministic manner."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_demographics\",\"location\":\"C:/data/marketing/customer_demographics.csv\",\"sheet_name\":null,\"label\":\"Customer Demographics\",\"description\":\"This table contains demographic information about customers, including age, gender, and location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"gender\",\"column_type\":\"xsd:string\",\"column_description\":\"Gender of the customer.\"}]},\"table_id2\":{\"name\":\"marketing_campaign_results\",\"location\":\"C:/data/marketing/marketing_campaign_results.xlsx\",\"sheet_name\":\"Results\",\"label\":\"Marketing Campaign Results\",\"description\":\"This table contains the results of various marketing campaigns, including success metrics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"campaign_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each marketing campaign.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"success\",\"column_type\":\"xsd:boolean\",\"column_description\":\"Indicates whether the campaign was successful.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/marketing/irrelevant_data.xlsx\",\"sheet_name\":\"Irrelevant\",\"label\":\"Irrelevant Data\",\"description\":\"This table contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"data_point\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant data point.\"}]}},\"instructions\":[\"Load the customer demographics table from the CSV file.\",\"Load the marketing campaign results table from the Excel file.\",\"Filter the marketing campaign results to include only successful campaigns.\",\"Join the filtered marketing campaign results with the customer demographics on customer ID.\",\"Aggregate the joined data to find the total number of successful campaigns per age group.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_demographics.csv\",\"type\":\"csv\"},\"output\":\"customer_demographics\",\"comment\":\"Loading the customer demographics data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/marketing_campaign_results.xlsx\",\"type\":\"xlsx\"},\"output\":\"marketing_campaign_results\",\"comment\":\"Loading the marketing campaign results from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"marketing_campaign_results\",\"query\":\"success == True\"},\"output\":\"successful_campaigns\",\"comment\":\"Filtering the marketing campaign results to include only successful campaigns.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"successful_campaigns\",\"customer_demographics\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining successful campaigns with customer demographics on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"age\",\"aggregations\":[{\"column\":\"campaign_id\",\"name\":\"total_successful_campaigns\",\"function\":\"count\"}]},\"output\":\"age_group_campaigns\",\"comment\":\"Aggregating the joined data to find the total number of successful campaigns per age group.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid math function using hyperbolic trigonometry.","output":"'tanh(col1) * sinh(col2)'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid arithmetic expression due to an unsupported operation.","output":"'col1 // col2' // Error: '//' operator is not supported in Tamarind expressions."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption data with timestamps and consumption values in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"energy_prices\",\"location\":\"C:/data/energy_prices.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains historical energy prices with timestamps and price values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The date and time of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price_per_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kilowatt-hour.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\",\"Filter the data to include only records where the consumption is greater than 100 kWh.\",\"Aggregate the filtered data by month to calculate the total energy consumption.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"consumption_kwh > 100\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the data to include only records where the consumption is greater than 100 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_consumption\",\"groupBy\":\"timestamp.dt.to_period('M')\",\"aggregations\":[{\"column\":\"consumption_kwh\",\"name\":\"total_monthly_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Aggregating the filtered data by month to calculate the total energy consumption.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications for each product, including product ID, name, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"}]},\"table_id2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx\",\"sheet_name\":\"2023\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including potential customers and average purchase value.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Product ID to match with product specifications.\"},{\"column\":\"col_2\",\"column_name\":\"potential_customers\",\"column_type\":\"xsd:integer\",\"column_description\":\"Estimated number of potential customers.\"},{\"column\":\"col_3\",\"column_name\":\"average_purchase_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Average value of purchases for the product.\"},{\"column\":\"col_4\",\"column_name\":\"research_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the market research entry.\"}]},\"table_id3\":{\"name\":\"historical_sales\",\"location\":\"C:/data/historical_sales.csv\",\"sheet_name\":null,\"label\":\"Historical Sales Data\",\"description\":\"Contains historical sales data for various products, not relevant for current analysis.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"sale_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each sale.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Product ID related to the sale.\"},{\"column\":\"col_3\",\"column_name\":\"sale_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the sale.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel spreadsheet.\",\"Filter the market research data to include only entries from the last year.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Calculate the potential market size by multiplying the number of potential customers by the average purchase value.\",\"Aggregate the joined data by product category to find the total potential market size for each category.\",\"Sort the aggregated data by total potential market size in descending order.\",\"Identify the top 5 product categories with the highest potential market size.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading market research data from an Excel spreadsheet.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"`research_date` >= '2022-01-01'\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only entries from the last year.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_market_size\",\"formula\":\"`potential_customers` * `average_purchase_value`\"},\"output\":\"joined_data_with_market_size\",\"comment\":\"Calculating potential market size by multiplying potential customers by average purchase value.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_market_size\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"potential_market_size\",\"name\":\"total_potential_market_size\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by product category to find total potential market size for each category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_potential_market_size\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total potential market size in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"query\":\"index < 5\"},\"output\":\"top_product_categories\",\"comment\":\"Identifying the top 5 product categories with the highest potential market size.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_feedback\",\"location\":\"C:/data/customer_feedback.csv\",\"sheet_name\":null,\"label\":\"Customer Feedback Data\",\"description\":\"Contains customer feedback with ratings and comments.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Customer rating from 1 to 5.\"},{\"column\":\"column_3_id\",\"column_name\":\"feedback\",\"column_type\":\"xsd:string\",\"column_description\":\"Textual feedback from the customer.\"}]},\"table_id2\":{\"name\":\"customer_details\",\"location\":\"C:/data/customer_details.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Details\",\"description\":\"Contains detailed information about customers including their region.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"},{\"column\":\"column_3_id\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the customer feedback table from the CSV file.\",\"Load the customer details table from the Excel file.\",\"Filter the customer feedback to include only feedback with a rating of 4 or higher.\",\"Join the filtered feedback with customer details on the customer ID.\",\"Aggregate the joined data to calculate the average rating per region.\",\"Sort the aggregated data by average rating in descending order.\",\"Calculate a new column for the percentage of positive feedback.\",\"Filter the sorted data to include only regions with an average rating above 4.5.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_feedback.csv\",\"type\":\"csv\"},\"output\":\"customer_feedback\",\"comment\":\"Loading customer feedback data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_details.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_details\",\"comment\":\"Loading customer details data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_feedback\",\"query\":\"rating >= 4\"},\"output\":\"positive_feedback\",\"comment\":\"Filtering feedback to include only ratings of 4 or higher.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"positive_feedback\",\"customer_details\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_feedback_details\",\"comment\":\"Joining filtered feedback with customer details on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_feedback_details\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"rating\",\"name\":\"average_rating\",\"function\":\"mean\"}]},\"output\":\"average_rating_per_region\",\"comment\":\"Calculating average rating per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_rating_per_region\",\"sortBy\":\"average_rating\",\"order\":\"desc\"},\"output\":\"sorted_average_rating\",\"comment\":\"Sorting regions by average rating in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_average_rating\",\"columnName\":\"positive_feedback_percentage\",\"formula\":\"(`average_rating` / 5) * 100\"},\"output\":\"rating_with_percentage\",\"comment\":\"Calculating the percentage of positive feedback.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"rating_with_percentage\",\"query\":\"average_rating > 4.5\"},\"output\":\"high_rating_regions\",\"comment\":\"Filtering regions with an average rating above 4.5.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials Table\",\"description\":\"This table contains information about the raw materials used in the manufacturing process, including material ID, name, and quantity.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"column_2_id\",\"column_name\":\"material_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the material.\"},{\"column\":\"column_3_id\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the material available.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule Table\",\"description\":\"This table contains the production schedule, including product ID, material ID, and required quantity.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"material_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"column_3_id\",\"column_name\":\"required_quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material required for production.\"}]},\"table_id3\":{\"name\":\"inventory\",\"location\":\"C:/data/manufacturing/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory Table\",\"description\":\"This table contains the current inventory levels of finished products.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the product.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains information about the materials used in manufacturing.\",\"Filter the 'raw_materials' table to include only materials with a quantity greater than 100.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Aggregate the joined table to calculate the total quantity of materials needed per product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"quantity > 100\"},\"output\":\"filtered_raw_materials\",\"comment\":\"Filtering raw materials to include only those with a quantity greater than 100.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_raw_materials\",\"production_schedule\"],\"joinOn\":[\"material_id\"],\"joinType\":\"inner\"},\"output\":\"joined_materials_schedule\",\"comment\":\"Joining filtered raw materials with production schedule on material_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_materials_schedule\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"quantity\",\"name\":\"total_material_quantity\",\"function\":\"sum\"}]},\"output\":\"aggregated_materials\",\"comment\":\"Aggregating the joined table to calculate the total quantity of materials needed per product.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"This table contains detailed specifications of products, including their development status.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"development_status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current development status of the product.\"}]},\"table_id2\":{\"name\":\"team_assignments\",\"location\":\"C:/data/team_assignments.csv\",\"sheet_name\":null,\"label\":\"Team Assignments\",\"description\":\"This table lists team members assigned to each product.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"team_member_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each team member.\"}]},\"table_id3\":{\"name\":\"project_timeline\",\"location\":\"C:/data/project_timeline.csv\",\"sheet_name\":null,\"label\":\"Project Timeline\",\"description\":\"This table contains timelines for various projects, not directly related to product development.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"project_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each project.\"},{\"column\":\"col_2\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Start date of the project.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only those with a development status of 'In Progress'.\",\"Join the filtered product specifications with the team assignments table on the product ID.\",\"Aggregate the joined table to calculate the total number of team members assigned to each product.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"development_status == 'In Progress'\"},\"output\":\"in_progress_products\",\"comment\":\"Filtering products to include only those with a development status of 'In Progress'.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/team_assignments.csv\",\"type\":\"csv\"},\"output\":\"team_assignments\",\"comment\":\"Loading the team assignments table from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"in_progress_products\",\"team_assignments\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"product_team_info\",\"comment\":\"Joining filtered product specifications with team assignments on product ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"product_team_info\",\"groupBy\":\"product_id\",\"aggregations\":[{\"column\":\"team_member_id\",\"name\":\"total_team_members\",\"function\":\"count\"}]},\"output\":\"product_team_summary\",\"comment\":\"Aggregating to calculate the total number of team members assigned to each product.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"customer_data\",\"location\":\"C:/data/marketing/customer_data.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"This table contains information about customers, including their lifetime value and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"lifetime_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total value a customer has contributed over their lifetime.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The geographical region of the customer.\"}]},\"table_id2\":{\"name\":\"sales_data\",\"location\":\"C:/data/marketing/sales_data.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"This table contains sales transactions, including product details and sales amount.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product sold.\"},{\"column\":\"col_3\",\"column_name\":\"sales_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of money generated from the sale.\"}]}},\"instructions\":[\"Load the customer data table from the CSV file.\",\"Filter the customer data to include only those with a lifetime value greater than 1000.\",\"Aggregate the filtered data to find the average lifetime value by region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/marketing/customer_data.csv\",\"type\":\"csv\"},\"output\":\"customer_data\",\"comment\":\"Loading the customer data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"customer_data\",\"query\":\"lifetime_value > 1000\"},\"output\":\"high_value_customers\",\"comment\":\"Filtering customer data to include only those with a lifetime value greater than 1000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"high_value_customers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"lifetime_value\",\"name\":\"average_lifetime_value\",\"function\":\"mean\"}]},\"output\":\"average_lifetime_value_by_region\",\"comment\":\"Aggregating the filtered data to find the average lifetime value by region.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/healthcare_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"A table containing patient information including age, diagnosis, and treatment details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"hospital_staff\",\"location\":\"C:/healthcare_data/hospital_staff.csv\",\"sheet_name\":null,\"label\":\"Hospital Staff\",\"description\":\"A table containing information about hospital staff including their roles and departments.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"staff_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each staff member.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the staff member.\"},{\"column\":\"col_3\",\"column_name\":\"department\",\"column_type\":\"xsd:string\",\"column_description\":\"Department where the staff member works.\"}]}},\"instructions\":[\"Load the patient records table from the healthcare database.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Aggregate the filtered data to find the average age of diabetic patients.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetic_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with diabetes.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"diabetic_patients\",\"groupBy\":null,\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"average_age_diabetic_patients\",\"comment\":\"Calculating the average age of patients diagnosed with diabetes.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"court_cases\",\"location\":\"C:/legal_data/court_cases.csv\",\"sheet_name\":null,\"label\":\"Court Cases\",\"description\":\"A table containing details of various court cases including case ID, lawyer ID, and case date.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court case.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"column_3_id\",\"column_name\":\"case_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/legal_data/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers Information\",\"description\":\"A table containing information about lawyers including lawyer ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"column_2_id\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_judges\",\"location\":\"C:/legal_data/court_judges.csv\",\"sheet_name\":null,\"label\":\"Court Judges\",\"description\":\"A table containing information about judges including judge ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"judge_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each judge.\"},{\"column\":\"column_2_id\",\"column_name\":\"judge_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the judge.\"}]}},\"instructions\":[\"Load the table 'court_cases' which contains details of various legal cases.\",\"Load the table 'lawyers' which contains information about lawyers involved in the cases.\",\"Filter the 'court_cases' table to include only cases from the year 2023.\",\"Join the filtered 'court_cases' table with the 'lawyers' table on the lawyer_id column.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\",\"Sort the aggregated data by the total number of cases in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/court_cases.csv\",\"type\":\"csv\"},\"output\":\"court_cases\",\"comment\":\"Loading the court cases dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/lawyers.csv\",\"type\":\"csv\"},\"output\":\"lawyers\",\"comment\":\"Loading the lawyers dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"court_cases\",\"query\":\"`case_date`.year == 2023\"},\"output\":\"filtered_court_cases\",\"comment\":\"Filtering court cases to include only those from the year 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_court_cases\",\"lawyers\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_cases_lawyers\",\"comment\":\"Joining filtered court cases with lawyers information on lawyer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_cases_lawyers\",\"groupBy\":\"lawyer_name\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating data to find the total number of cases handled by each lawyer.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"lawyer_case_counts\",\"sortBy\":\"total_cases\",\"order\":\"desc\"},\"output\":\"sorted_lawyer_case_counts\",\"comment\":\"Sorting the aggregated data by the total number of cases in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"Contains detailed specifications for each product, including product ID, name, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product.\"}]},\"table_id2\":{\"name\":\"market_research\",\"location\":\"C:/data/market_research.xlsx\",\"sheet_name\":\"Data\",\"label\":\"Market Research Data\",\"description\":\"Contains market research data including demand forecasts and customer demographics.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"average_demand\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Average demand for the product.\"},{\"column\":\"col_3\",\"column_name\":\"potential_customers\",\"column_type\":\"xsd:integer\",\"column_description\":\"Estimated number of potential customers.\"},{\"column\":\"col_4\",\"column_name\":\"year\",\"column_type\":\"xsd:integer\",\"column_description\":\"Year of the market research data.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data that is not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Load the market research data from the Excel file.\",\"Filter the market research data to include only data from the last two years.\",\"Join the product specifications with the filtered market research data on the product ID.\",\"Calculate the potential market size by multiplying the average demand by the number of potential customers.\",\"Aggregate the data to find the total potential market size for each product category.\",\"Sort the aggregated data by total potential market size in descending order.\",\"Identify the top 5 product categories with the highest potential market size.\",\"Forecast the demand for the top product category using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/market_research.xlsx\",\"type\":\"xlsx\"},\"output\":\"market_research\",\"comment\":\"Loading the market research data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"market_research\",\"query\":\"year >= 2021\"},\"output\":\"filtered_market_research\",\"comment\":\"Filtering market research data to include only data from the last two years.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"product_specifications\",\"filtered_market_research\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining product specifications with filtered market research data on product ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"potential_market_size\",\"formula\":\"average_demand * potential_customers\"},\"output\":\"data_with_market_size\",\"comment\":\"Calculating potential market size by multiplying average demand by the number of potential customers.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_market_size\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"potential_market_size\",\"name\":\"total_market_size\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find total potential market size for each product category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_market_size\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total potential market size in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_data\",\"query\":\"index < 5\"},\"output\":\"top_product_categories\",\"comment\":\"Identifying the top 5 product categories with the highest potential market size.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"joined_data\",\"columns\":[\"average_demand\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"dateColumn\":\"year\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"demand_forecast\",\"comment\":\"Forecasting demand for the top product category using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid 'applyTimeSeriesTransformations' action with an unsupported transformation.","output":"{ \"name\": \"applyTimeSeriesTransformations\", \"parameters\": { \"table\": \"stock_data\", \"transformations\": [ { \"columns\": [\"closing_price\"], \"operation\": \"unsupported_transformation\" } ] }, \"output\": \"transformed_stock\" } // Error: 'unsupported_transformation' is not a valid transformation operation."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat does the 'name' field in a Tamarind workflow action represent?","output":"The 'name' field identifies the operation to be performed in the workflow."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of shipments including shipment dates, weights, and route IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"col_2\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the shipment.\"},{\"column\":\"col_3\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"},{\"column\":\"col_4\",\"column_name\":\"route_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the route of the shipment.\"}]},\"table_id2\":{\"name\":\"route_info\",\"location\":\"C:/data/logistics/route_info.csv\",\"sheet_name\":null,\"label\":\"Route Information\",\"description\":\"Contains information about different routes including route IDs and descriptions.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"route_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each route.\"},{\"column\":\"col_2\",\"column_name\":\"route_description\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the route.\"}]},\"table_id3\":{\"name\":\"driver_info\",\"location\":\"C:/data/logistics/driver_info.csv\",\"sheet_name\":null,\"label\":\"Driver Information\",\"description\":\"Contains details about drivers including driver IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"driver_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each driver.\"},{\"column\":\"col_2\",\"column_name\":\"driver_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the driver.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of shipments including dates and weights.\",\"Filter the shipment data to include only shipments with a weight greater than 1000 kg.\",\"Join the filtered shipment data with the 'route_info' table on the route_id column.\",\"Aggregate the joined data to calculate the total weight of shipments for each route.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"weight > 1000\"},\"output\":\"filtered_shipments\",\"comment\":\"Filtering shipments to include only those with a weight greater than 1000 kg.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/route_info.csv\",\"type\":\"csv\"},\"output\":\"route_info\",\"comment\":\"Loading the route information from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_shipments\",\"route_info\"],\"joinOn\":[\"route_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered shipment data with route information on route_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"route_id\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate the total weight of shipments for each route.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"patient_records\",\"location\":\"C:/healthcare_data/patient_records.csv\",\"sheet_name\":null,\"label\":\"Patient Records\",\"description\":\"Contains detailed records of patients including their diagnoses and personal information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"age\",\"column_type\":\"xsd:integer\",\"column_description\":\"Age of the patient.\"},{\"column\":\"col_3\",\"column_name\":\"diagnosis\",\"column_type\":\"xsd:string\",\"column_description\":\"Medical diagnosis of the patient.\"}]},\"table_id2\":{\"name\":\"medication\",\"location\":\"C:/healthcare_data/medication.csv\",\"sheet_name\":null,\"label\":\"Medication Records\",\"description\":\"Contains information about medications prescribed to patients.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_2\",\"column_name\":\"medication_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the prescribed medication.\"}]},\"table_id3\":{\"name\":\"hospital_visits\",\"location\":\"C:/healthcare_data/hospital_visits.csv\",\"sheet_name\":null,\"label\":\"Hospital Visits\",\"description\":\"Records of hospital visits by patients, including visit dates and reasons.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"visit_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each hospital visit.\"},{\"column\":\"col_2\",\"column_name\":\"patient_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each patient.\"},{\"column\":\"col_3\",\"column_name\":\"visit_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the hospital visit.\"}]}},\"instructions\":[\"Load the patient records table from the healthcare database.\",\"Filter the patient records to include only those with a diagnosis of diabetes.\",\"Join the filtered patient records with the medication table to get prescribed medications.\",\"Aggregate the data to find the average age of diabetic patients for each medication.\",\"Sort the aggregated data by average age in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/patient_records.csv\",\"type\":\"csv\"},\"output\":\"patient_records\",\"comment\":\"Loading the patient records dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"patient_records\",\"query\":\"diagnosis == 'diabetes'\"},\"output\":\"diabetic_patients\",\"comment\":\"Filtering patient records to include only those diagnosed with diabetes.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/healthcare_data/medication.csv\",\"type\":\"csv\"},\"output\":\"medication\",\"comment\":\"Loading the medication records dataset from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"diabetic_patients\",\"medication\"],\"joinOn\":[\"patient_id\"],\"joinType\":\"inner\"},\"output\":\"diabetic_patients_medication\",\"comment\":\"Joining diabetic patient records with medication records based on patient_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"diabetic_patients_medication\",\"groupBy\":\"medication_name\",\"aggregations\":[{\"column\":\"age\",\"name\":\"average_age\",\"function\":\"mean\"}]},\"output\":\"average_age_by_medication\",\"comment\":\"Aggregating data to find the average age of diabetic patients for each medication.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_age_by_medication\",\"sortBy\":\"average_age\",\"order\":\"desc\"},\"output\":\"sorted_average_age_by_medication\",\"comment\":\"Sorting the aggregated data by average age in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/data/building_energy.csv\",\"sheet_name\":null,\"label\":\"Building Energy Consumption\",\"description\":\"This table contains energy consumption data for various buildings, including timestamps and usage in kWh.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:dateTime\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_3\",\"column_name\":\"energy_usage_kwh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Energy usage in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"building_info\",\"location\":\"C:/data/building_info.csv\",\"sheet_name\":null,\"label\":\"Building Information\",\"description\":\"This table contains information about buildings, including their location and size.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"location\",\"column_type\":\"xsd:string\",\"column_description\":\"The location of the building.\"},{\"column\":\"col_3\",\"column_name\":\"size_sqft\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The size of the building in square feet.\"}]},\"table_id3\":{\"name\":\"maintenance_schedule\",\"location\":\"C:/data/maintenance_schedule.csv\",\"sheet_name\":null,\"label\":\"Maintenance Schedule\",\"description\":\"This table contains the maintenance schedule for various buildings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"building_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each building.\"},{\"column\":\"col_2\",\"column_name\":\"maintenance_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The scheduled date for maintenance.\"},{\"column\":\"col_3\",\"column_name\":\"maintenance_type\",\"column_type\":\"xsd:string\",\"column_description\":\"Type of maintenance scheduled.\"}]}},\"instructions\":[\"Load the building energy consumption data from the CSV file.\",\"Filter the data to include only records where energy usage exceeds 500 kWh.\",\"Aggregate the filtered data to calculate the total energy usage per building.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/building_energy.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the building energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"energy_usage_kwh > 500\"},\"output\":\"high_energy_usage\",\"comment\":\"Filtering the data to include only records where energy usage exceeds 500 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"high_energy_usage\",\"groupBy\":\"building_id\",\"aggregations\":[{\"column\":\"energy_usage_kwh\",\"name\":\"total_energy_usage\",\"function\":\"sum\"}]},\"output\":\"total_energy_per_building\",\"comment\":\"Aggregating the filtered data to calculate the total energy usage per building.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid comparison expression using equality.","output":"'col1 == col2'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_details\",\"location\":\"C:/legal_data/case_details.csv\",\"sheet_name\":null,\"label\":\"Case Details\",\"description\":\"This table contains detailed information about various legal cases, including case ID, judge, verdict, and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each case.\"},{\"column\":\"col_2\",\"column_name\":\"judge\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the judge presiding over the case.\"},{\"column\":\"col_3\",\"column_name\":\"verdict\",\"column_type\":\"xsd:string\",\"column_description\":\"The verdict of the case, e.g., 'Guilty' or 'Not Guilty'.\"},{\"column\":\"col_4\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the verdict was given.\"}]},\"table_id2\":{\"name\":\"lawyer_details\",\"location\":\"C:/legal_data/lawyer_details.csv\",\"sheet_name\":null,\"label\":\"Lawyer Details\",\"description\":\"This table contains information about lawyers, including their ID, name, and specialty.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the lawyer.\"},{\"column\":\"col_3\",\"column_name\":\"specialty\",\"column_type\":\"xsd:string\",\"column_description\":\"The specialty area of the lawyer.\"}]}},\"instructions\":[\"Load the table 'case_details' which contains information about various legal cases.\",\"Filter the 'case_details' table to include only cases where the verdict is 'Guilty'.\",\"Aggregate the filtered data by 'judge' to count the number of guilty verdicts per judge.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/legal_data/case_details.csv\",\"type\":\"csv\"},\"output\":\"case_details\",\"comment\":\"Loading the case details dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_details\",\"query\":\"verdict == 'Guilty'\"},\"output\":\"guilty_cases\",\"comment\":\"Filtering the case details to include only cases with a 'Guilty' verdict.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"guilty_cases\",\"groupBy\":\"judge\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"guilty_verdicts_count\",\"function\":\"count\"}]},\"output\":\"guilty_verdicts_per_judge\",\"comment\":\"Aggregating the filtered data to count the number of guilty verdicts per judge.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the purpose of the 'applyTransformations' action?","output":"The 'applyTransformations' action applies a sequence of transformations to a dataset, such as interpolation, filling missing values, scaling, normalization, and statistical computations."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"raw_materials\",\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"sheet_name\":null,\"label\":\"Raw Materials\",\"description\":\"Table containing information about raw materials including their stock levels and IDs.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"stock_level\",\"column_name\":\"STOCK_LEVEL\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the material.\"}]},\"table_id2\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"Table containing the production schedule including material requirements and production days.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each material.\"},{\"column\":\"quantity_needed\",\"column_name\":\"QUANTITY_NEEDED\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of material needed for production.\"},{\"column\":\"production_days\",\"column_name\":\"PRODUCTION_DAYS\",\"column_type\":\"xsd:integer\",\"column_description\":\"Number of days the material is needed for production.\"}]},\"table_id3\":{\"name\":\"employee_data\",\"location\":\"C:/data/manufacturing/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"Table containing information about employees, not relevant to the current workflow.\",\"columns\":[{\"column\":\"employee_id\",\"column_name\":\"EMPLOYEE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"name\",\"column_name\":\"NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"}]}},\"instructions\":[\"Load the table 'raw_materials' which contains information about the materials used in production.\",\"Load the table 'production_schedule' which contains the schedule for manufacturing.\",\"Filter the 'raw_materials' table to include only materials with a stock level greater than 100.\",\"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\",\"Calculate a new column 'required_materials' by multiplying 'quantity_needed' by 'production_days' in the joined table.\",\"Aggregate the joined table to find the total 'required_materials' for each 'material_id'.\",\"Sort the aggregated table by 'required_materials' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/raw_materials.csv\",\"type\":\"csv\"},\"output\":\"raw_materials\",\"comment\":\"Loading the raw materials dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule dataset from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"raw_materials\",\"query\":\"STOCK_LEVEL > 100\"},\"output\":\"filtered_raw_materials\",\"comment\":\"Filtering raw materials to include only those with stock levels greater than 100.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_raw_materials\",\"production_schedule\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_table\",\"comment\":\"Joining filtered raw materials with production schedule on material_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_table\",\"columnName\":\"required_materials\",\"formula\":\"QUANTITY_NEEDED * PRODUCTION_DAYS\"},\"output\":\"joined_table_with_requirements\",\"comment\":\"Calculating required materials by multiplying quantity needed by production days.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_table_with_requirements\",\"groupBy\":\"MATERIAL_ID\",\"aggregations\":[{\"column\":\"required_materials\",\"name\":\"total_required_materials\",\"function\":\"sum\"}]},\"output\":\"aggregated_materials\",\"comment\":\"Aggregating to find total required materials for each material_id.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_materials\",\"sortBy\":\"total_required_materials\",\"order\":\"desc\"},\"output\":\"sorted_materials\",\"comment\":\"Sorting the aggregated materials by total required materials in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"production_schedule\",\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"sheet_name\":null,\"label\":\"Production Schedule\",\"description\":\"This table contains the production schedule with task details and scheduled dates.\",\"columns\":[{\"column\":\"task_id\",\"column_name\":\"TASK_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each production task.\"},{\"column\":\"scheduled_date\",\"column_name\":\"SCHEDULED_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the task is scheduled to be performed.\"},{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material required for the task.\"}]},\"table_id2\":{\"name\":\"inventory\",\"location\":\"C:/data/manufacturing/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory\",\"description\":\"This table contains current inventory levels of materials.\",\"columns\":[{\"column\":\"material_id\",\"column_name\":\"MATERIAL_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the material in inventory.\"},{\"column\":\"quantity_available\",\"column_name\":\"QUANTITY_AVAILABLE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The quantity of material currently available in inventory.\"}]},\"table_id3\":{\"name\":\"employee_schedule\",\"location\":\"C:/data/manufacturing/employee_schedule.xlsx\",\"sheet_name\":\"Schedule\",\"label\":\"Employee Schedule\",\"description\":\"This table contains the work schedule for employees.\",\"columns\":[{\"column\":\"employee_id\",\"column_name\":\"EMPLOYEE_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"work_date\",\"column_name\":\"WORK_DATE\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the employee is scheduled to work.\"}]}},\"instructions\":[\"Load the production schedule table from the CSV file.\",\"Filter the production schedule to include only tasks scheduled for the current week.\",\"Join the filtered schedule with the inventory table to check material availability.\",\"Aggregate the joined data to calculate total material requirements for the week.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/production_schedule.csv\",\"type\":\"csv\"},\"output\":\"production_schedule\",\"comment\":\"Loading the production schedule from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"production_schedule\",\"query\":\"SCHEDULED_DATE >= '2023-10-23' and SCHEDULED_DATE <= '2023-10-29'\"},\"output\":\"current_week_schedule\",\"comment\":\"Filtering the production schedule to include only tasks for the current week.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/manufacturing/inventory.csv\",\"type\":\"csv\"},\"output\":\"inventory\",\"comment\":\"Loading the inventory data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"current_week_schedule\",\"inventory\"],\"joinOn\":[\"MATERIAL_ID\"],\"joinType\":\"inner\"},\"output\":\"schedule_with_inventory\",\"comment\":\"Joining the filtered schedule with inventory to check material availability.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"schedule_with_inventory\",\"groupBy\":\"MATERIAL_ID\",\"aggregations\":[{\"column\":\"QUANTITY_AVAILABLE\",\"name\":\"total_material_required\",\"function\":\"sum\"}]},\"output\":\"weekly_material_requirements\",\"comment\":\"Aggregating data to calculate total material requirements for the week.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"employee_data\",\"location\":\"C:/data/employee_data.csv\",\"sheet_name\":null,\"label\":\"Employee Data\",\"description\":\"This table contains information about employees including their ID, name, salary, and hire date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"employee_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each employee.\"},{\"column\":\"col_2\",\"column_name\":\"name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the employee.\"},{\"column\":\"col_3\",\"column_name\":\"salary\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Salary of the employee.\"},{\"column\":\"col_4\",\"column_name\":\"hire_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the employee was hired.\"}]},\"table_id2\":{\"name\":\"department_data\",\"location\":\"C:/data/department_data.csv\",\"sheet_name\":null,\"label\":\"Department Data\",\"description\":\"This table contains information about departments including department ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"department_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each department.\"},{\"column\":\"col_2\",\"column_name\":\"department_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the department.\"}]}},\"instructions\":[\"Load the employee data table from the CSV file located at C:/data/employee_data.csv.\",\"Filter the employee data to include only employees with a salary greater than 50,000.\",\"Sort the filtered employee data by the 'hire_date' column in ascending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/employee_data.csv\",\"type\":\"csv\"},\"output\":\"employee_data\",\"comment\":\"Loading the employee data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"employee_data\",\"query\":\"salary > 50000\"},\"output\":\"filtered_employee_data\",\"comment\":\"Filtering employees with a salary greater than 50,000.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"filtered_employee_data\",\"sortBy\":\"hire_date\",\"order\":\"asc\"},\"output\":\"sorted_employee_data\",\"comment\":\"Sorting the filtered employee data by hire date in ascending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"legal_cases\",\"location\":\"C:/data/legal_cases.csv\",\"sheet_name\":null,\"label\":\"Legal Cases\",\"description\":\"A table containing information about various legal cases, including case IDs, start and end dates, and other details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each legal case.\"},{\"column\":\"col_2\",\"column_name\":\"start_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the legal case started.\"},{\"column\":\"col_3\",\"column_name\":\"end_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the legal case ended.\"},{\"column\":\"col_4\",\"column_name\":\"year\",\"column_type\":\"xsd:integer\",\"column_description\":\"The year in which the legal case was filed.\"}]},\"table_id2\":{\"name\":\"court_decisions\",\"location\":\"C:/data/court_decisions.xlsx\",\"sheet_name\":\"Decisions\",\"label\":\"Court Decisions\",\"description\":\"A table containing court decisions related to legal cases, including case IDs and judges.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each legal case.\"},{\"column\":\"col_2\",\"column_name\":\"judge\",\"column_type\":\"xsd:string\",\"column_description\":\"The judge who made the decision on the case.\"},{\"column\":\"col_3\",\"column_name\":\"court\",\"column_type\":\"xsd:string\",\"column_description\":\"The court where the decision was made.\"}]},\"table_id3\":{\"name\":\"irrelevant_table\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Irrelevant\",\"label\":\"Irrelevant Data\",\"description\":\"A table containing irrelevant data not related to legal cases.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"random_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Random identifier for irrelevant data.\"},{\"column\":\"col_B\",\"column_name\":\"random_value\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Random value for irrelevant data.\"}]}},\"instructions\":[\"Load the 'legal_cases' table from the CSV file located at 'C:/data/legal_cases.csv'.\",\"Load the 'court_decisions' table from the Excel file located at 'C:/data/court_decisions.xlsx', sheet 'Decisions'.\",\"Filter the 'legal_cases' table to include only cases from the year 2022.\",\"Join the filtered 'legal_cases' table with the 'court_decisions' table on the 'case_id' column using an inner join.\",\"Aggregate the joined table to count the number of cases per 'judge'.\",\"Sort the aggregated data by the count of cases in descending order.\",\"Calculate a new column 'case_duration_days' in the 'legal_cases' table by subtracting 'start_date' from 'end_date'.\",\"Filter the 'legal_cases' table to include only cases with 'case_duration_days' greater than 30.\",\"Join the filtered 'legal_cases' table with the 'court_decisions' table on the 'case_id' column using a left join.\",\"Aggregate the final joined table to find the average 'case_duration_days' per 'court'.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal_cases.csv\",\"type\":\"csv\"},\"output\":\"legal_cases\",\"comment\":\"Loading the legal cases dataset from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/court_decisions.xlsx\",\"type\":\"xlsx\"},\"output\":\"court_decisions\",\"comment\":\"Loading the court decisions dataset from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"legal_cases\",\"query\":\"year == 2022\"},\"output\":\"legal_cases_2022\",\"comment\":\"Filtering legal cases to include only those from the year 2022.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"legal_cases_2022\",\"court_decisions\"],\"joinOn\":[\"case_id\"],\"joinType\":\"inner\"},\"output\":\"joined_cases_decisions\",\"comment\":\"Joining filtered legal cases with court decisions on case_id using an inner join.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_cases_decisions\",\"groupBy\":\"judge\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"case_count\",\"function\":\"count\"}]},\"output\":\"cases_per_judge\",\"comment\":\"Aggregating data to count the number of cases per judge.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"cases_per_judge\",\"sortBy\":\"case_count\",\"order\":\"desc\"},\"output\":\"sorted_cases_per_judge\",\"comment\":\"Sorting the aggregated data by the count of cases in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"legal_cases\",\"columnName\":\"case_duration_days\",\"formula\":\"`end_date` - `start_date`\"},\"output\":\"legal_cases_with_duration\",\"comment\":\"Calculating case duration in days by subtracting start_date from end_date.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"legal_cases_with_duration\",\"query\":\"case_duration_days > 30\"},\"output\":\"long_duration_cases\",\"comment\":\"Filtering legal cases to include only those with a duration greater than 30 days.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"long_duration_cases\",\"court_decisions\"],\"joinOn\":[\"case_id\"],\"joinType\":\"left\"},\"output\":\"long_cases_with_decisions\",\"comment\":\"Joining long duration cases with court decisions on case_id using a left join.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"long_cases_with_decisions\",\"groupBy\":\"court\",\"aggregations\":[{\"column\":\"case_duration_days\",\"name\":\"average_duration\",\"function\":\"mean\"}]},\"output\":\"average_duration_per_court\",\"comment\":\"Aggregating data to find the average case duration per court.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"Contains details of all shipments including product IDs, quantities, and shipment status.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product being shipped.\"},{\"column\":\"column_3_id\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Quantity of the product in the shipment.\"},{\"column\":\"column_4_id\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment.\"}]},\"table_id2\":{\"name\":\"warehouse_inventory\",\"location\":\"C:/data/logistics/warehouse_inventory.xlsx\",\"sheet_name\":\"Inventory\",\"label\":\"Warehouse Inventory\",\"description\":\"Details of inventory available in each warehouse including product IDs and weights.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"warehouse_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each warehouse.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the product in the warehouse.\"},{\"column\":\"column_3_id\",\"column_name\":\"weight_per_unit\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of each unit of the product.\"}]},\"table_id3\":{\"name\":\"transport_routes\",\"location\":\"C:/data/logistics/transport_routes.csv\",\"sheet_name\":null,\"label\":\"Transport Routes\",\"description\":\"Contains information about various transport routes and their distances.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"route_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transport route.\"},{\"column\":\"column_2_id\",\"column_name\":\"distance\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Distance of the transport route in kilometers.\"}]}},\"instructions\":[\"Load the 'shipment_data' table from the CSV file located at C:/data/logistics/shipment_data.csv.\",\"Load the 'warehouse_inventory' table from the Excel file located at C:/data/logistics/warehouse_inventory.xlsx.\",\"Filter the 'shipment_data' table to include only shipments with a status of 'In Transit'.\",\"Join the filtered 'shipment_data' with 'warehouse_inventory' on the 'product_id' column using an inner join.\",\"Calculate a new column 'total_weight' in the joined table by multiplying 'quantity' by 'weight_per_unit'.\",\"Aggregate the joined table by 'warehouse_id' to find the total 'total_weight' for each warehouse.\",\"Sort the aggregated data by 'total_weight' in descending order.\",\"Filter the sorted data to include only warehouses with a 'total_weight' greater than 1000.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/warehouse_inventory.xlsx\",\"type\":\"xlsx\"},\"output\":\"warehouse_inventory\",\"comment\":\"Loading warehouse inventory data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"status == 'In Transit'\"},\"output\":\"in_transit_shipments\",\"comment\":\"Filtering shipments to include only those that are currently in transit.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"in_transit_shipments\",\"warehouse_inventory\"],\"joinOn\":[\"product_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining in-transit shipments with warehouse inventory on product_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"total_weight\",\"formula\":\"quantity * weight_per_unit\"},\"output\":\"joined_data_with_weight\",\"comment\":\"Calculating total weight for each shipment by multiplying quantity by weight per unit.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_weight\",\"groupBy\":\"warehouse_id\",\"aggregations\":[{\"column\":\"total_weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to find total weight for each warehouse.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_weight\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting warehouses by total weight in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_data\",\"query\":\"total_weight > 1000\"},\"output\":\"filtered_warehouses\",\"comment\":\"Filtering to include only warehouses with total weight greater than 1000.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"energy_consumption\",\"location\":\"C:/energy_data/energy_consumption.csv\",\"sheet_name\":null,\"label\":\"Energy Consumption Data\",\"description\":\"This table contains energy consumption records with timestamps and consumption values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The date and time of the energy consumption record.\"},{\"column\":\"col_2\",\"column_name\":\"consumption_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of energy consumed in kilowatt-hours.\"}]},\"table_id2\":{\"name\":\"energy_prices\",\"location\":\"C:/energy_data/energy_prices.csv\",\"sheet_name\":null,\"label\":\"Energy Prices Data\",\"description\":\"This table contains energy prices with timestamps and price values.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"timestamp\",\"column_type\":\"xsd:timeStamp\",\"column_description\":\"The date and time of the energy price record.\"},{\"column\":\"col_2\",\"column_name\":\"price_per_kWh\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The price of energy per kilowatt-hour.\"}]}},\"instructions\":[\"Load the energy consumption data from the CSV file located in the energy_data folder.\",\"Filter the data to include only records where the consumption is greater than 500 kWh.\",\"Aggregate the filtered data by month to calculate the total consumption for each month.\",\"Sort the aggregated data by total consumption in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/energy_data/energy_consumption.csv\",\"type\":\"csv\"},\"output\":\"energy_consumption\",\"comment\":\"Loading the energy consumption data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"energy_consumption\",\"query\":\"consumption_kWh > 500\"},\"output\":\"filtered_energy_consumption\",\"comment\":\"Filtering the data to include only records where the consumption is greater than 500 kWh.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_energy_consumption\",\"groupBy\":\"timestamp\",\"aggregations\":[{\"column\":\"consumption_kWh\",\"name\":\"total_monthly_consumption\",\"function\":\"sum\"}]},\"output\":\"monthly_energy_consumption\",\"comment\":\"Aggregating the filtered data by month to calculate the total consumption for each month.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"monthly_energy_consumption\",\"sortBy\":\"total_monthly_consumption\",\"order\":\"desc\"},\"output\":\"sorted_monthly_energy_consumption\",\"comment\":\"Sorting the aggregated data by total consumption in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"shipment_data\",\"location\":\"C:/data/logistics/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Data\",\"description\":\"This table contains details of all shipments including shipment ID, customer ID, status, and weight.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"shipment_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer associated with the shipment.\"},{\"column\":\"column_3_id\",\"column_name\":\"status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current status of the shipment (e.g., delivered, in transit).\"},{\"column\":\"column_4_id\",\"column_name\":\"weight\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Weight of the shipment in kilograms.\"}]},\"table_id2\":{\"name\":\"customer_info\",\"location\":\"C:/data/logistics/customer_info.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Information\",\"description\":\"This table contains customer details including customer ID and name.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/logistics/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"This table contains information about the current inventory levels of various products.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the product.\"}]}},\"instructions\":[\"Load the table 'shipment_data' which contains details of all shipments.\",\"Filter the shipment data to include only shipments with a status of 'delivered'.\",\"Join the filtered shipment data with the 'customer_info' table on the 'customer_id' column.\",\"Aggregate the joined data to calculate the total weight of shipments delivered to each customer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading the shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"status == 'delivered'\"},\"output\":\"delivered_shipments\",\"comment\":\"Filtering shipment data to include only delivered shipments.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics/customer_info.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_info\",\"comment\":\"Loading customer information from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"delivered_shipments\",\"customer_info\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining delivered shipment data with customer information on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"customer_id\",\"aggregations\":[{\"column\":\"weight\",\"name\":\"total_weight\",\"function\":\"sum\"}]},\"output\":\"customer_shipment_weights\",\"comment\":\"Aggregating data to calculate the total weight of shipments delivered to each customer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"case_records\",\"location\":\"C:/data/legal/case_records.csv\",\"sheet_name\":null,\"label\":\"Legal Case Records\",\"description\":\"A table containing detailed information about legal cases, including case ID, lawyer ID, and case date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"case_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each legal case.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the lawyer handling the case.\"},{\"column\":\"col_3\",\"column_name\":\"case_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the case was filed.\"}]},\"table_id2\":{\"name\":\"lawyers\",\"location\":\"C:/data/legal/lawyers.csv\",\"sheet_name\":null,\"label\":\"Lawyers Information\",\"description\":\"A table containing information about lawyers, including lawyer ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"lawyer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each lawyer.\"},{\"column\":\"col_2\",\"column_name\":\"lawyer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the lawyer.\"}]},\"table_id3\":{\"name\":\"court_sessions\",\"location\":\"C:/data/legal/court_sessions.csv\",\"sheet_name\":null,\"label\":\"Court Sessions\",\"description\":\"A table containing information about court sessions, including session ID and date.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"session_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each court session.\"},{\"column\":\"col_2\",\"column_name\":\"session_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the court session.\"}]}},\"instructions\":[\"Load the table 'case_records' which contains information about legal cases.\",\"Filter the 'case_records' table to include only cases from the year 2022.\",\"Join the filtered 'case_records' table with the 'lawyers' table on the 'lawyer_id' column.\",\"Aggregate the joined table to find the total number of cases handled by each lawyer.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/case_records.csv\",\"type\":\"csv\"},\"output\":\"case_records\",\"comment\":\"Loading the table containing legal case records from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"case_records\",\"query\":\"`case_date`.year == 2022\"},\"output\":\"filtered_case_records\",\"comment\":\"Filtering the case records to include only cases from the year 2022.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/legal/lawyers.csv\",\"type\":\"csv\"},\"output\":\"lawyers\",\"comment\":\"Loading the table containing information about lawyers from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_case_records\",\"lawyers\"],\"joinOn\":[\"lawyer_id\"],\"joinType\":\"inner\"},\"output\":\"cases_with_lawyers\",\"comment\":\"Joining the filtered case records with the lawyers table on the lawyer_id column.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"cases_with_lawyers\",\"groupBy\":\"lawyer_name\",\"aggregations\":[{\"column\":\"case_id\",\"name\":\"total_cases\",\"function\":\"count\"}]},\"output\":\"lawyer_case_counts\",\"comment\":\"Aggregating the joined table to find the total number of cases handled by each lawyer.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"product_specifications\",\"location\":\"C:/data/product_specifications.csv\",\"sheet_name\":null,\"label\":\"Product Specifications\",\"description\":\"This table contains detailed specifications of products, including development status and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"development_status\",\"column_type\":\"xsd:string\",\"column_description\":\"Current development status of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category to which the product belongs.\"},{\"column\":\"col_4\",\"column_name\":\"development_time\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Time taken for product development in days.\"}]},\"table_id2\":{\"name\":\"product_sales\",\"location\":\"C:/data/product_sales.csv\",\"sheet_name\":null,\"label\":\"Product Sales\",\"description\":\"This table contains sales data for products, including sales volume and revenue.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"sales_volume\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Number of units sold.\"},{\"column\":\"col_3\",\"column_name\":\"revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from sales.\"}]}},\"instructions\":[\"Load the product specifications table from the CSV file.\",\"Filter the product specifications to include only products with a development status of 'in progress'.\",\"Aggregate the filtered data to calculate the average development time for each product category.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_specifications.csv\",\"type\":\"csv\"},\"output\":\"product_specifications\",\"comment\":\"Loading the product specifications table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"product_specifications\",\"query\":\"development_status == 'in progress'\"},\"output\":\"in_progress_products\",\"comment\":\"Filtering the product specifications to include only products with a development status of 'in progress'.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"in_progress_products\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"development_time\",\"name\":\"average_development_time\",\"function\":\"mean\"}]},\"output\":\"average_development_time_by_category\",\"comment\":\"Aggregating the filtered data to calculate the average development time for each product category.\"}]}"}
