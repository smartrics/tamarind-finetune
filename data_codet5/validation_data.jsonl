{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers, including their sustainability scores.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_score\",\"column_name\":\"SUSTAINABILITY_SCORE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Score representing the sustainability level of the supplier.\"},{\"column\":\"supplier_name\",\"column_name\":\"SUPPLIER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/supply_chain/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Contains details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the product to its supplier.\"},{\"column\":\"category\",\"column_name\":\"CATEGORY\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/supply_chain/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains logistics information, not relevant for current workflow.\",\"columns\":[{\"column\":\"logistics_id\",\"column_name\":\"LOGISTICS_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"transport_mode\",\"column_name\":\"TRANSPORT_MODE\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transport used for logistics.\"}]}},\"instructions\":[\"Load the table 'supplier_data' which contains information about suppliers and their sustainability scores.\",\"Filter the supplier data to include only those with a sustainability score above 80.\",\"Join the filtered supplier data with the 'product_data' table on the supplier ID to get product details.\",\"Aggregate the joined data to calculate the total number of sustainable products per category.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data to access sustainability scores.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_SCORE > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with sustainability scores above 80.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data to join with filtered supplier data.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"CATEGORY\",\"aggregations\":[{\"column\":\"PRODUCT_ID\",\"name\":\"total_sustainable_products\",\"function\":\"count\"}]},\"output\":\"sustainable_products_per_category\",\"comment\":\"Aggregating data to calculate the total number of sustainable products per category.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"sales_2023\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2023\",\"description\":\"Contains sales transactions for the year 2023, including total revenue and cost.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"total_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total cost incurred for the transaction.\"},{\"column\":\"col_5\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the transaction took place.\"},{\"column\":\"col_6\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the transaction.\"}]},\"table_2\":{\"name\":\"customers\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Information\",\"description\":\"Contains customer details including customer ID and contact information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_number\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact number of the customer.\"}]},\"table_3\":{\"name\":\"inventory\",\"location\":\"C:/data/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"Contains inventory details including product ID and stock levels.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:integer\",\"column_description\":\"Current stock level of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Load the customer information from the Excel file located at C:/data/customers.xlsx.\",\"Filter the sales data to include only transactions with a total revenue greater than 1000.\",\"Join the filtered sales data with the customer information based on the customer ID.\",\"Calculate a new column for profit by subtracting total cost from total revenue in the joined table.\",\"Aggregate the data by region to calculate the total sales and average profit.\",\"Sort the aggregated data by total sales in descending order.\",\"Apply a log transformation to the total sales column to stabilize variance.\",\"Forecast the next 12 months of sales using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_2023\",\"comment\":\"Loading sales data for 2023 from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customers\",\"comment\":\"Loading customer information from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_2023\",\"query\":\"total_revenue > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customers\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer information based on customer ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sales_with_customers\",\"columnName\":\"profit\",\"formula\":\"total_revenue - total_cost\"},\"output\":\"sales_with_profit\",\"comment\":\"Calculating profit by subtracting total cost from total revenue.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_profit\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"},{\"column\":\"profit\",\"name\":\"average_profit\",\"function\":\"mean\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating data by region to calculate total sales and average profit.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_regional_sales\",\"comment\":\"Sorting aggregated data by total sales in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_regional_sales\",\"transformations\":[{\"columns\":[\"total_sales\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_sales\",\"comment\":\"Applying a log transformation to the total sales column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_sales\",\"columns\":[\"total_sales\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"date\"}},\"output\":\"forecasted_sales\",\"comment\":\"Forecasting the next 12 months of sales using the Holt-Winters method.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/forecasted_sales.csv\",\"type\":\"csv\"},\"output\":\"forecasted_sales\",\"comment\":\"Saving the final forecasted data to a new CSV file.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'forecastData' predict multiple future time points in a single action?","output":"Yes, 'forecastData' can predict multiple future time points based on the 'forecastPeriod' parameter."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nIs it possible to mix arithmetic operations and functions in a single expression?","output":"Yes, functions and arithmetic operations can be combined, such as 'log(col1) + sqrt(col2)'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat should be done if a transformation in 'applyTransformations' requires additional parameters?","output":"If a transformation requires additional parameters, they must be provided in the 'parameters' field within the transformation definition."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'identifyTable' load multiple tables from different sources in a single action?","output":"No, each 'identifyTable' action loads a single table from a specified source."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid 'applyTransformations' action using multiple transformations.","output":"{ \"name\": \"applyTransformations\", \"parameters\": { \"table\": \"customer_data\", \"transformations\": [ { \"columns\": [\"age\"], \"operation\": \"standardize\" }, { \"columns\": [\"income\"], \"operation\": \"min_max_scale\", \"parameters\": { \"range\": [0,1] } } ] }, \"output\": \"normalized_customer_data\" }"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"This table contains sales transactions for the year 2023, including details such as transaction ID, customer ID, product ID, quantity, and total revenue.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Information\",\"description\":\"This table contains customer information, including customer ID, name, and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_id3\":{\"name\":\"product_data\",\"location\":\"C:/data/products.csv\",\"sheet_name\":null,\"label\":\"Product Information\",\"description\":\"This table contains product information, including product ID, name, and category.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Filter the sales data to include only transactions where the total revenue is greater than 1000.\",\"Join the filtered sales data with customer information from the Excel file located at C:/data/customers.xlsx.\",\"Aggregate the joined data to calculate the total revenue per region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data for 2023 from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`total_revenue` > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_data\",\"comment\":\"Loading customer information from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customer_data\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer information based on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_customers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating sales data by region to compute total revenue.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid Boolean expression using a comparison and logical operator.","output":"'(col1 > 10) and (col2 < 5)'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers, including sector and production details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"sector\",\"column_type\":\"xsd:string\",\"column_description\":\"The sector in which the supplier operates.\"},{\"column\":\"col_3\",\"column_name\":\"units_produced\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The number of units produced by the supplier.\"}]},\"table_id2\":{\"name\":\"emissions_data\",\"location\":\"C:/data/emissions_data.xlsx\",\"sheet_name\":null,\"label\":\"Emissions Data\",\"description\":\"Contains emissions data related to suppliers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"total_emissions\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total emissions produced by the supplier.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region where the supplier is located.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains logistics information, not relevant for emissions analysis.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"logistics_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for logistics records.\"},{\"column\":\"col_2\",\"column_name\":\"transport_mode\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transport used.\"}]}},\"instructions\":[\"Load the 'supplier_data' table from the CSV file located at C:/data/supplier_data.csv.\",\"Load the 'emissions_data' table from the Excel file located at C:/data/emissions_data.xlsx.\",\"Filter the 'supplier_data' table to include only suppliers from the 'Renewable Energy' sector.\",\"Join the filtered 'supplier_data' with 'emissions_data' on the 'supplier_id' column using an inner join.\",\"Calculate a new column 'emissions_per_unit' in the joined table by dividing 'total_emissions' by 'units_produced'.\",\"Aggregate the data by 'region' to find the total 'emissions_per_unit' for each region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/emissions_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"emissions_data\",\"comment\":\"Loading emissions data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"sector == 'Renewable Energy'\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers to include only those in the Renewable Energy sector.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"emissions_data\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with emissions data on supplier_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"emissions_per_unit\",\"formula\":\"total_emissions / units_produced\"},\"output\":\"joined_data_with_emissions\",\"comment\":\"Calculating emissions per unit by dividing total emissions by units produced.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_emissions\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"emissions_per_unit\",\"name\":\"total_emissions_per_unit\",\"function\":\"sum\"}]},\"output\":\"regional_emissions\",\"comment\":\"Aggregating emissions per unit by region.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'applyTimeSeriesTransformations' be used for multiple transformations in one action?","output":"Yes, 'applyTimeSeriesTransformations' allows multiple transformations to be applied sequentially in a single action."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat is the difference between 'rolling_mean' and 'expanding_mean' in time series transformations?","output":"'rolling_mean' computes the moving average over a fixed window, while 'expanding_mean' computes the cumulative mean over the entire dataset up to each point."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if 'filterData' is applied with an empty query condition?","output":"If an empty query condition is provided, an error is raised since a valid filter expression is required."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supply_chain_data\",\"location\":\"C:/data/supply_chain.csv\",\"sheet_name\":null,\"label\":\"Supply Chain Data\",\"description\":\"This table contains data on supply chain activities, including carbon emissions and units supplied.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"carbon_emission\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of carbon emissions in kilograms.\"},{\"column\":\"column_3_id\",\"column_name\":\"units_supplied\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The number of units supplied by the supplier.\"}]},\"table_id2\":{\"name\":\"supplier_info\",\"location\":\"C:/data/supplier_info.xlsx\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"This table contains detailed information about suppliers, including their IDs and names.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the supplier.\"}]},\"table_id3\":{\"name\":\"transport_data\",\"location\":\"C:/data/transport_data.csv\",\"sheet_name\":null,\"label\":\"Transport Data\",\"description\":\"This table contains data on transportation activities, including distances and modes of transport.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"transport_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transport activity.\"},{\"column\":\"column_2_id\",\"column_name\":\"distance\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The distance covered in kilometers.\"},{\"column\":\"column_3_id\",\"column_name\":\"mode_of_transport\",\"column_type\":\"xsd:string\",\"column_description\":\"The mode of transport used.\"}]}},\"instructions\":[\"Load the 'supply_chain_data' table from the CSV file located at C:/data/supply_chain.csv.\",\"Filter the 'supply_chain_data' to include only records where the 'carbon_emission' is greater than 1000.\",\"Load the 'supplier_info' table from the Excel file located at C:/data/supplier_info.xlsx.\",\"Join the filtered supply chain data with the supplier information on the 'supplier_id' column using an inner join.\",\"Aggregate the joined data to calculate the total carbon emissions per supplier.\",\"Sort the aggregated data by total carbon emissions in descending order.\",\"Calculate a new column 'emission_per_unit' by dividing 'total_emissions' by 'units_supplied'.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain.csv\",\"type\":\"csv\"},\"output\":\"supply_chain_data\",\"comment\":\"Loading the supply chain data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supply_chain_data\",\"query\":\"carbon_emission > 1000\"},\"output\":\"filtered_supply_chain_data\",\"comment\":\"Filtering supply chain data to include only records with carbon emissions greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_info.xlsx\",\"type\":\"xlsx\"},\"output\":\"supplier_info\",\"comment\":\"Loading the supplier information from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supply_chain_data\",\"supplier_info\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supply chain data with supplier information on supplier_id using an inner join.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"supplier_id\",\"aggregations\":[{\"column\":\"carbon_emission\",\"name\":\"total_emissions\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined data to calculate total carbon emissions per supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_emissions\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total carbon emissions in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_data\",\"columnName\":\"emission_per_unit\",\"formula\":\"total_emissions / units_supplied\"},\"output\":\"final_data\",\"comment\":\"Calculating a new column 'emission_per_unit' by dividing total emissions by units supplied.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers including their sustainability scores.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"sustainability_score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Score representing the sustainability performance of the supplier.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the supplier is located.\"}]},\"table_id2\":{\"name\":\"emissions_data\",\"location\":\"C:/data/emissions_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Emissions Data\",\"description\":\"Contains emissions data for each supplier across different scopes.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"emissions_scope1\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Emissions data for scope 1.\"},{\"column\":\"col_3\",\"column_name\":\"emissions_scope2\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Emissions data for scope 2.\"},{\"column\":\"col_4\",\"column_name\":\"emissions_scope3\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Emissions data for scope 3.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"random_column\",\"column_type\":\"xsd:string\",\"column_description\":\"A random column not used in the workflow.\"}]}},\"instructions\":[\"Load the 'supplier_data' table from the CSV file located at C:/data/supplier_data.csv.\",\"Load the 'emissions_data' table from the Excel file located at C:/data/emissions_data.xlsx.\",\"Filter the 'supplier_data' table to include only suppliers with a sustainability score above 80.\",\"Join the filtered 'supplier_data' with the 'emissions_data' on the 'supplier_id' column using an inner join.\",\"Calculate a new column 'total_emissions' in the joined table by summing 'emissions_scope1', 'emissions_scope2', and 'emissions_scope3'.\",\"Aggregate the data by 'region' to calculate the total emissions for each region.\",\"Sort the aggregated data by 'total_emissions' in descending order.\",\"Apply a log transformation to the 'total_emissions' column to stabilize variance.\",\"Filter the transformed data to include only regions with total emissions above a threshold of 1000.\",\"Forecast future emissions for the next 12 months using the 'holt_winters' algorithm on the 'total_emissions' column.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/emissions_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"emissions_data\",\"comment\":\"Loading emissions data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"sustainability_score > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with a sustainability score above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"emissions_data\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with emissions data on supplier_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"total_emissions\",\"formula\":\"emissions_scope1 + emissions_scope2 + emissions_scope3\"},\"output\":\"data_with_total_emissions\",\"comment\":\"Calculating total emissions by summing emissions across all scopes.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_total_emissions\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_emissions\",\"name\":\"total_emissions\",\"function\":\"sum\"}]},\"output\":\"regional_emissions\",\"comment\":\"Aggregating total emissions by region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_emissions\",\"sortBy\":\"total_emissions\",\"order\":\"desc\"},\"output\":\"sorted_regional_emissions\",\"comment\":\"Sorting regions by total emissions in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_regional_emissions\",\"transformations\":[{\"columns\":[\"total_emissions\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_emissions\",\"comment\":\"Applying log transformation to total emissions.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"transformed_emissions\",\"query\":\"total_emissions > 1000\"},\"output\":\"high_emission_regions\",\"comment\":\"Filtering regions with total emissions above 1000.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"high_emission_regions\",\"columns\":[\"total_emissions\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"region\"}},\"output\":\"emissions_forecast\",\"comment\":\"Forecasting future emissions for the next 12 months using Holt-Winters.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'applyTransformations' be used for feature engineering in machine learning?","output":"Yes, 'applyTransformations' can be used for feature engineering, such as normalization, standardization, and applying mathematical transformations to enhance dataset usability."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers including their sustainability scores.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"sustainability_score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Score representing the sustainability practices of the supplier.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Geographical region of the supplier.\"}]},\"table_id2\":{\"name\":\"emissions_data\",\"location\":\"C:/data/emissions_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Emissions Data\",\"description\":\"Contains emissions data for each supplier including carbon and methane emissions.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"carbon_emissions\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount of carbon emissions produced by the supplier.\"},{\"column\":\"col_3\",\"column_name\":\"methane_emissions\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Amount of methane emissions produced by the supplier.\"}]},\"table_id3\":{\"name\":\"transport_data\",\"location\":\"C:/data/transport_data.csv\",\"sheet_name\":null,\"label\":\"Transport Data\",\"description\":\"Contains data on transportation modes and distances for each supplier.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"transport_mode\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transportation used by the supplier.\"},{\"column\":\"col_3\",\"column_name\":\"distance\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Distance covered by the transportation mode.\"}]}},\"instructions\":[\"Load the 'supplier_data' table from the CSV file.\",\"Load the 'emissions_data' table from the Excel file.\",\"Filter the 'supplier_data' to include only suppliers with a sustainability score above 80.\",\"Join the filtered 'supplier_data' with 'emissions_data' on the 'supplier_id' column.\",\"Calculate a new column 'total_emissions' by summing 'carbon_emissions' and 'methane_emissions'.\",\"Aggregate the joined table by 'region' to calculate the total emissions per region.\",\"Sort the aggregated data by 'total_emissions' in descending order.\",\"Filter the sorted data to include only regions with total emissions above 1000.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/emissions_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"emissions_data\",\"comment\":\"Loading emissions data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"sustainability_score > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with a sustainability score above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"emissions_data\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with emissions data on supplier_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"total_emissions\",\"formula\":\"carbon_emissions + methane_emissions\"},\"output\":\"data_with_total_emissions\",\"comment\":\"Calculating total emissions by summing carbon and methane emissions.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_total_emissions\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_emissions\",\"name\":\"total_emissions_per_region\",\"function\":\"sum\"}]},\"output\":\"regional_emissions\",\"comment\":\"Aggregating data by region to calculate total emissions per region.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_emissions\",\"sortBy\":\"total_emissions_per_region\",\"order\":\"desc\"},\"output\":\"sorted_regional_emissions\",\"comment\":\"Sorting regions by total emissions in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_regional_emissions\",\"query\":\"total_emissions_per_region > 1000\"},\"output\":\"high_emission_regions\",\"comment\":\"Filtering regions with total emissions above 1000.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers including their sustainability scores.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_score\",\"column_name\":\"SUSTAINABILITY_SCORE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability score of the supplier.\"},{\"column\":\"supplier_name\",\"column_name\":\"SUPPLIER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/supply_chain/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Contains details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the product to its supplier.\"},{\"column\":\"category\",\"column_name\":\"CATEGORY\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/supply_chain/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains logistics information not relevant to the current workflow.\",\"columns\":[{\"column\":\"logistics_id\",\"column_name\":\"LOGISTICS_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"transport_mode\",\"column_name\":\"TRANSPORT_MODE\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transport used for logistics.\"}]}},\"instructions\":[\"Load the 'supplier_data' table containing information about suppliers and their sustainability scores.\",\"Load the 'product_data' table which includes details about products and their associated suppliers.\",\"Filter the 'supplier_data' to include only suppliers with a sustainability score above 80.\",\"Join the filtered supplier data with the 'product_data' on the supplier ID.\",\"Aggregate the joined data to calculate the average sustainability score for each product category.\",\"Sort the aggregated data by average sustainability score in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data containing sustainability scores.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data with associated supplier information.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_SCORE > 80\"},\"output\":\"filtered_suppliers\",\"comment\":\"Filtering suppliers with sustainability scores above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_suppliers\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"CATEGORY\",\"aggregations\":[{\"column\":\"SUSTAINABILITY_SCORE\",\"name\":\"avg_sustainability_score\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating average sustainability score for each product category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"avg_sustainability_score\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting data by average sustainability score in descending order.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/sorted_sustainability_data.csv\",\"type\":\"csv\"},\"output\":\"sorted_sustainability_data\",\"comment\":\"Saving the sorted data for further analysis.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan multiple transformations be applied in a single 'applyTransformations' action?","output":"Yes, 'applyTransformations' supports applying multiple transformations in sequence within a single action."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nAre expressions evaluated strictly left to right?","output":"No, expressions follow standard operator precedence, meaning multiplication and division are evaluated before addition and subtraction unless parentheses are used."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_2023\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2023\",\"description\":\"Contains sales transactions for the year 2023, including total revenue and total cost.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"column_3_id\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"},{\"column\":\"column_4_id\",\"column_name\":\"total_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total cost incurred for the transaction.\"},{\"column\":\"column_5_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Geographical region where the transaction took place.\"}]},\"table_id2\":{\"name\":\"customers\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Information\",\"description\":\"Contains customer details including customer_id and contact information.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"contact_email\",\"column_type\":\"xsd:string\",\"column_description\":\"Email address of the customer.\"}]},\"table_id3\":{\"name\":\"inventory\",\"location\":\"C:/data/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"Contains inventory details including product_id and stock levels.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:integer\",\"column_description\":\"Current stock level of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Load the customer information from the Excel file located at C:/data/customers.xlsx.\",\"Filter the sales data to include only transactions with a total revenue greater than 1000.\",\"Join the filtered sales data with the customer information using the customer_id as the key.\",\"Calculate a new column in the joined table to determine the profit margin by subtracting total cost from total revenue.\",\"Aggregate the data by region to calculate the total sales and average profit margin.\",\"Sort the aggregated data by total sales in descending order.\",\"Apply a log transformation to the average profit margin to stabilize variance.\",\"Forecast the total sales for the next 12 months using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_2023\",\"comment\":\"Loading the sales data for 2023 from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customers\",\"comment\":\"Loading customer information from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_2023\",\"query\":\"`total_revenue` > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customers\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer information using customer_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sales_with_customers\",\"columnName\":\"profit_margin\",\"formula\":\"`total_revenue` - `total_cost`\"},\"output\":\"sales_with_profit\",\"comment\":\"Calculating profit margin by subtracting total cost from total revenue.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_profit\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"},{\"column\":\"profit_margin\",\"name\":\"average_profit_margin\",\"function\":\"mean\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating data by region to calculate total sales and average profit margin.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_regional_sales\",\"comment\":\"Sorting aggregated data by total sales in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_regional_sales\",\"transformations\":[{\"columns\":[\"average_profit_margin\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_sales\",\"comment\":\"Applying log transformation to average profit margin to stabilize variance.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sales_2023\",\"columns\":[\"total_revenue\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"dateColumn\":\"transaction_date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12}},\"output\":\"sales_forecast\",\"comment\":\"Forecasting total sales for the next 12 months using Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers, including their sustainability ratings.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_rating\",\"column_name\":\"SUSTAINABILITY_RATING\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability rating of the supplier, ranging from 0 to 100.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Contains details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]},\"table_id3\":{\"name\":\"transport_data\",\"location\":\"C:/data/transport_data.csv\",\"sheet_name\":null,\"label\":\"Transport Data\",\"description\":\"Contains information about transportation methods and emissions.\",\"columns\":[{\"column\":\"transport_id\",\"column_name\":\"TRANSPORT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transport method.\"},{\"column\":\"emissions\",\"column_name\":\"EMISSIONS\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Emissions associated with the transport method.\"}]}},\"instructions\":[\"Load the 'supplier_data' table which contains information about suppliers and their sustainability ratings.\",\"Load the 'product_data' table which contains details about products and their associated suppliers.\",\"Filter the 'supplier_data' to include only suppliers with a sustainability rating above 80.\",\"Join the filtered supplier data with the 'product_data' on the supplier ID.\",\"Aggregate the joined data to calculate the total number of sustainable products per supplier.\",\"Sort the aggregated data by the total number of sustainable products in descending order.\",\"Calculate a new column 'sustainability_score' by multiplying the sustainability rating by the number of sustainable products.\",\"Filter the data to include only suppliers with a 'sustainability_score' above 500.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data containing sustainability ratings.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data with associated suppliers.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_RATING > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with sustainability rating above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"PRODUCT_ID\",\"name\":\"total_sustainable_products\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total sustainable products per supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_sustainable_products\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting data by total number of sustainable products in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_data\",\"columnName\":\"sustainability_score\",\"formula\":\"SUSTAINABILITY_RATING * total_sustainable_products\"},\"output\":\"data_with_score\",\"comment\":\"Calculating sustainability score by multiplying rating by number of sustainable products.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"data_with_score\",\"query\":\"sustainability_score > 500\"},\"output\":\"high_score_suppliers\",\"comment\":\"Filtering suppliers with a sustainability score above 500.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers and their sustainability scores.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_score\",\"column_name\":\"SUSTAINABILITY_SCORE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability score of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]},\"table_id3\":{\"name\":\"transport_data\",\"location\":\"C:/data/transport_data.csv\",\"sheet_name\":null,\"label\":\"Transport Data\",\"description\":\"Information about transportation modes and their emissions.\",\"columns\":[{\"column\":\"transport_mode\",\"column_name\":\"TRANSPORT_MODE\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transportation.\"},{\"column\":\"emissions\",\"column_name\":\"EMISSIONS\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Emissions in CO2e for the transportation mode.\"}]},\"table_id4\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"irrelevant_column\",\"column_name\":\"IRRELEVANT_COLUMN\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the table 'supplier_data' containing information about suppliers and their sustainability scores.\",\"Load the table 'product_data' which includes details about products and their associated suppliers.\",\"Filter the 'supplier_data' to include only suppliers with a sustainability score above 80.\",\"Join the filtered supplier data with the product data on the supplier ID.\",\"Aggregate the joined data to calculate the total number of sustainable products per supplier.\",\"Sort the aggregated data by the total number of sustainable products in descending order.\",\"Load the table 'transport_data' which contains information about transportation modes and emissions.\",\"Filter the 'transport_data' to include only transportation modes with emissions below 50 CO2e.\",\"Join the sorted supplier-product data with the filtered transport data on the transportation mode.\",\"Calculate a new column 'sustainability_index' by combining the sustainability score and transportation emissions.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data containing sustainability scores.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data with associated suppliers.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_SCORE > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with sustainability scores above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_supplier_product_data\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_supplier_product_data\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"PRODUCT_ID\",\"name\":\"total_sustainable_products\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Calculating total number of sustainable products per supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_sustainable_products\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting suppliers by total number of sustainable products in descending order.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/transport_data.csv\",\"type\":\"csv\"},\"output\":\"transport_data\",\"comment\":\"Loading transportation data with emissions information.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"transport_data\",\"query\":\"EMISSIONS < 50\"},\"output\":\"filtered_transport_data\",\"comment\":\"Filtering transportation modes with emissions below 50 CO2e.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"sorted_aggregated_data\",\"filtered_transport_data\"],\"joinOn\":[\"TRANSPORT_MODE\"],\"joinType\":\"inner\"},\"output\":\"final_joined_data\",\"comment\":\"Joining sorted supplier-product data with filtered transport data on transportation mode.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"final_joined_data\",\"columnName\":\"sustainability_index\",\"formula\":\"SUSTAINABILITY_SCORE - EMISSIONS\"},\"output\":\"final_data_with_index\",\"comment\":\"Calculating sustainability index by combining sustainability score and transportation emissions.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains details about suppliers including their sustainability scores.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_score\",\"column_name\":\"SUSTAINABILITY_SCORE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Score representing the sustainability of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Contains product information and associated supplier IDs.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains data on transportation emissions for each supplier.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier.\"},{\"column\":\"transport_emissions\",\"column_name\":\"TRANSPORT_EMISSIONS\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Emissions from transportation for each supplier.\"}]},\"table_id4\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"irrelevant_column\",\"column_name\":\"IRRELEVANT_COLUMN\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the 'supplier_data' table containing supplier details and sustainability scores.\",\"Load the 'product_data' table with product information and associated supplier IDs.\",\"Filter the 'supplier_data' table to include only suppliers with a sustainability score above 80.\",\"Join the filtered supplier data with the 'product_data' table on the supplier ID.\",\"Aggregate the joined data to calculate the total number of products supplied by each supplier.\",\"Sort the aggregated data by the total number of products in descending order.\",\"Calculate a new column 'sustainability_impact' by multiplying the sustainability score by the total number of products.\",\"Filter the data to include only suppliers with a 'sustainability_impact' greater than 5000.\",\"Join the filtered data with 'logistics_data' to include transportation emissions.\",\"Aggregate the final data to calculate the average transportation emissions per supplier.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data containing sustainability scores.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data with supplier IDs.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_SCORE > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with sustainability scores above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_supplier_product_data\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_supplier_product_data\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"PRODUCT_ID\",\"name\":\"total_products\",\"function\":\"count\"}]},\"output\":\"aggregated_supplier_data\",\"comment\":\"Aggregating data to calculate total number of products per supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_supplier_data\",\"sortBy\":\"total_products\",\"order\":\"desc\"},\"output\":\"sorted_supplier_data\",\"comment\":\"Sorting suppliers by total number of products in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_supplier_data\",\"columnName\":\"sustainability_impact\",\"formula\":\"SUSTAINABILITY_SCORE * total_products\"},\"output\":\"supplier_data_with_impact\",\"comment\":\"Calculating sustainability impact by multiplying score by total products.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data_with_impact\",\"query\":\"sustainability_impact > 5000\"},\"output\":\"high_impact_suppliers\",\"comment\":\"Filtering suppliers with sustainability impact greater than 5000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/logistics_data.csv\",\"type\":\"csv\"},\"output\":\"logistics_data\",\"comment\":\"Loading logistics data to include transportation emissions.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_impact_suppliers\",\"logistics_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"final_supplier_data\",\"comment\":\"Joining high impact suppliers with logistics data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"final_supplier_data\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"TRANSPORT_EMISSIONS\",\"name\":\"average_emissions\",\"function\":\"mean\"}]},\"output\":\"average_emissions_per_supplier\",\"comment\":\"Calculating average transportation emissions per supplier.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains supplier details including their sustainability scores.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_score\",\"column_name\":\"SUSTAINABILITY_SCORE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Score representing the sustainability practices of the supplier.\"}]},\"table_id2\":{\"name\":\"shipment_data\",\"location\":\"C:/data/supply_chain/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Details\",\"description\":\"Includes details about shipments and their carbon emissions.\",\"columns\":[{\"column\":\"shipment_id\",\"column_name\":\"SHIPMENT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the shipment to a supplier.\"},{\"column\":\"carbon_emissions\",\"column_name\":\"CARBON_EMISSIONS\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The amount of carbon emissions in kilograms for the shipment.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/supply_chain/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Information\",\"description\":\"Contains inventory levels and product details, not relevant for this workflow.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"stock_level\",\"column_name\":\"STOCK_LEVEL\",\"column_type\":\"xsd:integer\",\"column_description\":\"Current stock level of the product.\"}]}},\"instructions\":[\"Load the 'supplier_data' table containing information about suppliers and their sustainability scores.\",\"Load the 'shipment_data' table which includes details about shipments and their carbon emissions.\",\"Filter the 'shipment_data' to include only shipments with carbon emissions greater than 1000 kg.\",\"Join the filtered 'shipment_data' with 'supplier_data' on the supplier ID to get sustainability scores for high-emission shipments.\",\"Aggregate the joined data by supplier to calculate the total carbon emissions and average sustainability score.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data containing sustainability scores.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data with carbon emissions details.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"shipment_data\",\"query\":\"CARBON_EMISSIONS > 1000\"},\"output\":\"high_emission_shipments\",\"comment\":\"Filtering shipments to include only those with carbon emissions greater than 1000 kg.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_emission_shipments\",\"supplier_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining high-emission shipments with supplier data to get sustainability scores.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"CARBON_EMISSIONS\",\"name\":\"total_carbon_emissions\",\"function\":\"sum\"},{\"column\":\"SUSTAINABILITY_SCORE\",\"name\":\"average_sustainability_score\",\"function\":\"mean\"}]},\"output\":\"supplier_emissions_summary\",\"comment\":\"Aggregating data by supplier to calculate total carbon emissions and average sustainability score.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers including their sustainability ratings.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_rating\",\"column_name\":\"SUSTAINABILITY_RATING\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Rating of the supplier's sustainability practices.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Includes details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the product to its supplier.\"},{\"column\":\"environmental_impact_score\",\"column_name\":\"ENVIRONMENTAL_IMPACT_SCORE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Score representing the environmental impact of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains logistics information not relevant to the current workflow.\",\"columns\":[{\"column\":\"logistics_id\",\"column_name\":\"LOGISTICS_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for logistics entries.\"},{\"column\":\"transport_mode\",\"column_name\":\"TRANSPORT_MODE\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transportation used in logistics.\"}]}},\"instructions\":[\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\",\"Load the table 'product_data' which includes details about products and their associated suppliers.\",\"Filter the supplier data to include only those with a sustainability rating above 80.\",\"Join the filtered supplier data with the product data on the supplier ID.\",\"Calculate a new column 'sustainable_score' by multiplying the product's environmental impact score by the supplier's sustainability rating.\",\"Filter the joined data to include only products with a sustainable score above 5000.\",\"Aggregate the data by product category to find the average sustainable score for each category.\",\"Sort the aggregated data by the average sustainable score in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data containing sustainability ratings.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data with supplier associations.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_RATING > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with sustainability ratings above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"sustainable_score\",\"formula\":\"ENVIRONMENTAL_IMPACT_SCORE * SUSTAINABILITY_RATING\"},\"output\":\"data_with_sustainable_score\",\"comment\":\"Calculating sustainable score by multiplying environmental impact score by sustainability rating.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"data_with_sustainable_score\",\"query\":\"sustainable_score > 5000\"},\"output\":\"high_sustainable_score_data\",\"comment\":\"Filtering products with a sustainable score above 5000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"high_sustainable_score_data\",\"groupBy\":\"PRODUCT_CATEGORY\",\"aggregations\":[{\"column\":\"sustainable_score\",\"name\":\"average_sustainable_score\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by product category to find average sustainable score.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_sustainable_score\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by average sustainable score in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2023\",\"description\":\"Contains sales transactions for the year 2023, including details such as transaction ID, customer ID, product ID, quantity, and total revenue.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_4\",\"column_name\":\"quantity\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Number of units sold in the transaction.\"},{\"column\":\"col_5\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"},{\"column\":\"col_6\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the transaction took place.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customers_2023.csv\",\"sheet_name\":null,\"label\":\"Customer Data 2023\",\"description\":\"Contains customer information for the year 2023, including customer ID, name, and contact details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_number\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact number of the customer.\"}]},\"table_id3\":{\"name\":\"product_data\",\"location\":\"C:/data/products_2023.csv\",\"sheet_name\":null,\"label\":\"Product Data 2023\",\"description\":\"Contains product information for the year 2023, including product ID, name, and price.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"price\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Price of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Filter the sales data to include only transactions with a total revenue greater than 1000.\",\"Aggregate the filtered sales data by region to calculate the total sales revenue for each region.\",\"Sort the aggregated data by total sales revenue in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data for 2023 from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`total_revenue` > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_sales\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating filtered sales data by region to calculate total sales revenue.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_regional_sales\",\"comment\":\"Sorting the aggregated data by total sales revenue in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow does 'forecastData' ensure that timestamps are correctly ordered?","output":"If the timestamps in the dataset are not in ascending order, the system automatically sorts them before performing the forecast."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_2024\",\"location\":\"C:/data/sales_2024.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2024\",\"description\":\"Contains sales transactions for the year 2024, including revenue and cost details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"total_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total cost incurred for the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the transaction took place.\"}]},\"table_id2\":{\"name\":\"customers\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"CustomerInfo\",\"label\":\"Customer Information\",\"description\":\"Contains customer details including customer ID and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_id3\":{\"name\":\"inventory\",\"location\":\"C:/data/inventory_2024.csv\",\"sheet_name\":null,\"label\":\"Inventory Data 2024\",\"description\":\"Contains inventory details for the year 2024, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2024.csv.\",\"Filter the sales data to include only transactions with a total revenue greater than 1000.\",\"Join the filtered sales data with the customer information from the Excel file located at C:/data/customers.xlsx.\",\"Aggregate the joined data to calculate the total revenue per region.\",\"Sort the aggregated data by total revenue in descending order.\",\"Calculate a new column in the sorted data to determine the profit margin by subtracting total cost from total revenue.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2024.csv\",\"type\":\"csv\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data for 2024 from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`total_revenue` > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customer_data\",\"comment\":\"Loading customer information from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customer_data\"],\"joinOn\":[\"region\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer information based on region.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_customers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating sales data by region to compute total revenue.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_regional_sales\",\"comment\":\"Sorting the aggregated data by total revenue in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_regional_sales\",\"columnName\":\"profit_margin\",\"formula\":\"`total_revenue` - `total_cost`\"},\"output\":\"sales_with_profit_margin\",\"comment\":\"Calculating profit margin by subtracting total cost from total revenue.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'calculateDerivedColumn' reference multiple existing columns in a formula?","output":"Yes, the formula can reference multiple columns, allowing complex calculations using multiple data points."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'filterData' be used without specifying the 'caseInsensitiveColumns' parameter?","output":"Yes, 'caseInsensitiveColumns' is optional. If not provided, filtering is case-sensitive by default."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if an invalid operator is used in the formula of 'calculateDerivedColumn'?","output":"An error is raised if an unsupported operator is used, as only '+', '-', '*', '/', '**', '%', and parentheses are allowed."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if transformations in 'applyTransformations' are applied in the wrong order?","output":"Transformations are applied sequentially in the order they appear. If the order is incorrect, the final output may not be as expected."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhy is specifying the 'confidenceInterval' parameter important in 'forecastData'?","output":"The 'confidenceInterval' parameter determines the range within which the forecasted values are expected to fall with a specified level of certainty."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if 'sortBy' is an empty list in 'sortData'?","output":"If 'sortBy' is an empty list, sorting is skipped as no columns are specified for sorting."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if an invalid aggregation function is specified in 'aggregateData'?","output":"If an invalid aggregation function is provided, an error is raised indicating that the function is not supported."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"This table contains information about suppliers, including their sustainability ratings and regional data.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"sustainability_rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability rating of the supplier on a scale from 0 to 100.\"},{\"column\":\"column_3_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the supplier is located.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/supply_chain/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"This table contains information about products and their suppliers.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]}},\"instructions\":[\"Load the 'supplier_data' table which contains information about suppliers and their sustainability ratings.\",\"Filter the 'supplier_data' to include only suppliers with a sustainability rating above 80.\",\"Aggregate the filtered data to calculate the average sustainability rating by region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading the supplier data table from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"sustainability_rating > 80\"},\"output\":\"filtered_suppliers\",\"comment\":\"Filtering suppliers with a sustainability rating above 80.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_suppliers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"sustainability_rating\",\"name\":\"average_sustainability_rating\",\"function\":\"mean\"}]},\"output\":\"average_sustainability_by_region\",\"comment\":\"Calculating the average sustainability rating by region.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supply_chain_data\",\"location\":\"C:/data/supply_chain_data.csv\",\"sheet_name\":null,\"label\":\"Supply Chain Data\",\"description\":\"Contains information about suppliers and their sustainability scores.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"sustainability_score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Score representing the sustainability level of the supplier.\"},{\"column\":\"col_3\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Contains details about products and their associated suppliers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the product to its supplier.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains logistics information for products, not relevant to this workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"logistics_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"col_2\",\"column_name\":\"transport_mode\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transportation used.\"}]}},\"instructions\":[\"Load the 'supply_chain_data' table containing information about suppliers and their sustainability scores.\",\"Filter the data to include only suppliers with a sustainability score greater than 80.\",\"Join the filtered supplier data with the 'product_data' table on the supplier_id to get product details.\",\"Aggregate the joined data to calculate the total number of sustainable products per category.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain_data.csv\",\"type\":\"csv\"},\"output\":\"supply_chain_data\",\"comment\":\"Loading the supply chain dataset containing supplier sustainability scores.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supply_chain_data\",\"query\":\"`sustainability_score` > 80\"},\"output\":\"sustainable_suppliers\",\"comment\":\"Filtering suppliers with a sustainability score greater than 80.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading the product dataset to join with supplier data.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"sustainable_suppliers\",\"product_data\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"sustainable_products\",\"comment\":\"Joining filtered supplier data with product data based on supplier_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sustainable_products\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"product_id\",\"name\":\"total_sustainable_products\",\"function\":\"count\"}]},\"output\":\"sustainable_products_per_category\",\"comment\":\"Aggregating data to calculate the total number of sustainable products per category.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan arithmetic operations be performed between a column and a constant value?","output":"Yes, arithmetic operations can be performed between a column and a constant, such as 'col1 + 5' or 'col2 * 2.5'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid arithmetic expression using exponentiation and modulus.","output":"'col1 ** 3 % 4'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if an unsupported forecast frequency is provided in 'forecastData'?","output":"If an unsupported frequency is provided, an error is raised indicating that the specified frequency is not recognized."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supply_chain_data\",\"location\":\"C:/data/supply_chain.csv\",\"sheet_name\":null,\"label\":\"Supply Chain Data\",\"description\":\"Contains data on supply chain activities including carbon footprint and supplier details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"carbon_footprint\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The carbon footprint associated with the supply chain activity.\"},{\"column\":\"column_3_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Geographical region of the supply chain activity.\"}]},\"table_id2\":{\"name\":\"supplier_info\",\"location\":\"C:/data/supplier_info.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains detailed information about suppliers including their IDs and names.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains data related to logistics operations, not directly relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"logistics_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics operation.\"},{\"column\":\"column_2_id\",\"column_name\":\"transport_mode\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transport used in the logistics operation.\"}]}},\"instructions\":[\"Load the 'supply_chain_data' table from the CSV file located at 'C:/data/supply_chain.csv'.\",\"Filter the 'supply_chain_data' table to include only records where the carbon footprint is greater than 1000.\",\"Join the filtered supply chain data with the 'supplier_info' table on the 'supplier_id' column.\",\"Aggregate the joined data by 'region' to calculate the total carbon footprint.\",\"Sort the aggregated data by 'total_carbon_footprint' in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain.csv\",\"type\":\"csv\"},\"output\":\"supply_chain_data\",\"comment\":\"Loading the supply chain data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supply_chain_data\",\"query\":\"carbon_footprint > 1000\"},\"output\":\"filtered_supply_chain_data\",\"comment\":\"Filtering supply chain data to include only records with a carbon footprint greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_info.csv\",\"type\":\"csv\"},\"output\":\"supplier_info\",\"comment\":\"Loading the supplier information from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supply_chain_data\",\"supplier_info\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supply chain data with supplier information on supplier_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"carbon_footprint\",\"name\":\"total_carbon_footprint\",\"function\":\"sum\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating the joined data by region to calculate the total carbon footprint.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_carbon_footprint\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting the aggregated data by total carbon footprint in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat should be done if the file type is unknown before using 'identifyTable'?","output":"The file type should be verified before using 'identifyTable' to ensure it is either 'csv' or 'xlsx', as unsupported formats will raise an error."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if an arithmetic expression contains a division by zero?","output":"If division by zero occurs, the result is treated as 'null' to prevent computation errors."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table1\":{\"name\":\"sales_2023\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2023\",\"description\":\"This table contains sales transactions for the year 2023, including revenue and cost details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_3\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"total_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total cost incurred for the transaction.\"},{\"column\":\"col_5\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the transaction.\"}]},\"table2\":{\"name\":\"customers\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Data\",\"description\":\"This table contains customer information including customer IDs and regions.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table3\":{\"name\":\"inventory\",\"location\":\"C:/data/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"This table contains inventory details, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Filter the sales data to include only transactions where the total revenue is greater than 1000.\",\"Load the customer data from the Excel file located at C:/data/customers.xlsx.\",\"Join the filtered sales data with the customer data on the customer_id column using an inner join.\",\"Aggregate the joined data by region to calculate the total sales revenue for each region.\",\"Sort the aggregated data by total sales in descending order.\",\"Calculate a new column in the sorted data to determine the profit margin by subtracting total cost from total revenue.\",\"Apply a log transformation to the profit margin column to stabilize variance.\",\"Forecast the total sales revenue for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_2023\",\"comment\":\"Loading the sales data for 2023 from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_2023\",\"query\":\"total_revenue > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customers\",\"comment\":\"Loading customer data from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customers\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer data based on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_customers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating sales data by region to compute total revenue.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_regional_sales\",\"comment\":\"Sorting the aggregated data by total sales in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_regional_sales\",\"columnName\":\"profit_margin\",\"formula\":\"total_revenue - total_cost\"},\"output\":\"sales_with_profit\",\"comment\":\"Calculating profit margin by subtracting total cost from total revenue.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sales_with_profit\",\"transformations\":[{\"columns\":[\"profit_margin\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_sales\",\"comment\":\"Applying a log transformation to the profit margin column.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"transformed_sales\",\"columns\":[\"total_sales\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7}},\"output\":\"forecasted_sales\",\"comment\":\"Forecasting the total sales revenue for the next 30 days using the Holt-Winters method.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/forecasted_sales.csv\",\"type\":\"csv\"},\"output\":\"forecasted_sales\",\"comment\":\"Saving the final forecasted data to a new CSV file.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'calculateDerivedColumn' use conditional logic within formulas?","output":"No, conditional expressions such as 'if-else' are not supported in formulas."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if a transformation in 'applyTransformations' does not specify required parameters?","output":"If a required parameter is missing from a transformation, an error is raised indicating the missing parameter."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers, including their location and units supplied.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"country\",\"column_name\":\"COUNTRY\",\"column_type\":\"xsd:string\",\"column_description\":\"Country where the supplier is located.\"},{\"column\":\"units_supplied\",\"column_name\":\"UNITS_SUPPLIED\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Number of units supplied by the supplier.\"}]},\"table_id2\":{\"name\":\"emissions_data\",\"location\":\"C:/data/emissions_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Emissions Data\",\"description\":\"Contains emissions data for each supplier, including total emissions.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"total_emissions\",\"column_name\":\"TOTAL_EMISSIONS\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total emissions produced by the supplier.\"},{\"column\":\"industry\",\"column_name\":\"INDUSTRY\",\"column_type\":\"xsd:string\",\"column_description\":\"Industry category of the supplier.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains logistics information, not relevant for emissions analysis.\",\"columns\":[{\"column\":\"logistics_id\",\"column_name\":\"LOGISTICS_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for logistics records.\"},{\"column\":\"transport_mode\",\"column_name\":\"TRANSPORT_MODE\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transport used for logistics.\"}]}},\"instructions\":[\"Load the 'supplier_data' table from the CSV file located at 'C:/data/supplier_data.csv'.\",\"Load the 'emissions_data' table from the Excel file located at 'C:/data/emissions_data.xlsx'.\",\"Filter the 'supplier_data' table to include only suppliers from the 'USA'.\",\"Join the filtered 'supplier_data' with 'emissions_data' on the 'supplier_id' column using an inner join.\",\"Calculate a new column 'emissions_per_unit' by dividing 'total_emissions' by 'units_supplied'.\",\"Aggregate the joined table by 'industry' to calculate the total 'emissions_per_unit' for each industry.\",\"Sort the aggregated data by 'total_emissions_per_unit' in descending order.\",\"Identify the top 5 industries with the highest emissions per unit.\",\"Apply a log transformation to the 'total_emissions_per_unit' column for normalization.\",\"Forecast future emissions for the top industry using the Holt-Winters method with a seasonal period of 12.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/emissions_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"emissions_data\",\"comment\":\"Loading emissions data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"COUNTRY == 'USA'\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering supplier data to include only those from the USA.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"emissions_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with emissions data on supplier_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"emissions_per_unit\",\"formula\":\"TOTAL_EMISSIONS / UNITS_SUPPLIED\"},\"output\":\"joined_data_with_emissions\",\"comment\":\"Calculating emissions per unit by dividing total emissions by units supplied.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_emissions\",\"groupBy\":\"INDUSTRY\",\"aggregations\":[{\"column\":\"emissions_per_unit\",\"name\":\"total_emissions_per_unit\",\"function\":\"sum\"}]},\"output\":\"industry_emissions\",\"comment\":\"Aggregating data by industry to calculate total emissions per unit.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"industry_emissions\",\"sortBy\":\"total_emissions_per_unit\",\"order\":\"desc\"},\"output\":\"sorted_industry_emissions\",\"comment\":\"Sorting industries by total emissions per unit in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_industry_emissions\",\"query\":\"index < 5\"},\"output\":\"top_industries\",\"comment\":\"Identifying the top 5 industries with the highest emissions per unit.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"top_industries\",\"transformations\":[{\"columns\":[\"total_emissions_per_unit\"],\"operation\":\"log_transform\"}]},\"output\":\"normalized_top_industries\",\"comment\":\"Applying log transformation to normalize emissions per unit.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"normalized_top_industries\",\"columns\":[\"total_emissions_per_unit\"],\"forecastParameters\":{\"forecastPeriod\":12,\"frequency\":\"MS\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":12,\"dateColumn\":\"date\"}},\"output\":\"emissions_forecast\",\"comment\":\"Forecasting future emissions for the top industry using Holt-Winters method.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supply_chain_data\",\"location\":\"C:/data/supply_chain.csv\",\"sheet_name\":null,\"label\":\"Supply Chain Data\",\"description\":\"Contains information about the supply chain including carbon emissions and supplier details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"carbon_emission\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The carbon emission value for each supply chain record.\"},{\"column\":\"column_3_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region associated with the supply chain record.\"}]},\"table_id2\":{\"name\":\"supplier_info\",\"location\":\"C:/data/supplier_info.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Supplier Information\",\"description\":\"Contains detailed information about suppliers including their IDs and names.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data that is not relevant to the supply chain sustainability workflow.\",\"columns\":[{\"column\":\"col_A\",\"column_name\":\"data_point\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant data point.\"}]}},\"instructions\":[\"Load the 'supply_chain_data' table from the CSV file located at 'C:/data/supply_chain.csv'.\",\"Filter the 'supply_chain_data' table to include only records where the 'carbon_emission' is less than 1000.\",\"Join the filtered 'supply_chain_data' with the 'supplier_info' table on the 'supplier_id' column using an inner join.\",\"Aggregate the joined table by 'region' to calculate the total 'carbon_emission' for each region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain.csv\",\"type\":\"csv\"},\"output\":\"supply_chain_data\",\"comment\":\"Loading the supply chain data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supply_chain_data\",\"query\":\"carbon_emission < 1000\"},\"output\":\"filtered_supply_chain_data\",\"comment\":\"Filtering the supply chain data to include only records with carbon emissions less than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_info.xlsx\",\"type\":\"xlsx\"},\"output\":\"supplier_info\",\"comment\":\"Loading the supplier information from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supply_chain_data\",\"supplier_info\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining the filtered supply chain data with supplier information on supplier_id using an inner join.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"carbon_emission\",\"name\":\"total_carbon_emission\",\"function\":\"sum\"}]},\"output\":\"regional_carbon_emission\",\"comment\":\"Aggregating the joined data by region to calculate the total carbon emission for each region.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'aggregateData' perform multiple aggregation functions in a single action?","output":"Yes, 'aggregateData' allows multiple aggregation functions to be applied simultaneously by specifying multiple aggregation instructions in the 'aggregations' list."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supply_chain_data\",\"location\":\"C:/data/supply_chain_data.csv\",\"sheet_name\":null,\"label\":\"Supply Chain Data\",\"description\":\"This table contains information about suppliers, including their sustainability ratings and regions.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"sustainability_rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The sustainability rating of the supplier, ranging from 0 to 100.\"},{\"column\":\"column_3_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region where the supplier is located.\"}]},\"table_id2\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"This table contains logistics information, including transportation modes and costs.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"transport_mode\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transportation used for logistics.\"},{\"column\":\"column_2_id\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Cost associated with the transportation.\"}]}},\"instructions\":[\"Load the 'supply_chain_data' table which contains information about suppliers and their sustainability ratings.\",\"Filter the data to include only suppliers with a sustainability rating above 80.\",\"Aggregate the filtered data to calculate the average sustainability rating by region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain_data.csv\",\"type\":\"csv\"},\"output\":\"supply_chain_data\",\"comment\":\"Loading the supply chain data which includes supplier sustainability ratings.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supply_chain_data\",\"query\":\"sustainability_rating > 80\"},\"output\":\"filtered_suppliers\",\"comment\":\"Filtering suppliers to include only those with a sustainability rating above 80.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_suppliers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"sustainability_rating\",\"name\":\"average_sustainability\",\"function\":\"mean\"}]},\"output\":\"average_sustainability_by_region\",\"comment\":\"Calculating the average sustainability rating by region.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan multiple actions in a Tamarind workflow share the same output table name?","output":"Yes, multiple actions can share the same output table name, but the latest action will overwrite the existing data unless explicitly prevented."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan expressions be used to filter rows in a dataset?","output":"Yes, expressions can be used in filtering actions, such as selecting rows where 'revenue > 1000'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_2023\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2023\",\"description\":\"Contains sales transactions for the year 2023, including revenue and cost details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"total_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total cost incurred for the transaction.\"}]},\"table_id2\":{\"name\":\"customers\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Information\",\"description\":\"Contains customer details including customer_id and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_id3\":{\"name\":\"inventory\",\"location\":\"C:/data/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"Contains inventory details for various products, not relevant to the sales workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Load the customer information from the Excel file located at C:/data/customers.xlsx.\",\"Filter the sales data to include only transactions with a total revenue greater than 5000.\",\"Join the filtered sales data with the customer information based on the customer_id.\",\"Calculate a new column 'profit_margin' by subtracting total cost from total revenue in the joined table.\",\"Aggregate the data by region to compute the total sales and average profit margin.\",\"Sort the aggregated data by total sales in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_2023\",\"comment\":\"Loading the sales data for 2023 from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customers\",\"comment\":\"Loading customer information from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_2023\",\"query\":\"`total_revenue` > 5000\"},\"output\":\"high_value_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 5000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"high_value_sales\",\"customers\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining high value sales with customer information based on customer_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sales_with_customers\",\"columnName\":\"profit_margin\",\"formula\":\"`total_revenue` - `total_cost`\"},\"output\":\"sales_with_profit\",\"comment\":\"Calculating profit margin by subtracting total cost from total revenue.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_profit\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"},{\"column\":\"profit_margin\",\"name\":\"average_profit_margin\",\"function\":\"mean\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating data by region to compute total sales and average profit margin.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_regional_sales\",\"comment\":\"Sorting the aggregated data by total sales in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supply_chain_data\",\"location\":\"C:/data/supply_chain_data.csv\",\"sheet_name\":null,\"label\":\"Supply Chain Data\",\"description\":\"This table contains information on various suppliers, including their sustainability scores and regions.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"sustainability_score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"A score representing the sustainability practices of the supplier.\"},{\"column\":\"column_3_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"The region where the supplier is located.\"}]},\"table_id2\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"This table contains logistics information, not directly related to sustainability scores.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"logistics_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"column_2_id\",\"column_name\":\"transport_mode\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transportation used.\"},{\"column\":\"column_3_id\",\"column_name\":\"carbon_emission\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Carbon emissions associated with the logistics entry.\"}]}},\"instructions\":[\"Load the 'supply_chain_data' table which contains information on suppliers and their sustainability scores.\",\"Filter the data to include only suppliers with a sustainability score greater than 80.\",\"Aggregate the filtered data to calculate the average sustainability score by region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain_data.csv\",\"type\":\"csv\"},\"output\":\"supply_chain_data\",\"comment\":\"Loading the supply chain data containing supplier information and sustainability scores.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supply_chain_data\",\"query\":\"sustainability_score > 80\"},\"output\":\"filtered_suppliers\",\"comment\":\"Filtering suppliers to include only those with a sustainability score greater than 80.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_suppliers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"sustainability_score\",\"name\":\"average_sustainability_score\",\"function\":\"mean\"}]},\"output\":\"average_sustainability_by_region\",\"comment\":\"Calculating the average sustainability score by region for the filtered suppliers.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains information about suppliers including their sustainability ratings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"sustainability_rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability rating of the supplier, ranging from 0 to 100.\"},{\"column\":\"col_3\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Information\",\"description\":\"Contains details about products and their associated suppliers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier associated with the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Information\",\"description\":\"Contains logistics data not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"logistics_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"col_2\",\"column_name\":\"transport_mode\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transport used for logistics.\"}]}},\"instructions\":[\"Load the 'supplier_data' table containing information about suppliers and their sustainability ratings.\",\"Load the 'product_data' table which includes details about products and their associated suppliers.\",\"Filter the 'supplier_data' table to include only suppliers with a sustainability rating above 80.\",\"Join the filtered 'supplier_data' with 'product_data' on the supplier_id column to associate products with sustainable suppliers.\",\"Aggregate the joined table to calculate the total number of sustainable products per category.\",\"Sort the aggregated data by the number of sustainable products in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data containing sustainability ratings.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data with associated suppliers.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"sustainability_rating > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with sustainability ratings above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"product_data\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"sustainable_products\",\"comment\":\"Joining filtered suppliers with products to find sustainable products.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sustainable_products\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"product_id\",\"name\":\"total_sustainable_products\",\"function\":\"count\"}]},\"output\":\"aggregated_sustainable_products\",\"comment\":\"Aggregating data to count sustainable products per category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_sustainable_products\",\"sortBy\":\"total_sustainable_products\",\"order\":\"desc\"},\"output\":\"sorted_sustainable_products\",\"comment\":\"Sorting categories by the number of sustainable products in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if a Boolean expression is incorrectly formatted?","output":"An error is raised if a Boolean expression is incorrectly formatted, such as 'col1 and > 5', where the logical operator is misplaced."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'applyTransformations' be used without specifying any transformations?","output":"No, at least one transformation must be provided in the 'transformations' list for 'applyTransformations' to execute."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains supplier IDs, names, and sustainability scores.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"},{\"column\":\"col_3\",\"column_name\":\"sustainability_score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability score of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Details\",\"description\":\"Includes product IDs, names, and associated supplier IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"col_3\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"ID of the supplier providing the product.\"}]},\"table_id3\":{\"name\":\"transport_data\",\"location\":\"C:/data/transport_data.csv\",\"sheet_name\":null,\"label\":\"Transportation Emissions\",\"description\":\"Contains transportation modes and their carbon emissions per product.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transport_mode\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transportation.\"},{\"column\":\"col_2\",\"column_name\":\"carbon_emissions\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Carbon emissions per product for the transportation mode.\"}]},\"table_id4\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the supply chain sustainability analysis.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An irrelevant column.\"}]}},\"instructions\":[\"Load the table 'supplier_data' which contains information about suppliers and their sustainability scores.\",\"Load the table 'product_data' which includes product details and associated supplier IDs.\",\"Filter the 'supplier_data' to include only suppliers with a sustainability score above 80.\",\"Join the filtered supplier data with the 'product_data' on the supplier ID.\",\"Aggregate the joined data to calculate the total number of products supplied by each sustainable supplier.\",\"Load the table 'transport_data' which contains transportation modes and their carbon emissions.\",\"Join the aggregated supplier-product data with 'transport_data' on the transportation mode.\",\"Calculate a new column 'total_emissions' by multiplying the number of products by carbon emissions per product.\",\"Sort the data by 'total_emissions' in ascending order to identify the most sustainable supply chain routes.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"sustainability_score > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with a sustainability score above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"product_data\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_supplier_product_data\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_supplier_product_data\",\"groupBy\":\"supplier_id\",\"aggregations\":[{\"column\":\"product_id\",\"name\":\"total_products\",\"function\":\"count\"}]},\"output\":\"aggregated_supplier_product_data\",\"comment\":\"Aggregating data to count total products supplied by each sustainable supplier.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/transport_data.csv\",\"type\":\"csv\"},\"output\":\"transport_data\",\"comment\":\"Loading transportation data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"aggregated_supplier_product_data\",\"transport_data\"],\"joinOn\":[\"transport_mode\"],\"joinType\":\"inner\"},\"output\":\"joined_transport_data\",\"comment\":\"Joining aggregated supplier-product data with transport data on transport mode.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_transport_data\",\"columnName\":\"total_emissions\",\"formula\":\"total_products * carbon_emissions\"},\"output\":\"emissions_data\",\"comment\":\"Calculating total emissions by multiplying total products by carbon emissions per product.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"emissions_data\",\"sortBy\":\"total_emissions\",\"order\":\"asc\"},\"output\":\"sorted_emissions_data\",\"comment\":\"Sorting data by total emissions in ascending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid arithmetic expression combining multiple operations.","output":"'(col1 + col2) * col3 / col4 - col5'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if a column referenced in a Tamarind workflow does not exist?","output":"If a column referenced in an operation does not exist, an error is raised to prevent unintended execution."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers and their sustainability scores.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_score\",\"column_name\":\"SUSTAINABILITY_SCORE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability score of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"},{\"column\":\"category\",\"column_name\":\"CATEGORY\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product.\"}]},\"table_id3\":{\"name\":\"transport_data\",\"location\":\"C:/data/transport_data.csv\",\"sheet_name\":null,\"label\":\"Transport Data\",\"description\":\"Information about transportation modes and emissions.\",\"columns\":[{\"column\":\"transport_mode\",\"column_name\":\"TRANSPORT_MODE\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transportation.\"},{\"column\":\"emissions\",\"column_name\":\"EMISSIONS\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Emissions in CO2 per unit for the transportation mode.\"}]},\"table_id4\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains unrelated data not used in this workflow.\",\"columns\":[{\"column\":\"irrelevant_column\",\"column_name\":\"IRRELEVANT_COLUMN\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant column not used in this workflow.\"}]}},\"instructions\":[\"Load the 'supplier_data' table which contains information about suppliers and their sustainability scores.\",\"Load the 'product_data' table which contains details about products and their associated suppliers.\",\"Filter the 'supplier_data' to include only suppliers with a sustainability score above 80.\",\"Join the filtered 'supplier_data' with 'product_data' on the supplier ID to get products from sustainable suppliers.\",\"Aggregate the joined data to calculate the total number of sustainable products per category.\",\"Load the 'transport_data' table which contains information about transportation modes and emissions.\",\"Filter the 'transport_data' to include only transportation modes with emissions below 50 CO2 per unit.\",\"Join the sustainable products data with filtered 'transport_data' on transportation mode to get sustainable logistics options.\",\"Calculate a new column 'total_sustainability_score' by summing the sustainability score and inverse of emissions.\",\"Sort the final data by 'total_sustainability_score' in descending order to prioritize the most sustainable options.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data containing sustainability scores.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data with associated suppliers.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_SCORE > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with sustainability scores above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"sustainable_products\",\"comment\":\"Joining filtered suppliers with products to get sustainable products.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sustainable_products\",\"groupBy\":\"CATEGORY\",\"aggregations\":[{\"column\":\"PRODUCT_ID\",\"name\":\"total_sustainable_products\",\"function\":\"count\"}]},\"output\":\"sustainable_products_per_category\",\"comment\":\"Aggregating to find total sustainable products per category.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/transport_data.csv\",\"type\":\"csv\"},\"output\":\"transport_data\",\"comment\":\"Loading transport data with emissions information.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"transport_data\",\"query\":\"EMISSIONS < 50\"},\"output\":\"filtered_transport_data\",\"comment\":\"Filtering transportation modes with emissions below 50 CO2 per unit.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"sustainable_products_per_category\",\"filtered_transport_data\"],\"joinOn\":[\"TRANSPORT_MODE\"],\"joinType\":\"inner\"},\"output\":\"sustainable_logistics\",\"comment\":\"Joining sustainable products with transport data for logistics options.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sustainable_logistics\",\"columnName\":\"total_sustainability_score\",\"formula\":\"SUSTAINABILITY_SCORE + (1 / EMISSIONS)\"},\"output\":\"sustainability_score_calculated\",\"comment\":\"Calculating total sustainability score by combining scores and inverse emissions.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"sustainability_score_calculated\",\"sortBy\":\"total_sustainability_score\",\"order\":\"desc\"},\"output\":\"sorted_sustainable_options\",\"comment\":\"Sorting data by total sustainability score to prioritize options.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_data.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"This table contains sales transactions including customer IDs, product IDs, and total revenue.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"}]},\"table_id2\":{\"name\":\"customer_info\",\"location\":\"C:/data/customer_info.csv\",\"sheet_name\":null,\"label\":\"Customer Information\",\"description\":\"This table contains information about customers including their ID and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_id3\":{\"name\":\"product_catalog\",\"location\":\"C:/data/product_catalog.csv\",\"sheet_name\":null,\"label\":\"Product Catalog\",\"description\":\"This table contains information about products including product IDs and descriptions.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"product_description\",\"column_type\":\"xsd:string\",\"column_description\":\"Description of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located in the data directory.\",\"Filter the sales data to include only transactions with a total revenue greater than 5000.\",\"Join the filtered sales data with the customer information table on the customer ID.\",\"Aggregate the joined data to calculate the total sales revenue per region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_data.csv\",\"type\":\"csv\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`total_revenue` > 5000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 5000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customer_info.csv\",\"type\":\"csv\"},\"output\":\"customer_info\",\"comment\":\"Loading the customer information from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customer_info\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer information based on customer ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_customers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating sales data by region to compute total revenue.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers, including their sustainability ratings.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_rating\",\"column_name\":\"SUSTAINABILITY_RATING\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability rating of the supplier, ranging from 0 to 100.\"},{\"column\":\"supplier_name\",\"column_name\":\"SUPPLIER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Contains details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"},{\"column\":\"product_name\",\"column_name\":\"PRODUCT_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains information about logistics and transportation details.\",\"columns\":[{\"column\":\"logistics_id\",\"column_name\":\"LOGISTICS_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"transport_mode\",\"column_name\":\"TRANSPORT_MODE\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transportation used for logistics.\"},{\"column\":\"carbon_emission\",\"column_name\":\"CARBON_EMISSION\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Carbon emission associated with the logistics entry.\"}]}},\"instructions\":[\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\",\"Load the table 'product_data' which contains details about products and their associated suppliers.\",\"Filter the supplier data to include only those with a sustainability rating above 80.\",\"Join the filtered supplier data with the product data on the supplier ID.\",\"Aggregate the joined data to calculate the total number of products supplied by each sustainable supplier.\",\"Sort the aggregated data by the total number of products in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading the supplier data containing sustainability ratings.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading the product data with supplier associations.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_RATING > 80\"},\"output\":\"sustainable_suppliers\",\"comment\":\"Filtering suppliers with a sustainability rating above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"sustainable_suppliers\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"sustainable_supplier_products\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sustainable_supplier_products\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"PRODUCT_ID\",\"name\":\"total_products\",\"function\":\"count\"}]},\"output\":\"aggregated_supplier_products\",\"comment\":\"Aggregating data to calculate total products supplied by each sustainable supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_supplier_products\",\"sortBy\":\"total_products\",\"order\":\"desc\"},\"output\":\"sorted_supplier_products\",\"comment\":\"Sorting the aggregated data by total number of products in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers, including their IDs and sustainability ratings.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_rating\",\"column_name\":\"SUSTAINABILITY_RATING\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Rating of the supplier's sustainability practices.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/supply_chain/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Includes details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/supply_chain/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains logistics information, not relevant to the current workflow.\",\"columns\":[{\"column\":\"logistics_id\",\"column_name\":\"LOGISTICS_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"transport_mode\",\"column_name\":\"TRANSPORT_MODE\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transport used for logistics.\"}]}},\"instructions\":[\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\",\"Load the table 'product_data' which includes details about products and their associated suppliers.\",\"Filter the 'supplier_data' to include only suppliers with a sustainability rating above 80.\",\"Join the filtered supplier data with the 'product_data' on the supplier ID.\",\"Aggregate the joined data to calculate the total number of products supplied by each sustainable supplier.\",\"Sort the aggregated data by the total number of products in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading the supplier data containing sustainability ratings.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading the product data with supplier associations.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_RATING > 80\"},\"output\":\"sustainable_suppliers\",\"comment\":\"Filtering suppliers to include only those with a sustainability rating above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"sustainable_suppliers\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"PRODUCT_ID\",\"name\":\"total_products\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total number of products supplied by each sustainable supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_products\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total number of products in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid Boolean expression using 'or' and negation.","output":"'not (col1 < 5) or (col2 == 10)'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of an invalid Boolean expression due to incorrect syntax.","output":"'col1 and > 5' // Error: Incorrect syntax, 'and' must be between two valid expressions."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains details about suppliers, including their sustainability scores.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"supplier_name\",\"column_name\":\"SUPPLIER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"},{\"column\":\"sustainability_score\",\"column_name\":\"SUSTAINABILITY_SCORE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability score of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Information\",\"description\":\"Contains details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"product_name\",\"column_name\":\"PRODUCT_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Information\",\"description\":\"Contains logistics details which are not relevant to the current workflow.\",\"columns\":[{\"column\":\"logistics_id\",\"column_name\":\"LOGISTICS_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"transport_mode\",\"column_name\":\"TRANSPORT_MODE\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transport used for logistics.\"}]}},\"instructions\":[\"Load the table 'supplier_data' which contains information about suppliers and their sustainability scores.\",\"Load the table 'product_data' which contains details about products and their associated suppliers.\",\"Filter the supplier data to include only those with a sustainability score above 80.\",\"Join the filtered supplier data with the product data on the supplier ID.\",\"Aggregate the joined data to calculate the total number of products per supplier.\",\"Sort the aggregated data by the total number of products in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading the supplier data which includes sustainability scores.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading the product data which includes supplier associations.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_SCORE > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with sustainability scores above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"PRODUCT_ID\",\"name\":\"total_products\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate the total number of products per supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_products\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting the aggregated data by total number of products in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_2023\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2023\",\"description\":\"This table contains sales transactions for the year 2023, including details such as transaction date, total revenue, and customer information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"},{\"column\":\"col_3\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total revenue generated from the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for the customer.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"CustomerInfo\",\"label\":\"Customer Information\",\"description\":\"This table contains customer details such as customer ID, name, and contact information.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for the customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_number\",\"column_type\":\"xsd:string\",\"column_description\":\"The contact number of the customer.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/inventory_2023.csv\",\"sheet_name\":null,\"label\":\"Inventory Data 2023\",\"description\":\"This table contains inventory details for the year 2023, including product IDs and stock levels.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:integer\",\"column_description\":\"The current stock level of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Filter the sales data to include only transactions where the total revenue is greater than 1000.\",\"Sort the filtered sales data by the transaction date in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_2023\",\"comment\":\"Loading the sales data for 2023 from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_2023\",\"query\":\"`total_revenue` > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"filtered_sales\",\"sortBy\":\"transaction_date\",\"order\":\"desc\"},\"output\":\"sorted_sales\",\"comment\":\"Sorting the filtered sales data by transaction date in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if a transformation in 'applyTimeSeriesTransformations' references a column that does not exist?","output":"If a column referenced in a transformation does not exist, an error is raised to prevent incorrect computations."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow does 'applyTransformations' handle missing columns in a transformation?","output":"If a column specified in a transformation does not exist in the table, an error is raised to prevent incorrect computations."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers, including their sustainability ratings.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_rating\",\"column_name\":\"SUSTAINABILITY_RATING\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability rating of the supplier, ranging from 0 to 100.\"},{\"column\":\"supplier_name\",\"column_name\":\"SUPPLIER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/supply_chain/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Contains details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"product_name\",\"column_name\":\"PRODUCT_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]},\"table_id3\":{\"name\":\"transport_data\",\"location\":\"C:/data/supply_chain/transport_data.csv\",\"sheet_name\":null,\"label\":\"Transport Data\",\"description\":\"Details about transportation modes and emissions, not relevant to the current workflow.\",\"columns\":[{\"column\":\"transport_id\",\"column_name\":\"TRANSPORT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transport entry.\"},{\"column\":\"emissions\",\"column_name\":\"EMISSIONS\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Emissions associated with the transport mode.\"}]}},\"instructions\":[\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\",\"Load the table 'product_data' which contains details about products and their associated suppliers.\",\"Filter the 'supplier_data' table to include only suppliers with a sustainability rating above 80.\",\"Join the filtered supplier data with the product data on the supplier ID.\",\"Aggregate the joined data to calculate the total number of products supplied by each sustainable supplier.\",\"Sort the aggregated data by the total number of products in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data to analyze sustainability ratings.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data to associate products with suppliers.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_RATING > 80\"},\"output\":\"sustainable_suppliers\",\"comment\":\"Filtering suppliers to include only those with high sustainability ratings.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"sustainable_suppliers\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"sustainable_products\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sustainable_products\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"PRODUCT_ID\",\"name\":\"total_products\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total products supplied by each sustainable supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_products\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by total number of products in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'applyTimeSeriesTransformations' handle missing values in time series data?","output":"Yes, missing values can be handled using transformations such as interpolation, forward-filling, and rolling statistics."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'forecastData' be used for anomaly detection instead of time-series forecasting?","output":"Yes, 'forecastData' supports anomaly detection algorithms such as 'isolation_forest', 'local_outlier_factor', 'one_class_svm', 'gmm', and 'z-score'."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid arithmetic expression using division and negation.","output":"'-col1 / col2'"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if 'forecastData' is used on a dataset with no datetime column?","output":"If no datetime column exists and 'dateColumn' is not specified, an error is raised as forecasting requires a time-based index."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supply_chain_data\",\"location\":\"C:/data/supply_chain_data.csv\",\"sheet_name\":null,\"label\":\"Supply Chain Data\",\"description\":\"This table contains information about suppliers, including their sustainability scores and other relevant details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"sustainability_score\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The sustainability score of the supplier, ranging from 0 to 100.\"},{\"column\":\"column_3_id\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"This table contains information about products, including their categories and associated suppliers.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_2_id\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"The category of the product.\"},{\"column\":\"column_3_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The supplier ID associated with the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"This table contains logistics information, such as transportation modes and costs, which is not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"logistics_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"column_2_id\",\"column_name\":\"transport_mode\",\"column_type\":\"xsd:string\",\"column_description\":\"The mode of transportation used.\"},{\"column\":\"column_3_id\",\"column_name\":\"cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The cost associated with the logistics entry.\"}]}},\"instructions\":[\"Load the 'supply_chain_data' table which contains information about suppliers and their sustainability scores.\",\"Filter the data to include only suppliers with a sustainability score greater than 80.\",\"Join the filtered supplier data with the 'product_data' table on the supplier_id column.\",\"Aggregate the joined data to calculate the total number of sustainable products per category.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain_data.csv\",\"type\":\"csv\"},\"output\":\"supply_chain_data\",\"comment\":\"Loading the supply chain data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supply_chain_data\",\"query\":\"sustainability_score > 80\"},\"output\":\"filtered_suppliers\",\"comment\":\"Filtering suppliers with a sustainability score greater than 80.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading the product data from a CSV file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_suppliers\",\"product_data\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with product data on supplier_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"product_id\",\"name\":\"total_sustainable_products\",\"function\":\"count\"}]},\"output\":\"sustainable_products_per_category\",\"comment\":\"Aggregating data to calculate the total number of sustainable products per category.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nHow does 'applyTransformations' handle missing values in columns during transformations?","output":"Depending on the transformation used, missing values may be interpolated, filled using 'fillna', or result in errors if not handled explicitly."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains information about suppliers including their sustainability ratings.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_rating\",\"column_name\":\"SUSTAINABILITY_RATING\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Rating of the supplier's sustainability practices.\"},{\"column\":\"supplier_name\",\"column_name\":\"SUPPLIER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Information\",\"description\":\"Contains details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"},{\"column\":\"product_name\",\"column_name\":\"PRODUCT_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Information\",\"description\":\"Contains logistics data including transportation modes and emissions.\",\"columns\":[{\"column\":\"logistics_id\",\"column_name\":\"LOGISTICS_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"emissions\",\"column_name\":\"EMISSIONS\",\"column_type\":\"xsd:decimal\",\"column_description\":\"CO2 emissions associated with the logistics entry.\"},{\"column\":\"transport_mode\",\"column_name\":\"TRANSPORT_MODE\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transportation used.\"}]}},\"instructions\":[\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\",\"Load the table 'product_data' which contains details about products and their associated suppliers.\",\"Filter the supplier data to include only those with a sustainability rating above 80.\",\"Join the filtered supplier data with the product data on the supplier ID.\",\"Aggregate the joined data to calculate the total number of products supplied by each sustainable supplier.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data containing sustainability ratings.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data with associated suppliers.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_RATING > 80\"},\"output\":\"sustainable_suppliers\",\"comment\":\"Filtering suppliers to include only those with a sustainability rating above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"sustainable_suppliers\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"sustainable_supplier_products\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sustainable_supplier_products\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"PRODUCT_ID\",\"name\":\"total_products\",\"function\":\"count\"}]},\"output\":\"sustainable_supplier_aggregation\",\"comment\":\"Aggregating data to calculate the total number of products supplied by each sustainable supplier.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supply_chain_data\",\"location\":\"C:/data/supply_chain_data.csv\",\"sheet_name\":null,\"label\":\"Supply Chain Data\",\"description\":\"Contains information about suppliers, products, and transactions in the supply chain.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"column_3_id\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the transaction.\"}]},\"table_id2\":{\"name\":\"emissions_data\",\"location\":\"C:/data/emissions_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Emissions Data\",\"description\":\"Contains emissions data for each supplier.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"emissions\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total emissions reported by the supplier.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"unrelated_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An unrelated column.\"}]}},\"instructions\":[\"Load the 'supply_chain_data' table from the CSV file.\",\"Load the 'emissions_data' table from the Excel file.\",\"Filter the 'supply_chain_data' to include only records from 2023.\",\"Join 'supply_chain_data' with 'emissions_data' on the 'supplier_id' column.\",\"Calculate the total emissions for each supplier by summing the 'emissions' column.\",\"Sort the resulting table by 'total_emissions' in descending order.\",\"Filter the sorted table to include only the top 10 suppliers with the highest emissions.\",\"Calculate the average emissions per product for these top suppliers.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain_data.csv\",\"type\":\"csv\"},\"output\":\"supply_chain_data\",\"comment\":\"Loading the supply chain data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/emissions_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"emissions_data\",\"comment\":\"Loading the emissions data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supply_chain_data\",\"query\":\"`transaction_date`.dt.year == 2023\"},\"output\":\"filtered_supply_chain_data\",\"comment\":\"Filtering supply chain data to include only records from 2023.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supply_chain_data\",\"emissions_data\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining supply chain data with emissions data on supplier_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"supplier_id\",\"aggregations\":[{\"column\":\"emissions\",\"name\":\"total_emissions\",\"function\":\"sum\"}]},\"output\":\"supplier_emissions\",\"comment\":\"Calculating total emissions for each supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"supplier_emissions\",\"sortBy\":\"total_emissions\",\"order\":\"desc\"},\"output\":\"sorted_emissions\",\"comment\":\"Sorting suppliers by total emissions in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_emissions\",\"query\":\"index < 10\"},\"output\":\"top_suppliers\",\"comment\":\"Filtering to include only the top 10 suppliers with the highest emissions.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"top_suppliers\",\"groupBy\":[],\"aggregations\":[{\"column\":\"total_emissions\",\"name\":\"average_emissions_per_product\",\"function\":\"mean\"}]},\"output\":\"top_suppliers_with_avg_emissions\",\"comment\":\"Calculating average emissions per product for top suppliers.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat happens if an invalid transformation operation is provided in 'applyTransformations'?","output":"If an invalid transformation operation is specified, an error is raised indicating that the operation is not recognized."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_2023\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2023\",\"description\":\"Contains sales transactions for the year 2023, including total revenue and total cost.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"total_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total cost incurred for the transaction.\"},{\"column\":\"col_5\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the transaction took place.\"},{\"column\":\"col_6\",\"column_name\":\"date\",\"column_type\":\"xsd:date\",\"column_description\":\"Date of the transaction.\"}]},\"table_id2\":{\"name\":\"customers\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Information\",\"description\":\"Contains customer details including customer ID and name.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"}]},\"table_id3\":{\"name\":\"inventory_2023\",\"location\":\"C:/data/inventory_2023.csv\",\"sheet_name\":null,\"label\":\"Inventory Data 2023\",\"description\":\"Contains inventory details for the year 2023, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"item_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"col_2\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the item.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Load the customer information from the Excel file located at C:/data/customers.xlsx.\",\"Filter the sales data to include only transactions with a total revenue greater than 1000.\",\"Join the filtered sales data with the customer information using the customer_id as the key.\",\"Calculate a new column in the joined table to determine the profit by subtracting total cost from total revenue.\",\"Aggregate the data by region to calculate the total sales and average profit.\",\"Sort the aggregated data by total sales in descending order.\",\"Apply a log transformation to the average profit column to stabilize variance.\",\"Forecast the total sales for the next 30 days using the Holt-Winters method.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_2023\",\"comment\":\"Loading the sales data for 2023 from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customers\",\"comment\":\"Loading customer information from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_2023\",\"query\":\"total_revenue > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customers\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer information using customer_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sales_with_customers\",\"columnName\":\"profit\",\"formula\":\"total_revenue - total_cost\"},\"output\":\"sales_with_profit\",\"comment\":\"Calculating profit by subtracting total cost from total revenue.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_profit\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"},{\"column\":\"profit\",\"name\":\"average_profit\",\"function\":\"mean\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating data by region to calculate total sales and average profit.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_regional_sales\",\"comment\":\"Sorting the aggregated data by total sales in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_regional_sales\",\"transformations\":[{\"columns\":[\"average_profit\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_sales\",\"comment\":\"Applying a log transformation to the average profit column to stabilize variance.\"},{\"action\":\"forecastData\",\"parameters\":{\"table\":\"sales_2023\",\"columns\":[\"total_revenue\"],\"forecastParameters\":{\"forecastPeriod\":30,\"frequency\":\"D\",\"dateColumn\":\"date\",\"algorithm\":\"holt_winters\",\"seasonal_periods\":7,\"confidenceInterval\":95}},\"output\":\"sales_forecast\",\"comment\":\"Forecasting the total sales for the next 30 days using the Holt-Winters method.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhy must datasets exist before being referenced in Tamarind workflows?","output":"Datasets must exist before being referenced to prevent errors and ensure that all actions operate on valid inputs."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan Tamarind workflows handle missing values in datasets?","output":"Yes, Tamarind workflows support handling missing values through operations such as interpolation, forward-filling, backward-filling, and dropping null values."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"sales_2023\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2023\",\"description\":\"Contains sales transactions for the year 2023, including revenue and customer IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"}]},\"table_2\":{\"name\":\"customers\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Information\",\"description\":\"Contains customer details including customer ID and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_3\":{\"name\":\"inventory\",\"location\":\"C:/data/inventory_2023.csv\",\"sheet_name\":null,\"label\":\"Inventory Data 2023\",\"description\":\"Contains inventory details for the year 2023, not relevant to the current workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:integer\",\"column_description\":\"Current stock level of the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Filter the sales data to include only transactions with a total revenue greater than 1000.\",\"Join the filtered sales data with the customer information from the Excel file located at C:/data/customers.xlsx.\",\"Aggregate the joined data by region to calculate the total sales revenue.\",\"Sort the aggregated data by total sales revenue in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_2023\",\"comment\":\"Loading the sales data for 2023 from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_2023\",\"query\":\"`total_revenue` > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customers\",\"comment\":\"Loading customer information from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customers\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer information based on customer_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_customers\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating sales data by region to calculate total sales revenue.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_regional_sales\",\"comment\":\"Sorting the aggregated data by total sales revenue in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'sortData' be used without specifying the 'order' parameter?","output":"Yes, if the 'order' parameter is not provided, sorting defaults to ascending order ('asc')."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan 'applyTransformations' be used to apply transformations to multiple columns at once?","output":"Yes, a transformation can specify multiple columns within the 'columns' parameter to apply the operation to all of them simultaneously."}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nProvide an example of a valid chained comparison using multiple operators.","output":"'5 <= col1 < 20 and col2 > 10'"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2023\",\"description\":\"This table contains sales transactions for the year 2023, including details such as transaction date, total revenue, and customer information.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date of the transaction.\"},{\"column\":\"column_2_id\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"The total revenue generated from the transaction.\"},{\"column\":\"column_3_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for the customer.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customers_2023.csv\",\"sheet_name\":null,\"label\":\"Customer Data 2023\",\"description\":\"This table contains customer information for the year 2023, including customer ID, name, and contact details.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"The unique identifier for the customer.\"},{\"column\":\"column_2_id\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"The name of the customer.\"},{\"column\":\"column_3_id\",\"column_name\":\"contact_number\",\"column_type\":\"xsd:string\",\"column_description\":\"The contact number of the customer.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Filter the sales data to include only transactions where the total revenue is greater than 1000.\",\"Sort the filtered sales data by transaction date in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data for 2023 from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`total_revenue` > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"filtered_sales\",\"sortBy\":\"transaction_date\",\"order\":\"desc\"},\"output\":\"sorted_sales\",\"comment\":\"Sorting the filtered sales data by transaction date in descending order.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan Boolean expressions contain both comparison and arithmetic operations?","output":"Yes, Boolean expressions can include both, such as '(col1 + col2) > 100 and col3 < 50'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_1\":{\"name\":\"sales_2023\",\"location\":\"C:/data/sales_2023.csv\",\"sheet_name\":null,\"label\":\"Sales Data 2023\",\"description\":\"This table contains sales transactions for the year 2023, including details such as total revenue, total cost, and customer ID.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"total_revenue\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total revenue generated from the transaction.\"},{\"column\":\"col_3\",\"column_name\":\"total_cost\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total cost incurred for the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer associated with the transaction.\"}]},\"table_2\":{\"name\":\"customers\",\"location\":\"C:/data/customers.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Customer Data\",\"description\":\"This table contains customer information including customer ID, name, and region.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the customer is located.\"}]},\"table_3\":{\"name\":\"inventory\",\"location\":\"C:/data/inventory.csv\",\"sheet_name\":null,\"label\":\"Inventory Data\",\"description\":\"This table contains inventory details including product ID, stock levels, and warehouse location.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"stock_level\",\"column_type\":\"xsd:integer\",\"column_description\":\"Current stock level of the product.\"},{\"column\":\"col_3\",\"column_name\":\"warehouse_location\",\"column_type\":\"xsd:string\",\"column_description\":\"Location of the warehouse storing the product.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\",\"Filter the sales data to include only transactions where the total revenue is greater than 1000.\",\"Load the customer data from the Excel file located at C:/data/customers.xlsx.\",\"Join the filtered sales data with the customer data on the customer_id column using an inner join.\",\"Calculate a new column in the joined table to determine the profit margin by subtracting total cost from total revenue.\",\"Aggregate the data by region to calculate the total sales and average profit margin.\",\"Sort the aggregated data by total sales in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_2023.csv\",\"type\":\"csv\"},\"output\":\"sales_2023\",\"comment\":\"Loading the sales data for 2023 from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_2023\",\"query\":\"total_revenue > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with total revenue greater than 1000.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/customers.xlsx\",\"type\":\"xlsx\"},\"output\":\"customers\",\"comment\":\"Loading customer data from an Excel file.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_sales\",\"customers\"],\"joinOn\":[\"customer_id\"],\"joinType\":\"inner\"},\"output\":\"sales_with_customers\",\"comment\":\"Joining filtered sales data with customer data based on customer_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sales_with_customers\",\"columnName\":\"profit_margin\",\"formula\":\"total_revenue - total_cost\"},\"output\":\"sales_with_profit\",\"comment\":\"Calculating profit margin by subtracting total cost from total revenue.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sales_with_profit\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_revenue\",\"name\":\"total_sales\",\"function\":\"sum\"},{\"column\":\"profit_margin\",\"name\":\"average_profit_margin\",\"function\":\"mean\"}]},\"output\":\"regional_sales\",\"comment\":\"Aggregating sales data by region to compute total sales and average profit margin.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"regional_sales\",\"sortBy\":\"total_sales\",\"order\":\"desc\"},\"output\":\"sorted_regional_sales\",\"comment\":\"Sorting the aggregated data by total sales in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains information about suppliers including their sustainability ratings.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_rating\",\"column_name\":\"SUSTAINABILITY_RATING\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability rating of the supplier.\"},{\"column\":\"supplier_name\",\"column_name\":\"SUPPLIER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"shipment_data\",\"location\":\"C:/data/shipment_data.csv\",\"sheet_name\":null,\"label\":\"Shipment Details\",\"description\":\"Includes details about shipments and their carbon emissions.\",\"columns\":[{\"column\":\"shipment_id\",\"column_name\":\"SHIPMENT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each shipment.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier associated with the shipment.\"},{\"column\":\"carbon_emissions\",\"column_name\":\"CARBON_EMISSIONS\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Carbon emissions for the shipment.\"}]},\"table_id3\":{\"name\":\"inventory_data\",\"location\":\"C:/data/inventory_data.csv\",\"sheet_name\":null,\"label\":\"Inventory Information\",\"description\":\"Contains information about inventory levels and stock details.\",\"columns\":[{\"column\":\"inventory_id\",\"column_name\":\"INVENTORY_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each inventory item.\"},{\"column\":\"stock_level\",\"column_name\":\"STOCK_LEVEL\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Current stock level of the inventory item.\"}]}},\"instructions\":[\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\",\"Load the table 'shipment_data' which includes details about shipments and their carbon emissions.\",\"Filter the 'supplier_data' table to include only suppliers with a sustainability rating above 80.\",\"Join the filtered supplier data with the 'shipment_data' table on the supplier ID.\",\"Calculate the total carbon emissions for each supplier by summing the emissions from all their shipments.\",\"Aggregate the data to find the average carbon emissions per shipment for each supplier.\",\"Sort the aggregated data by average carbon emissions in ascending order.\",\"Identify suppliers with average carbon emissions below a threshold of 50.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/shipment_data.csv\",\"type\":\"csv\"},\"output\":\"shipment_data\",\"comment\":\"Loading shipment data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_RATING > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with a sustainability rating above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"shipment_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with shipment data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"CARBON_EMISSIONS\",\"name\":\"total_emissions\",\"function\":\"sum\"}]},\"output\":\"total_emissions_per_supplier\",\"comment\":\"Calculating total carbon emissions for each supplier.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"SUPPLIER_ID\",\"aggregations\":[{\"column\":\"CARBON_EMISSIONS\",\"name\":\"average_emissions\",\"function\":\"mean\"}]},\"output\":\"average_emissions_per_supplier\",\"comment\":\"Calculating average carbon emissions per shipment for each supplier.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"average_emissions_per_supplier\",\"sortBy\":\"average_emissions\",\"order\":\"asc\"},\"output\":\"sorted_suppliers\",\"comment\":\"Sorting suppliers by average carbon emissions in ascending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_suppliers\",\"query\":\"average_emissions < 50\"},\"output\":\"sustainable_suppliers\",\"comment\":\"Identifying suppliers with average carbon emissions below 50.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers, including their sustainability ratings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"sustainability_rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability rating of the supplier, ranging from 0 to 100.\"},{\"column\":\"col_3\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/supply_chain/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Contains information about products, including their associated supplier IDs.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"},{\"column\":\"col_3\",\"column_name\":\"category\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/supply_chain/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains logistics information, not relevant for this workflow.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"logistics_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for logistics records.\"},{\"column\":\"col_2\",\"column_name\":\"transport_mode\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transport used for logistics.\"}]}},\"instructions\":[\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\",\"Filter the supplier data to include only those with a sustainability rating above 80.\",\"Join the filtered supplier data with the 'product_data' table on the supplier ID to associate products with sustainable suppliers.\",\"Aggregate the joined data to calculate the total number of sustainable products per category.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data to access sustainability ratings.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"sustainability_rating > 80\"},\"output\":\"sustainable_suppliers\",\"comment\":\"Filtering suppliers to include only those with a sustainability rating above 80.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data to associate products with suppliers.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"sustainable_suppliers\",\"product_data\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"sustainable_products\",\"comment\":\"Joining filtered supplier data with product data to link products with sustainable suppliers.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sustainable_products\",\"groupBy\":\"category\",\"aggregations\":[{\"column\":\"product_id\",\"name\":\"total_sustainable_products\",\"function\":\"count\"}]},\"output\":\"sustainable_products_per_category\",\"comment\":\"Aggregating data to calculate the total number of sustainable products per category.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers including their sustainability ratings.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"col_2\",\"column_name\":\"sustainability_rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability rating of the supplier, ranging from 0 to 100.\"},{\"column\":\"col_3\",\"column_name\":\"supplier_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/supply_chain/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Contains information about products and their associated suppliers.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"product_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"col_2\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the product to its supplier.\"},{\"column\":\"col_3\",\"column_name\":\"product_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/supply_chain/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains logistics information not directly related to sustainability.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"logistics_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"col_2\",\"column_name\":\"transport_mode\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transport used for logistics.\"}]}},\"instructions\":[\"Load the 'supplier_data' table which contains information about suppliers and their sustainability ratings.\",\"Filter the 'supplier_data' table to include only suppliers with a sustainability rating above 80.\",\"Join the filtered supplier data with the 'product_data' table on the 'supplier_id' column.\",\"Aggregate the joined data to calculate the total number of sustainable products per supplier.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading the supplier data containing sustainability ratings.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"sustainability_rating > 80\"},\"output\":\"sustainable_suppliers\",\"comment\":\"Filtering suppliers to include only those with a sustainability rating above 80.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading the product data to join with filtered supplier data.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"sustainable_suppliers\",\"product_data\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"sustainable_products\",\"comment\":\"Joining sustainable suppliers with product data on supplier_id.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"sustainable_products\",\"groupBy\":\"supplier_id\",\"aggregations\":[{\"column\":\"product_id\",\"name\":\"total_sustainable_products\",\"function\":\"count\"}]},\"output\":\"sustainable_product_count\",\"comment\":\"Aggregating data to calculate the total number of sustainable products per supplier.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nWhat should be considered when using 'detect_outliers' in a time series dataset?","output":"'detect_outliers' identifies anomalies based on statistical thresholds, so it should be used carefully to avoid misclassifying normal fluctuations as outliers."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains information about suppliers including their sustainability ratings.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"sustainability_rating\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability rating of the supplier.\"},{\"column\":\"column_3_id\",\"column_name\":\"units_supplied\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Number of units supplied by the supplier.\"}]},\"table_id2\":{\"name\":\"emissions_data\",\"location\":\"C:/data/emissions_data.xlsx\",\"sheet_name\":null,\"label\":\"Emissions Data\",\"description\":\"Contains emissions data for each supplier.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"total_emissions\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total emissions reported by the supplier.\"},{\"column\":\"column_3_id\",\"column_name\":\"region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the supplier operates.\"}]},\"table_id3\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.xlsx\",\"sheet_name\":\"Sheet1\",\"label\":\"Irrelevant Data\",\"description\":\"Contains data not relevant to the current workflow.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"irrelevant_column\",\"column_type\":\"xsd:string\",\"column_description\":\"Irrelevant data column.\"}]}},\"instructions\":[\"Load the 'supplier_data' table from the CSV file located at C:/data/supplier_data.csv.\",\"Load the 'emissions_data' table from the Excel file located at C:/data/emissions_data.xlsx.\",\"Filter the 'supplier_data' table to include only suppliers with a sustainability rating above 80.\",\"Join the filtered 'supplier_data' table with the 'emissions_data' table on the 'supplier_id' column using an inner join.\",\"Calculate a new column 'emission_per_unit' in the joined table by dividing 'total_emissions' by 'units_supplied'.\",\"Aggregate the joined table by 'region' to calculate the total emissions and average sustainability rating.\",\"Sort the aggregated data by 'total_emissions' in descending order.\",\"Apply a log transformation to the 'total_emissions' column to stabilize variance.\",\"Filter the transformed data to include only regions with a log-transformed total emissions value below 10.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data from a CSV file.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/emissions_data.xlsx\",\"type\":\"xlsx\"},\"output\":\"emissions_data\",\"comment\":\"Loading emissions data from an Excel file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"sustainability_rating > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with a sustainability rating above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"emissions_data\"],\"joinOn\":[\"supplier_id\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with emissions data on supplier_id.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"emission_per_unit\",\"formula\":\"total_emissions / units_supplied\"},\"output\":\"joined_data_with_emission_per_unit\",\"comment\":\"Calculating emissions per unit supplied.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data_with_emission_per_unit\",\"groupBy\":\"region\",\"aggregations\":[{\"column\":\"total_emissions\",\"name\":\"total_emissions\",\"function\":\"sum\"},{\"column\":\"sustainability_rating\",\"name\":\"avg_sustainability_rating\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by region to calculate total emissions and average sustainability rating.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_emissions\",\"order\":\"desc\"},\"output\":\"sorted_aggregated_data\",\"comment\":\"Sorting aggregated data by total emissions in descending order.\"},{\"action\":\"applyTransformations\",\"parameters\":{\"table\":\"sorted_aggregated_data\",\"transformations\":[{\"columns\":[\"total_emissions\"],\"operation\":\"log_transform\"}]},\"output\":\"transformed_data\",\"comment\":\"Applying log transformation to total emissions.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"transformed_data\",\"query\":\"total_emissions < 10\"},\"output\":\"final_filtered_data\",\"comment\":\"Filtering regions with log-transformed total emissions below 10.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"sales_data\",\"location\":\"C:/data/sales_data.csv\",\"sheet_name\":null,\"label\":\"Sales Data\",\"description\":\"This table contains sales transactions including transaction ID, date, customer ID, and total amount.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"transaction_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each transaction.\"},{\"column\":\"col_2\",\"column_name\":\"transaction_date\",\"column_type\":\"xsd:date\",\"column_description\":\"The date when the transaction occurred.\"},{\"column\":\"col_3\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the customer involved in the transaction.\"},{\"column\":\"col_4\",\"column_name\":\"total_amount\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Total amount of the transaction.\"}]},\"table_id2\":{\"name\":\"customer_data\",\"location\":\"C:/data/customer_data.csv\",\"sheet_name\":null,\"label\":\"Customer Data\",\"description\":\"This table contains customer information including customer ID, name, and contact details.\",\"columns\":[{\"column\":\"col_1\",\"column_name\":\"customer_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each customer.\"},{\"column\":\"col_2\",\"column_name\":\"customer_name\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the customer.\"},{\"column\":\"col_3\",\"column_name\":\"contact_number\",\"column_type\":\"xsd:string\",\"column_description\":\"Contact number of the customer.\"}]}},\"instructions\":[\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\",\"Filter the sales data to include only transactions where the total amount is greater than 1000.\",\"Sort the filtered sales data by transaction date in descending order.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/sales_data.csv\",\"type\":\"csv\"},\"output\":\"sales_data\",\"comment\":\"Loading the sales data from the specified CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sales_data\",\"query\":\"`total_amount` > 1000\"},\"output\":\"filtered_sales\",\"comment\":\"Filtering sales data to include only transactions with a total amount greater than 1000.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"filtered_sales\",\"sortBy\":\"transaction_date\",\"order\":\"desc\"},\"output\":\"sorted_sales\",\"comment\":\"Sorting the filtered sales data by transaction date in descending order.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supply_chain_data\",\"location\":\"C:/data/supply_chain.csv\",\"sheet_name\":null,\"label\":\"Supply Chain Data\",\"description\":\"This table contains data on supply chain operations, including supplier regions and carbon emissions.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"supplier_id\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"column_2_id\",\"column_name\":\"supplier_region\",\"column_type\":\"xsd:string\",\"column_description\":\"Region where the supplier is located.\"},{\"column\":\"column_3_id\",\"column_name\":\"carbon_emission\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Carbon emissions reported by the supplier.\"}]},\"table_id2\":{\"name\":\"irrelevant_data\",\"location\":\"C:/data/irrelevant_data.csv\",\"sheet_name\":null,\"label\":\"Irrelevant Data\",\"description\":\"This table contains unrelated data not relevant to the supply chain sustainability analysis.\",\"columns\":[{\"column\":\"column_1_id\",\"column_name\":\"unrelated_column\",\"column_type\":\"xsd:string\",\"column_description\":\"An unrelated column.\"}]}},\"instructions\":[\"Load the 'supply_chain_data' table from the CSV file located at 'C:/data/supply_chain.csv'.\",\"Filter the 'supply_chain_data' table to include only records where the 'carbon_emission' is less than 50.\",\"Aggregate the filtered data by 'supplier_region' to calculate the total 'carbon_emission' for each region.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supply_chain.csv\",\"type\":\"csv\"},\"output\":\"supply_chain_data\",\"comment\":\"Loading the supply chain data from a CSV file.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supply_chain_data\",\"query\":\"carbon_emission < 50\"},\"output\":\"filtered_supply_chain_data\",\"comment\":\"Filtering supply chain data to include only records with carbon emissions less than 50.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"filtered_supply_chain_data\",\"groupBy\":\"supplier_region\",\"aggregations\":[{\"column\":\"carbon_emission\",\"name\":\"total_carbon_emission\",\"function\":\"sum\"}]},\"output\":\"regional_carbon_emission\",\"comment\":\"Aggregating filtered data by supplier region to calculate total carbon emissions for each region.\"}]}"}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Information\",\"description\":\"Contains details about suppliers, including their sustainability ratings.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_rating\",\"column_name\":\"SUSTAINABILITY_RATING\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Rating of supplier's sustainability practices.\"},{\"column\":\"supplier_name\",\"column_name\":\"SUPPLIER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Information\",\"description\":\"Details about products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier linking the product to its supplier.\"},{\"column\":\"eco_friendliness_score\",\"column_name\":\"ECO_FRIENDLINESS_SCORE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Score representing the eco-friendliness of the product.\"},{\"column\":\"category\",\"column_name\":\"CATEGORY\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Information\",\"description\":\"Contains logistics data, not relevant for sustainability analysis.\",\"columns\":[{\"column\":\"logistics_id\",\"column_name\":\"LOGISTICS_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"transport_mode\",\"column_name\":\"TRANSPORT_MODE\",\"column_type\":\"xsd:string\",\"column_description\":\"Mode of transport used for logistics.\"},{\"column\":\"carbon_emission\",\"column_name\":\"CARBON_EMISSION\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Carbon emission associated with the logistics entry.\"}]}},\"instructions\":[\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\",\"Load the table 'product_data' which includes details about products and their associated suppliers.\",\"Filter the 'supplier_data' to retain only suppliers with a sustainability rating above 80.\",\"Join the filtered supplier data with the 'product_data' on the supplier ID.\",\"Calculate a new column 'sustainable_score' by multiplying the product's eco-friendliness score by the supplier's sustainability rating.\",\"Aggregate the data by product category to find the average sustainable score for each category.\",\"Sort the aggregated data by the average sustainable score in descending order.\",\"Identify the top 5 product categories with the highest average sustainable scores.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data containing sustainability ratings.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data with supplier associations.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_RATING > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with sustainability ratings above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"joined_data\",\"columnName\":\"sustainable_score\",\"formula\":\"ECO_FRIENDLINESS_SCORE * SUSTAINABILITY_RATING\"},\"output\":\"data_with_sustainable_score\",\"comment\":\"Calculating sustainable score by multiplying eco-friendliness score with sustainability rating.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"data_with_sustainable_score\",\"groupBy\":\"CATEGORY\",\"aggregations\":[{\"column\":\"sustainable_score\",\"name\":\"average_sustainable_score\",\"function\":\"mean\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data by product category to find average sustainable score.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"average_sustainable_score\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting aggregated data by average sustainable score in descending order.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"sorted_data\",\"query\":\"index < 5\"},\"output\":\"top_5_categories\",\"comment\":\"Identifying the top 5 product categories with the highest average sustainable scores.\"}]}"}
{"input":"# Tamarind workflow language (TWL)\nYou are an expert assistant for the Tamarind Workflow Language\u00e2\u20ac\u201da structured, JSON-based language for defining data transformation workflows on tabular datasets.\n\nYour role is to build, validate, and debug workflows using a strict set of supported operations. You must follow the specifications precisely.\nSpecifically you must:\n - Write syntactically and semantically valid workflows\n - Identify and fix errors in malformed workflows\n - Recommend allowed actions and valid parameters\n - Suggest safe expressions and transformations\n - Explain outputs and behavior of each action\n - Always follow the specification. Do not guess or hallucinate syntax outside the supported operations.\n\nFollow the Tamarind Workflow specification strictly:\n - Only use predefined actions from the schema.\n - Validate all parameters and their values.\n - Do not assume missing details\u00e2\u20ac\u201dreturn a structured error instead.\n - Each action must include a \"comment\" explaining its purpose and chosen parameters.\n - Enclose column or table names with special characters in backticks.\n - Ensure correct execution order: outputs must be created before being referenced.\n - Do not hallucinate or infer unspecified actions.\n - Reject workflows with missing, incorrect, or undefined actions or parameters.\n - If only one actions is invalid, preserve valid actions and return an error for the faulty one.\n - All tables are referred to by unique names.\n\n## Core Concepts\nEach workflow is specified as:\n - A JSON object with `\"workflow\"` (list of actions) or\n - A JSON object with `\"workflow\": []` and an `\"error\"` (string explaining the issue).\n\nEach action is a dictionary with:\n - `name`: (string) \u00e2\u20ac\u201d The operation type (e.g. filterData, forecastData)\n - `parameters`: (object) \u00e2\u20ac\u201d Inputs and options for the action\n - `output`: (string) \u00e2\u20ac\u201d Name for the resulting table\n\n## Supported Actions\nYou must validate structure, required parameters, and values for:\n\n - `identifyTable`: Load a table from a CSV/XLSX URL or path.\n - `filterData`: Filter rows with boolean expressions.\n - `joinTables`: Join two tables with specified keys and join type.\n - `aggregateData`: Group and aggregate using functions like sum, mean, etc.\n - `sortData`: Sort by columns with specified order.\n - `calculateDerivedColumn`: Compute a new column with a math expression.\n - `applyTransformations`: Apply general transformations like fillna, standardize, log_transform, etc.\n - `applyTimeSeriesTransformations`: Time-series operations like resample, lag, rolling_mean, decompose.\n - `forecastData`: Forecast numeric time series using algorithms like holt_winters, xgboost, or detect anomalies with isolation_forest, z-score, etc.\n\nEach action must produce a valid output table name.\n\n## Expression Language\nYou must enforce the following rules for filter and formula expressions.\n\n### Allowed syntax:\n - Arithmetic: `+`, `-`, `*`, `/`, `**`, `%`\n - Comparison: `==`, `!=`, `<`, `>`, `<=`, `>=`\n - Boolean logic: `and`, `or`, `not`, plus `&`, `|`, `~` for vectorised operations\n - Math functions: `log`, `sqrt`, `abs`, `sin`, `mean`, `cumsum`, `diff`, etc.\n - Column access: direct (`col`), attribute (`df.col`), or subscript (`df['col']`)\n - List membership: col in [...], col not in (...)\n - Backtick-enclosed names for columns with spaces or special characters\n\n### Disallowed syntax:\n\n - Any Python control flow (if, for, while)\n - lambda, comprehensions, or generator expressions\n - Function calls outside approved math set\n - Use of is, is not, or scalar boolean constants (e.g., True and False)\n\n### Error Behaviour\nRaise clear errors if:\n - A referenced table or column does not exist\n - An invalid expression is provided\n - An unsupported transformation, aggregation, or forecast configuration is attempted\n - Required parameters are missing\n\n\n\n\nCan logical operators be mixed with comparison operators?","output":"Yes, logical operators such as 'and', 'or', and 'not' can be combined with comparison operators, like 'col1 > 5 and col2 < 10'."}
{"input":"You are an assistant that helps users construct, validate, and correct workflows written in the Tamarind Workflow Language\u00e2\u20ac\u201da JSON-based language for tabular data transformation. Respond with valid JSON or explanations based strictly on the specification.\n\n{\"metadata\":{\"table_id1\":{\"name\":\"supplier_data\",\"location\":\"C:/data/supplier_data.csv\",\"sheet_name\":null,\"label\":\"Supplier Data\",\"description\":\"Contains details of suppliers including their sustainability scores.\",\"columns\":[{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each supplier.\"},{\"column\":\"sustainability_score\",\"column_name\":\"SUSTAINABILITY_SCORE\",\"column_type\":\"xsd:decimal\",\"column_description\":\"Sustainability score of the supplier.\"},{\"column\":\"supplier_name\",\"column_name\":\"SUPPLIER_NAME\",\"column_type\":\"xsd:string\",\"column_description\":\"Name of the supplier.\"}]},\"table_id2\":{\"name\":\"product_data\",\"location\":\"C:/data/product_data.csv\",\"sheet_name\":null,\"label\":\"Product Data\",\"description\":\"Contains details of products and their associated suppliers.\",\"columns\":[{\"column\":\"product_id\",\"column_name\":\"PRODUCT_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each product.\"},{\"column\":\"category\",\"column_name\":\"CATEGORY\",\"column_type\":\"xsd:string\",\"column_description\":\"Category of the product.\"},{\"column\":\"supplier_id\",\"column_name\":\"SUPPLIER_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Identifier for the supplier of the product.\"}]},\"table_id3\":{\"name\":\"logistics_data\",\"location\":\"C:/data/logistics_data.csv\",\"sheet_name\":null,\"label\":\"Logistics Data\",\"description\":\"Contains logistics details which are not relevant to the current workflow.\",\"columns\":[{\"column\":\"logistics_id\",\"column_name\":\"LOGISTICS_ID\",\"column_type\":\"xsd:string\",\"column_description\":\"Unique identifier for each logistics entry.\"},{\"column\":\"route\",\"column_name\":\"ROUTE\",\"column_type\":\"xsd:string\",\"column_description\":\"Route taken for logistics.\"}]}},\"instructions\":[\"Load the 'supplier_data' table containing supplier details and sustainability scores.\",\"Load the 'product_data' table containing product details and associated suppliers.\",\"Filter the 'supplier_data' to include only suppliers with a sustainability score above 80.\",\"Join the filtered supplier data with the 'product_data' on the supplier ID.\",\"Aggregate the joined data to calculate the total number of sustainable products per category.\",\"Sort the aggregated data by the total number of sustainable products in descending order.\",\"Calculate a new column 'sustainability_ratio' as the ratio of sustainable products to total products in each category.\",\"Filter the final data to include only categories with a sustainability ratio above 0.5.\"]}","output":"{\"workflow\":[{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/supplier_data.csv\",\"type\":\"csv\"},\"output\":\"supplier_data\",\"comment\":\"Loading supplier data containing sustainability scores.\"},{\"action\":\"identifyTable\",\"parameters\":{\"location\":\"C:/data/product_data.csv\",\"type\":\"csv\"},\"output\":\"product_data\",\"comment\":\"Loading product data with associated suppliers.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"supplier_data\",\"query\":\"SUSTAINABILITY_SCORE > 80\"},\"output\":\"filtered_supplier_data\",\"comment\":\"Filtering suppliers with sustainability scores above 80.\"},{\"action\":\"joinTables\",\"parameters\":{\"tables\":[\"filtered_supplier_data\",\"product_data\"],\"joinOn\":[\"SUPPLIER_ID\"],\"joinType\":\"inner\"},\"output\":\"joined_data\",\"comment\":\"Joining filtered supplier data with product data on supplier ID.\"},{\"action\":\"aggregateData\",\"parameters\":{\"table\":\"joined_data\",\"groupBy\":\"CATEGORY\",\"aggregations\":[{\"column\":\"PRODUCT_ID\",\"name\":\"total_sustainable_products\",\"function\":\"count\"}]},\"output\":\"aggregated_data\",\"comment\":\"Aggregating data to calculate total sustainable products per category.\"},{\"action\":\"sortData\",\"parameters\":{\"table\":\"aggregated_data\",\"sortBy\":\"total_sustainable_products\",\"order\":\"desc\"},\"output\":\"sorted_data\",\"comment\":\"Sorting categories by total number of sustainable products in descending order.\"},{\"action\":\"calculateDerivedColumn\",\"parameters\":{\"table\":\"sorted_data\",\"columnName\":\"sustainability_ratio\",\"formula\":\"total_sustainable_products / count(PRODUCT_ID)\"},\"output\":\"data_with_ratio\",\"comment\":\"Calculating sustainability ratio as the ratio of sustainable products to total products.\"},{\"action\":\"filterData\",\"parameters\":{\"table\":\"data_with_ratio\",\"query\":\"sustainability_ratio > 0.5\"},\"output\":\"final_data\",\"comment\":\"Filtering to include only categories with a sustainability ratio above 0.5.\"}]}"}
