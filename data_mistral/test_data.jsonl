{"id":"e74784c2-730f-41d8-9aaa-b2e4a61edc1c","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the employee performance data from the CSV file.\", \"Filter the data to include only employees with a performance score above 85.\", \"Sort the filtered data by employee name in ascending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"employee_performance\", \"location\": \"C:/data/employee_performance.csv\", \"sheet_name\": null, \"label\": \"Employee Performance Data\", \"description\": \"This table contains performance scores for employees, including their names, departments, and scores.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"employee_name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the employee.\"}, {\"column\": \"col_3\", \"column_name\": \"department\", \"column_type\": \"xsd:string\", \"column_description\": \"The department in which the employee works.\"}, {\"column\": \"col_4\", \"column_name\": \"performance_score\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The performance score of the employee.\"}]}, \"table_id2\": {\"name\": \"employee_salaries\", \"location\": \"C:/data/employee_salaries.csv\", \"sheet_name\": null, \"label\": \"Employee Salary Data\", \"description\": \"This table contains salary information for employees, including their base salary and bonuses.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"base_salary\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The base salary of the employee.\"}, {\"column\": \"col_3\", \"column_name\": \"bonus\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The bonus amount for the employee.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/employee_performance.csv\", \"type\": \"csv\"}, \"output\": \"employee_performance\", \"comment\": \"Loading the employee performance data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_performance\", \"query\": \"performance_score > 85\"}, \"output\": \"high_performance_employees\", \"comment\": \"Filtering the data to include only employees with a performance score above 85.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"high_performance_employees\", \"sortBy\": \"employee_name\", \"order\": \"asc\"}, \"output\": \"sorted_high_performance_employees\", \"comment\": \"Sorting the filtered data by employee name in ascending order.\"}]"}]}
{"id":"10aecd30-86be-433d-acf0-a59e44e1460e","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\", \"Load the customer information from the Excel file located at C:/data/customers.xlsx.\", \"Filter the sales data to include only transactions with a total revenue greater than 1000.\", \"Join the filtered sales data with the customer information using the customer_id column.\", \"Calculate a new column in the joined table to determine the profit by subtracting total cost from total revenue.\", \"Aggregate the data by customer region to compute the total sales and average profit.\", \"Sort the aggregated data by total sales in descending order.\", \"Apply a log transformation to the average profit column to stabilize variance.\", \"Forecast the next 12 months of total sales using the Holt-Winters method.\", \"Save the final forecasted data to a new CSV file at C:/data/forecasted_sales.csv.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_2023\", \"location\": \"C:/data/sales_2023.csv\", \"sheet_name\": null, \"label\": \"Sales Data 2023\", \"description\": \"Contains sales transactions for the year 2023, including revenue and cost details.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer involved in the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"total_revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total revenue generated from the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"total_cost\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total cost incurred for the transaction.\"}, {\"column\": \"col_5\", \"column_name\": \"transaction_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date when the transaction occurred.\"}]}, \"table_id2\": {\"name\": \"customers\", \"location\": \"C:/data/customers.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Information\", \"description\": \"Contains customer details including customer ID and region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the customer is located.\"}]}, \"table_id3\": {\"name\": \"inventory\", \"location\": \"C:/data/inventory.csv\", \"sheet_name\": null, \"label\": \"Inventory Data\", \"description\": \"Contains information about the inventory levels of products.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"stock_level\", \"column_type\": \"xsd:integer\", \"column_description\": \"Current stock level of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_2023.csv\", \"type\": \"csv\"}, \"output\": \"sales_2023\", \"comment\": \"Loading the sales data for 2023 from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customers.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customers\", \"comment\": \"Loading customer information from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_2023\", \"query\": \"`total_revenue` > 1000\"}, \"output\": \"filtered_sales\", \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales\", \"customers\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"sales_with_customers\", \"comment\": \"Joining filtered sales data with customer information based on customer_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sales_with_customers\", \"columnName\": \"profit\", \"formula\": \"`total_revenue` - `total_cost`\"}, \"output\": \"sales_with_profit\", \"comment\": \"Calculating profit by subtracting total cost from total revenue.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_with_profit\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_revenue\", \"name\": \"total_sales\", \"function\": \"sum\"}, {\"column\": \"profit\", \"name\": \"average_profit\", \"function\": \"mean\"}]}, \"output\": \"regional_sales\", \"comment\": \"Aggregating data by region to compute total sales and average profit.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"regional_sales\", \"sortBy\": \"total_sales\", \"order\": \"desc\"}, \"output\": \"sorted_regional_sales\", \"comment\": \"Sorting the aggregated data by total sales in descending order.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sorted_regional_sales\", \"transformations\": [{\"columns\": [\"average_profit\"], \"operation\": \"log_transform\"}]}, \"output\": \"transformed_sales\", \"comment\": \"Applying a log transformation to the average profit column.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"transformed_sales\", \"columns\": [\"total_sales\"], \"forecastParameters\": {\"forecastPeriod\": 12, \"frequency\": \"MS\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 12, \"dateColumn\": \"transaction_date\"}}, \"output\": \"forecasted_sales\", \"comment\": \"Forecasting the next 12 months of total sales using the Holt-Winters method.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/forecasted_sales.csv\", \"type\": \"csv\"}, \"output\": \"forecasted_sales\", \"comment\": \"Saving the final forecasted data to a new CSV file.\"}]"}]}
{"id":"791cc786-f0b9-40e7-bda3-0025aae45061","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'energy_usage' which contains daily energy consumption data.\", \"Load the table 'weather_data' which contains daily weather information.\", \"Filter the 'energy_usage' table to include only data from the year 2023.\", \"Join the 'energy_usage' table with the 'weather_data' table on the 'date' column.\", \"Calculate a new column 'adjusted_usage' by subtracting 'solar_generation' from 'total_usage'.\", \"Aggregate the joined table by 'month' to calculate the total 'adjusted_usage'.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_usage\", \"location\": \"C:/data/energy_usage.csv\", \"sheet_name\": null, \"label\": \"Energy Usage Data\", \"description\": \"This table contains daily energy consumption data including total usage and solar generation.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy usage record.\"}, {\"column\": \"col_2\", \"column_name\": \"total_usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The total energy usage for the day.\"}, {\"column\": \"col_3\", \"column_name\": \"solar_generation\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy generated by solar panels.\"}]}, \"table_id2\": {\"name\": \"weather_data\", \"location\": \"C:/data/weather_data.csv\", \"sheet_name\": null, \"label\": \"Weather Data\", \"description\": \"This table contains daily weather information including temperature and humidity.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the weather record.\"}, {\"column\": \"col_2\", \"column_name\": \"temperature\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The average temperature for the day.\"}, {\"column\": \"col_3\", \"column_name\": \"humidity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The average humidity for the day.\"}]}, \"table_id3\": {\"name\": \"historical_energy_prices\", \"location\": \"C:/data/historical_energy_prices.csv\", \"sheet_name\": null, \"label\": \"Historical Energy Prices\", \"description\": \"This table contains historical energy prices data, which is not relevant for the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the price record.\"}, {\"column\": \"col_2\", \"column_name\": \"price_per_kwh\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The price of energy per kilowatt-hour.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/energy_usage.csv\", \"type\": \"csv\"}, \"output\": \"energy_usage\", \"comment\": \"Loading the energy usage data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/weather_data.csv\", \"type\": \"csv\"}, \"output\": \"weather_data\", \"comment\": \"Loading the weather data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_usage\", \"query\": \"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"}, \"output\": \"energy_usage_2023\", \"comment\": \"Filtering energy usage data to include only records from the year 2023.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"energy_usage_2023\", \"weather_data\"], \"joinOn\": [\"date\"], \"joinType\": \"inner\"}, \"output\": \"joined_energy_weather\", \"comment\": \"Joining energy usage data with weather data on the date column.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_energy_weather\", \"columnName\": \"adjusted_usage\", \"formula\": \"`total_usage` - `solar_generation`\"}, \"output\": \"energy_with_adjusted_usage\", \"comment\": \"Calculating adjusted energy usage by subtracting solar generation from total usage.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"energy_with_adjusted_usage\", \"groupBy\": \"month\", \"aggregations\": [{\"column\": \"adjusted_usage\", \"name\": \"total_adjusted_usage\", \"function\": \"sum\"}]}, \"output\": \"monthly_adjusted_usage\", \"comment\": \"Aggregating the adjusted energy usage by month to calculate total adjusted usage.\"}]"}]}
{"id":"d79c2124-a3c4-4986-b0be-2dbb9b287c86","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the product specifications table from the CSV file.\", \"Load the market research data from the Excel sheet.\", \"Filter the market research data to include only entries from the last year.\", \"Join the product specifications with the filtered market research data on the product ID.\", \"Calculate the average customer rating for each product.\", \"Sort the products by their average customer rating in descending order.\", \"Identify the top 10 products with the highest average customer ratings.\", \"Aggregate the sales data by product category to find total sales.\", \"Calculate the profit margin for each product by subtracting the cost from the sales price.\", \"Forecast the sales for the next quarter using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"product_specifications\", \"location\": \"C:/data/product_specifications.csv\", \"sheet_name\": null, \"label\": \"Product Specifications\", \"description\": \"Contains detailed specifications for each product, including product ID, name, and cost.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"cost\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Cost of the product.\"}]}, \"table_id2\": {\"name\": \"market_research\", \"location\": \"C:/data/market_research.xlsx\", \"sheet_name\": \"2023\", \"label\": \"Market Research Data\", \"description\": \"Contains market research data including customer ratings and feedback.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Product ID to link with product specifications.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_rating\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Customer rating for the product.\"}, {\"column\": \"col_3\", \"column_name\": \"feedback_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the customer feedback.\"}]}, \"table_id3\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"Contains sales data including sales price and category for each product.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Product ID to link with product specifications.\"}, {\"column\": \"col_2\", \"column_name\": \"sales_price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Sales price of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"Category of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_specifications.csv\", \"type\": \"csv\"}, \"output\": \"product_specifications\", \"comment\": \"Loading the product specifications from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/market_research.xlsx\", \"type\": \"xlsx\"}, \"output\": \"market_research\", \"comment\": \"Loading the market research data from an Excel sheet.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"market_research\", \"query\": \"`feedback_date` >= '2022-01-01'\"}, \"output\": \"filtered_market_research\", \"comment\": \"Filtering market research data to include only entries from the last year.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"product_specifications\", \"filtered_market_research\"], \"joinOn\": [\"product_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining product specifications with filtered market research data on product ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"product_id\", \"aggregations\": [{\"column\": \"customer_rating\", \"name\": \"average_rating\", \"function\": \"mean\"}]}, \"output\": \"average_ratings\", \"comment\": \"Calculating the average customer rating for each product.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"average_ratings\", \"sortBy\": \"average_rating\", \"order\": \"desc\"}, \"output\": \"sorted_products\", \"comment\": \"Sorting products by their average customer rating in descending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sorted_products\", \"query\": \"index < 10\"}, \"output\": \"top_10_products\", \"comment\": \"Identifying the top 10 products with the highest average customer ratings.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_data\", \"groupBy\": \"category\", \"aggregations\": [{\"column\": \"sales_price\", \"name\": \"total_sales\", \"function\": \"sum\"}]}, \"output\": \"category_sales\", \"comment\": \"Aggregating sales data by product category to find total sales.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sales_data\", \"columnName\": \"profit_margin\", \"formula\": \"`sales_price` - `cost`\"}, \"output\": \"sales_with_profit\", \"comment\": \"Calculating the profit margin for each product by subtracting the cost from the sales price.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"sales_data\", \"columns\": [\"sales_price\"], \"forecastParameters\": {\"forecastPeriod\": 90, \"frequency\": \"D\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 30, \"dateColumn\": \"product_id\"}}, \"output\": \"sales_forecast\", \"comment\": \"Forecasting the sales for the next quarter using the Holt-Winters method.\"}]"}]}
{"id":"4d8c8c5b-a17b-4313-bbf1-5c0d5e2bab13","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback data from the CSV file.\", \"Load the customer purchase history from the Excel file.\", \"Filter the feedback data to include only feedback with a rating of 4 or 5.\", \"Join the filtered feedback data with the purchase history on customer ID.\", \"Calculate the total purchase amount for each customer.\", \"Sort the customers by total purchase amount in descending order.\", \"Identify the top 10 customers based on total purchase amount.\", \"Aggregate the feedback data to find the average rating for each product.\", \"Filter the aggregated data to include only products with an average rating above 4.5.\", \"Generate a report of the top 10 customers and high-rated products.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback\", \"description\": \"Contains customer feedback including ratings and comments.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"rating\", \"column_type\": \"xsd:integer\", \"column_description\": \"Customer rating for the product, ranging from 1 to 5.\"}, {\"column\": \"column_3_id\", \"column_name\": \"comment\", \"column_type\": \"xsd:string\", \"column_description\": \"Customer's feedback comment.\"}]}, \"table_id2\": {\"name\": \"purchase_history\", \"location\": \"C:/data/purchase_history.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Purchase History\", \"description\": \"Records of customer purchases including product details and amounts.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_3_id\", \"column_name\": \"purchase_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount spent by the customer on the purchase.\"}]}, \"table_id3\": {\"name\": \"product_catalog\", \"location\": \"C:/data/product_catalog.xlsx\", \"sheet_name\": \"Products\", \"label\": \"Product Catalog\", \"description\": \"List of products available for purchase with details.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"column_3_id\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"Category to which the product belongs.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading customer feedback data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/purchase_history.xlsx\", \"type\": \"xlsx\"}, \"output\": \"purchase_history\", \"comment\": \"Loading customer purchase history from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"rating >= 4\"}, \"output\": \"positive_feedback\", \"comment\": \"Filtering feedback to include only ratings of 4 or 5.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"positive_feedback\", \"purchase_history\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"feedback_with_purchases\", \"comment\": \"Joining filtered feedback with purchase history on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"feedback_with_purchases\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"purchase_amount\", \"name\": \"total_purchase\", \"function\": \"sum\"}]}, \"output\": \"customer_total_purchases\", \"comment\": \"Calculating total purchase amount for each customer.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"customer_total_purchases\", \"sortBy\": \"total_purchase\", \"order\": \"desc\"}, \"output\": \"sorted_customers\", \"comment\": \"Sorting customers by total purchase amount in descending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sorted_customers\", \"query\": \"index < 10\"}, \"output\": \"top_10_customers\", \"comment\": \"Identifying the top 10 customers based on total purchase amount.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"positive_feedback\", \"groupBy\": \"product_id\", \"aggregations\": [{\"column\": \"rating\", \"name\": \"average_rating\", \"function\": \"mean\"}]}, \"output\": \"product_average_ratings\", \"comment\": \"Aggregating feedback to find average rating for each product.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"product_average_ratings\", \"query\": \"average_rating > 4.5\"}, \"output\": \"high_rated_products\", \"comment\": \"Filtering products with an average rating above 4.5.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"top_10_customers\", \"high_rated_products\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"customer_product_report\", \"comment\": \"Generating a report of the top 10 customers and high-rated products.\"}]"}]}
{"id":"ecba1760-69e0-4e97-b9aa-ed15ec138d30","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the product specifications table from the CSV file.\", \"Filter the product specifications to include only those with a status of 'approved'.\", \"Join the filtered product specifications with the supplier information table on the supplier ID.\", \"Calculate the cost per unit by dividing the total cost by the quantity for each product.\", \"Aggregate the data to find the total cost per supplier.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"product_specifications\", \"location\": \"C:/data/product_specifications.csv\", \"sheet_name\": null, \"label\": \"Product Specifications\", \"description\": \"Contains detailed specifications of products including status, cost, and quantity.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"status\", \"column_type\": \"xsd:string\", \"column_description\": \"Approval status of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"total_cost\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total cost of the product.\"}, {\"column\": \"col_4\", \"column_name\": \"quantity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Quantity of the product.\"}, {\"column\": \"col_5\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier of the product.\"}]}, \"table_id2\": {\"name\": \"supplier_information\", \"location\": \"C:/data/supplier_information.csv\", \"sheet_name\": null, \"label\": \"Supplier Information\", \"description\": \"Contains information about suppliers including their IDs and names.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"col_2\", \"column_name\": \"supplier_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the supplier.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"Contains data that is not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_specifications.csv\", \"type\": \"csv\"}, \"output\": \"product_specifications\", \"comment\": \"Loading the product specifications from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"product_specifications\", \"query\": \"status == 'approved'\"}, \"output\": \"approved_products\", \"comment\": \"Filtering the product specifications to include only approved products.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_information.csv\", \"type\": \"csv\"}, \"output\": \"supplier_information\", \"comment\": \"Loading the supplier information from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"approved_products\", \"supplier_information\"], \"joinOn\": [\"supplier_id\"], \"joinType\": \"inner\"}, \"output\": \"products_with_suppliers\", \"comment\": \"Joining approved products with supplier information on supplier ID.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"products_with_suppliers\", \"columnName\": \"cost_per_unit\", \"formula\": \"total_cost / quantity\"}, \"output\": \"products_with_cost_per_unit\", \"comment\": \"Calculating cost per unit by dividing total cost by quantity for each product.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"products_with_cost_per_unit\", \"groupBy\": \"supplier_id\", \"aggregations\": [{\"column\": \"total_cost\", \"name\": \"total_cost_per_supplier\", \"function\": \"sum\"}]}, \"output\": \"total_cost_per_supplier\", \"comment\": \"Aggregating data to find the total cost per supplier.\"}]"}]}
{"id":"4ed0f287-e80c-4c33-bb06-b9c574995a0f","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback data from the CSV file.\", \"Load the customer details data from the Excel file.\", \"Filter the feedback data to include only feedback with a rating of 4 or higher.\", \"Join the filtered feedback data with the customer details on the customer ID.\", \"Aggregate the joined data to calculate the average rating per customer.\", \"Sort the aggregated data by average rating in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback Data\", \"description\": \"Contains customer feedback with ratings and comments.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"rating\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Customer rating from 1 to 5.\"}, {\"column\": \"column_3_id\", \"column_name\": \"feedback\", \"column_type\": \"xsd:string\", \"column_description\": \"Customer feedback comments.\"}]}, \"table_id2\": {\"name\": \"customer_details\", \"location\": \"C:/data/customer_details.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Details Data\", \"description\": \"Contains detailed information about customers.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"column_3_id\", \"column_name\": \"email\", \"column_type\": \"xsd:string\", \"column_description\": \"Email address of the customer.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant data column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading customer feedback data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_details.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_details\", \"comment\": \"Loading customer details data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"rating >= 4\"}, \"output\": \"filtered_feedback\", \"comment\": \"Filtering feedback data to include only ratings of 4 or higher.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_feedback\", \"customer_details\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered feedback with customer details on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"rating\", \"name\": \"average_rating\", \"function\": \"mean\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Calculating average rating per customer.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"average_rating\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting data by average rating in descending order.\"}]"}]}
{"id":"578811ae-973d-4cdd-b251-694cab06845c","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'raw_materials' which contains information about the materials used in manufacturing.\", \"Load the table 'production_schedule' which contains the schedule of production activities.\", \"Filter the 'raw_materials' table to include only materials with a stock level greater than 100.\", \"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\", \"Calculate a new column 'total_cost' in the joined table by multiplying 'quantity' by 'unit_price'.\", \"Aggregate the joined table by 'product_id' to find the total quantity and total cost for each product.\", \"Sort the aggregated data by 'total_cost' in descending order.\", \"Save the sorted data to a new table 'final_production_plan'.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"raw_materials\", \"location\": \"C:/data/manufacturing/raw_materials.csv\", \"sheet_name\": null, \"label\": \"Raw Materials Inventory\", \"description\": \"Contains information about raw materials including their stock levels and unit prices.\", \"columns\": [{\"column\": \"material_id\", \"column_name\": \"MATERIAL_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each material.\"}, {\"column\": \"stock_level\", \"column_name\": \"STOCK_LEVEL\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Current stock level of the material.\"}, {\"column\": \"unit_price\", \"column_name\": \"UNIT_PRICE\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Price per unit of the material.\"}]}, \"table_id2\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.xlsx\", \"sheet_name\": \"Schedule\", \"label\": \"Production Schedule\", \"description\": \"Contains the schedule of production activities including product IDs and required materials.\", \"columns\": [{\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"material_id\", \"column_name\": \"MATERIAL_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the material required for production.\"}, {\"column\": \"quantity\", \"column_name\": \"QUANTITY\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Quantity of material required for production.\"}]}, \"table_id3\": {\"name\": \"machine_maintenance\", \"location\": \"C:/data/manufacturing/machine_maintenance.csv\", \"sheet_name\": null, \"label\": \"Machine Maintenance Records\", \"description\": \"Contains records of machine maintenance activities, not relevant to the current workflow.\", \"columns\": [{\"column\": \"machine_id\", \"column_name\": \"MACHINE_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each machine.\"}, {\"column\": \"maintenance_date\", \"column_name\": \"MAINTENANCE_DATE\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the maintenance activity.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/raw_materials.csv\", \"type\": \"csv\"}, \"output\": \"raw_materials\", \"comment\": \"Loading the raw materials inventory from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.xlsx\", \"type\": \"xlsx\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the production schedule from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"raw_materials\", \"query\": \"STOCK_LEVEL > 100\"}, \"output\": \"filtered_materials\", \"comment\": \"Filtering raw materials to include only those with stock levels greater than 100.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_materials\", \"production_schedule\"], \"joinOn\": [\"MATERIAL_ID\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered materials with production schedule on material ID.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"total_cost\", \"formula\": \"QUANTITY * UNIT_PRICE\"}, \"output\": \"joined_data_with_cost\", \"comment\": \"Calculating total cost by multiplying quantity by unit price.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data_with_cost\", \"groupBy\": \"PRODUCT_ID\", \"aggregations\": [{\"column\": \"QUANTITY\", \"name\": \"total_quantity\", \"function\": \"sum\"}, {\"column\": \"total_cost\", \"name\": \"total_cost\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data by product ID to find total quantity and cost.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_cost\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting aggregated data by total cost in descending order.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/final_production_plan.csv\", \"type\": \"csv\"}, \"output\": \"final_production_plan\", \"comment\": \"Saving the sorted data to a new CSV file as the final production plan.\"}]"}]}
{"id":"e144f2b8-50ba-4d3e-802e-279b922beaec","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'production_schedule' which contains the daily production targets.\", \"Filter the production schedule to include only entries for the current month.\", \"Load the table 'machine_status' which contains the operational status of machines.\", \"Join the filtered production schedule with machine status data on the machine ID.\", \"Aggregate the joined data to calculate the total production target for each machine.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.csv\", \"sheet_name\": null, \"label\": \"Production Schedule\", \"description\": \"This table contains daily production targets for each machine in the manufacturing plant.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date for which the production target is set.\"}, {\"column\": \"col_2\", \"column_name\": \"machine_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each machine.\"}, {\"column\": \"col_3\", \"column_name\": \"production_target\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The production target for the machine on the given date.\"}]}, \"table_id2\": {\"name\": \"machine_status\", \"location\": \"C:/data/manufacturing/machine_status.csv\", \"sheet_name\": null, \"label\": \"Machine Status\", \"description\": \"This table contains the operational status of machines in the manufacturing plant.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"machine_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each machine.\"}, {\"column\": \"col_2\", \"column_name\": \"status\", \"column_type\": \"xsd:string\", \"column_description\": \"The operational status of the machine (e.g., operational, maintenance, offline).\"}]}, \"table_id3\": {\"name\": \"inventory_data\", \"location\": \"C:/data/manufacturing/inventory_data.csv\", \"sheet_name\": null, \"label\": \"Inventory Data\", \"description\": \"This table contains inventory levels for raw materials.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"material_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each raw material.\"}, {\"column\": \"col_2\", \"column_name\": \"quantity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The quantity of the raw material available in inventory.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.csv\", \"type\": \"csv\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the production schedule containing daily production targets.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"production_schedule\", \"query\": \"`date`.dt.month == pd.Timestamp('now').month\"}, \"output\": \"current_month_schedule\", \"comment\": \"Filtering the production schedule to include only entries for the current month.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/machine_status.csv\", \"type\": \"csv\"}, \"output\": \"machine_status\", \"comment\": \"Loading the machine status data to check operational status of machines.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"current_month_schedule\", \"machine_status\"], \"joinOn\": [\"machine_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining the filtered production schedule with machine status data on machine ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"machine_id\", \"aggregations\": [{\"column\": \"production_target\", \"name\": \"total_production_target\", \"function\": \"sum\"}]}, \"output\": \"aggregated_production_targets\", \"comment\": \"Aggregating the joined data to calculate the total production target for each machine.\"}]"}]}
{"id":"cfde3c74-9d0d-4776-b955-02b45b515dbd","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'shipment_data' which contains details of all shipments.\", \"Filter the shipments to include only those with a status of 'delivered'.\", \"Join the filtered shipment data with 'customer_data' on the 'customer_id' column.\", \"Aggregate the joined data to calculate the total weight of shipments per customer.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"shipment_data\", \"location\": \"C:/data/logistics/shipment_data.csv\", \"sheet_name\": null, \"label\": \"Shipment Data\", \"description\": \"This table contains details of all shipments including shipment ID, customer ID, status, and weight.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer associated with the shipment.\"}, {\"column\": \"col_3\", \"column_name\": \"status\", \"column_type\": \"xsd:string\", \"column_description\": \"Current status of the shipment (e.g., delivered, in transit).\"}, {\"column\": \"col_4\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the shipment in kilograms.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/logistics/customer_data.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"This table contains customer information including customer ID and name.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}]}, \"table_id3\": {\"name\": \"inventory_data\", \"location\": \"C:/data/logistics/inventory_data.csv\", \"sheet_name\": null, \"label\": \"Inventory Data\", \"description\": \"This table contains inventory details which are not relevant for the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"item_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each inventory item.\"}, {\"column\": \"col_2\", \"column_name\": \"quantity\", \"column_type\": \"xsd:integer\", \"column_description\": \"Quantity of the inventory item available.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/shipment_data.csv\", \"type\": \"csv\"}, \"output\": \"shipment_data\", \"comment\": \"Loading the shipment data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"shipment_data\", \"query\": \"status == 'delivered'\"}, \"output\": \"delivered_shipments\", \"comment\": \"Filtering shipments to include only those with a status of 'delivered'.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"delivered_shipments\", \"customer_data\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"shipments_with_customers\", \"comment\": \"Joining filtered shipment data with customer data on 'customer_id'.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"shipments_with_customers\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"weight\", \"name\": \"total_weight\", \"function\": \"sum\"}]}, \"output\": \"total_weight_per_customer\", \"comment\": \"Aggregating data to calculate the total weight of shipments per customer.\"}]"}]}
{"id":"c7d2634b-c978-4b7c-9723-a02b520de8fa","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback table from the CSV file.\", \"Filter the feedback to include only entries with a rating below 3.\", \"Load the customer details table from the Excel file.\", \"Join the filtered feedback with customer details using the customer ID.\", \"Aggregate the data to find the average rating per region.\", \"Sort the aggregated data by average rating in ascending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback\", \"description\": \"Table containing customer feedback with ratings and comments.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"rating\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Customer rating from 1 to 5.\"}, {\"column\": \"col_3\", \"column_name\": \"comment\", \"column_type\": \"xsd:string\", \"column_description\": \"Feedback comment from the customer.\"}]}, \"table_id2\": {\"name\": \"customer_details\", \"location\": \"C:/data/customer_details.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Details\", \"description\": \"Table containing detailed information about customers including their region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the customer is located.\"}, {\"column\": \"col_3\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}]}, \"table_id3\": {\"name\": \"product_catalog\", \"location\": \"C:/data/product_catalog.csv\", \"sheet_name\": null, \"label\": \"Product Catalog\", \"description\": \"Table containing information about products offered by the company.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Price of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading customer feedback data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"rating < 3\"}, \"output\": \"low_rating_feedback\", \"comment\": \"Filtering feedback to include only entries with a rating below 3.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_details.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_details\", \"comment\": \"Loading customer details from an Excel file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"low_rating_feedback\", \"customer_details\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"feedback_with_details\", \"comment\": \"Joining filtered feedback with customer details using the customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"feedback_with_details\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"rating\", \"name\": \"average_rating\", \"function\": \"mean\"}]}, \"output\": \"average_rating_by_region\", \"comment\": \"Aggregating data to find the average rating per region.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"average_rating_by_region\", \"sortBy\": \"average_rating\", \"order\": \"asc\"}, \"output\": \"sorted_average_rating_by_region\", \"comment\": \"Sorting the aggregated data by average rating in ascending order.\"}]"}]}
{"id":"0fc01bb5-be3f-43dc-899c-45ba6b911874","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\", \"Apply a time series transformation to set the 'date' column as the index.\", \"Forecast the 'sales' column for the next 30 days using the Holt-Winters algorithm with a seasonal period of 7 days.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains historical sales data with columns for date and sales amount.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the sales record.\"}, {\"column\": \"col_2\", \"column_name\": \"sales\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The sales amount for the corresponding date.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"This table contains customer information including customer ID and demographics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"applyTimeSeriesTransformations\", \"parameters\": {\"table\": \"sales_data\", \"transformations\": [{\"columns\": [\"date\"], \"operation\": \"set_index\"}]}, \"output\": \"sales_data_indexed\", \"comment\": \"Setting the 'date' column as the index for time series analysis.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"sales_data_indexed\", \"columns\": [\"sales\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7}}, \"output\": \"sales_forecast\", \"comment\": \"Forecasting the 'sales' column for the next 30 days using Holt-Winters algorithm.\"}]"}]}
{"id":"c0c3d9c5-fc94-4908-9b9d-b46914511161","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback data from the CSV file.\", \"Load the customer purchase history from the Excel file.\", \"Filter the feedback data to include only feedback with a rating of 4 or higher.\", \"Join the filtered feedback data with the purchase history on customer ID.\", \"Aggregate the joined data to calculate the average purchase amount per customer.\", \"Sort the aggregated data by average purchase amount in descending order.\"]\n\n                ### Context:\n                {\"table_1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback Data\", \"description\": \"This table contains customer feedback including ratings and comments.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"rating\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Rating given by the customer, ranging from 1 to 5.\"}, {\"column\": \"col_3\", \"column_name\": \"comment\", \"column_type\": \"xsd:string\", \"column_description\": \"Feedback comment provided by the customer.\"}]}, \"table_2\": {\"name\": \"purchase_history\", \"location\": \"C:/data/purchase_history.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Purchase History\", \"description\": \"This table contains records of customer purchases including amounts and dates.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"purchase_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Amount spent by the customer in a single purchase.\"}, {\"column\": \"col_3\", \"column_name\": \"purchase_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the purchase.\"}]}, \"table_3\": {\"name\": \"customer_support_tickets\", \"location\": \"C:/data/customer_support_tickets.csv\", \"sheet_name\": null, \"label\": \"Customer Support Tickets\", \"description\": \"This table contains records of customer support tickets.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"ticket_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each support ticket.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_3\", \"column_name\": \"issue_description\", \"column_type\": \"xsd:string\", \"column_description\": \"Description of the issue reported by the customer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading customer feedback data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/purchase_history.xlsx\", \"type\": \"xlsx\"}, \"output\": \"purchase_history\", \"comment\": \"Loading customer purchase history from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"rating >= 4\"}, \"output\": \"positive_feedback\", \"comment\": \"Filtering feedback data to include only feedback with a rating of 4 or higher.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"positive_feedback\", \"purchase_history\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered feedback data with purchase history on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"purchase_amount\", \"name\": \"average_purchase_amount\", \"function\": \"mean\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating joined data to calculate the average purchase amount per customer.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"average_purchase_amount\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting aggregated data by average purchase amount in descending order.\"}]"}]}
{"id":"fa0c6260-00a3-4bd1-ba7a-842a20b7e3ca","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback table from the CSV file.\", \"Filter the feedback to include only entries with a rating below 3.\", \"Join the filtered feedback with the customer details table using customer ID.\", \"Aggregate the joined table to count the number of negative feedbacks per region.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback\", \"description\": \"This table contains customer feedback including ratings and comments.\", \"columns\": [{\"column\": \"feedback_id\", \"column_name\": \"FEEDBACK_ID\", \"column_type\": \"xsd:integer\", \"column_description\": \"Unique identifier for each feedback entry.\"}, {\"column\": \"customer_id\", \"column_name\": \"CUSTOMER_ID\", \"column_type\": \"xsd:integer\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"rating\", \"column_name\": \"RATING\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Customer rating from 1 to 5.\"}, {\"column\": \"comments\", \"column_name\": \"COMMENTS\", \"column_type\": \"xsd:string\", \"column_description\": \"Customer comments on the service.\"}]}, \"table_id2\": {\"name\": \"customer_details\", \"location\": \"C:/data/customer_details.csv\", \"sheet_name\": null, \"label\": \"Customer Details\", \"description\": \"This table contains details about customers including their region.\", \"columns\": [{\"column\": \"customer_id\", \"column_name\": \"CUSTOMER_ID\", \"column_type\": \"xsd:integer\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"name\", \"column_name\": \"NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"region\", \"column_name\": \"REGION\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the customer is located.\"}]}, \"table_id3\": {\"name\": \"product_catalog\", \"location\": \"C:/data/product_catalog.csv\", \"sheet_name\": null, \"label\": \"Product Catalog\", \"description\": \"This table contains information about products offered by the company.\", \"columns\": [{\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:integer\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"product_name\", \"column_name\": \"PRODUCT_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"price\", \"column_name\": \"PRICE\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Price of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading the customer feedback table from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"RATING < 3\"}, \"output\": \"negative_feedback\", \"comment\": \"Filtering feedback to include only entries with a rating below 3.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_details.csv\", \"type\": \"csv\"}, \"output\": \"customer_details\", \"comment\": \"Loading the customer details table from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"negative_feedback\", \"customer_details\"], \"joinOn\": [\"CUSTOMER_ID\"], \"joinType\": \"inner\"}, \"output\": \"joined_feedback\", \"comment\": \"Joining the filtered feedback with customer details using customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_feedback\", \"groupBy\": \"REGION\", \"aggregations\": [{\"column\": \"FEEDBACK_ID\", \"name\": \"negative_feedback_count\", \"function\": \"count\"}]}, \"output\": \"regional_negative_feedback\", \"comment\": \"Aggregating the joined table to count the number of negative feedbacks per region.\"}]"}]}
{"id":"b4917d66-9db4-4d1b-9f7c-ea9a612a4a9d","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer demographics table from the CSV file.\", \"Load the marketing campaign results table from the Excel file.\", \"Filter the marketing campaign results to include only successful campaigns.\", \"Join the filtered campaign results with customer demographics based on customer ID.\", \"Aggregate the joined data to calculate the total number of successful campaigns per age group.\", \"Sort the aggregated data by the total number of successful campaigns in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_demographics\", \"location\": \"C:/data/customer_demographics.csv\", \"sheet_name\": null, \"label\": \"Customer Demographics\", \"description\": \"Contains demographic information about customers, including age, gender, and location.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age_group\", \"column_type\": \"xsd:string\", \"column_description\": \"Age group of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"gender\", \"column_type\": \"xsd:string\", \"column_description\": \"Gender of the customer.\"}]}, \"table_id2\": {\"name\": \"marketing_campaign_results\", \"location\": \"C:/data/marketing_campaigns.xlsx\", \"sheet_name\": \"Results\", \"label\": \"Marketing Campaign Results\", \"description\": \"Contains results of various marketing campaigns, including success metrics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"campaign_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each marketing campaign.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_3\", \"column_name\": \"success\", \"column_type\": \"xsd:boolean\", \"column_description\": \"Indicates whether the campaign was successful.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the marketing workflow.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_demographics.csv\", \"type\": \"csv\"}, \"output\": \"customer_demographics\", \"comment\": \"Loading customer demographics data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing_campaigns.xlsx\", \"type\": \"xlsx\"}, \"output\": \"marketing_campaign_results\", \"comment\": \"Loading marketing campaign results from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"marketing_campaign_results\", \"query\": \"success == True\"}, \"output\": \"successful_campaigns\", \"comment\": \"Filtering campaign results to include only successful campaigns.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"successful_campaigns\", \"customer_demographics\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining successful campaign results with customer demographics based on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"age_group\", \"aggregations\": [{\"column\": \"campaign_id\", \"name\": \"total_successful_campaigns\", \"function\": \"count\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data to calculate the total number of successful campaigns per age group.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_successful_campaigns\", \"order\": \"desc\"}, \"output\": \"sorted_aggregated_data\", \"comment\": \"Sorting the aggregated data by the total number of successful campaigns in descending order.\"}]"}]}
{"id":"1e6cfc3c-02ee-4018-9bc6-4eb051087564","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the patient data table from the CSV file located at C:/data/patient_data.csv.\", \"Load the medical records table from the Excel file located at C:/data/medical_records.xlsx.\", \"Filter the patient data to include only patients above the age of 50.\", \"Join the filtered patient data with the medical records on the patient_id column.\", \"Calculate the Body Mass Index (BMI) for each patient using the formula: weight / (height/100)^2.\", \"Filter the joined table to include only patients with a BMI greater than 25.\", \"Aggregate the data by gender to find the average BMI for each gender.\", \"Sort the aggregated data by average BMI in descending order.\", \"Apply a log transformation to the average BMI column to stabilize variance.\", \"Export the final table to a CSV file located at C:/output/average_bmi_by_gender.csv.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"patient_data\", \"location\": \"C:/data/patient_data.csv\", \"sheet_name\": null, \"label\": \"Patient Data\", \"description\": \"Contains demographic information of patients including age, gender, height, and weight.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"column_2_id\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the patient.\"}, {\"column\": \"column_3_id\", \"column_name\": \"gender\", \"column_type\": \"xsd:string\", \"column_description\": \"Gender of the patient.\"}, {\"column\": \"column_4_id\", \"column_name\": \"height\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Height of the patient in centimeters.\"}, {\"column\": \"column_5_id\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the patient in kilograms.\"}]}, \"table_id2\": {\"name\": \"medical_records\", \"location\": \"C:/data/medical_records.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Medical Records\", \"description\": \"Contains medical history and records of patients.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"column_2_id\", \"column_name\": \"medical_history\", \"column_type\": \"xsd:string\", \"column_description\": \"Medical history of the patient.\"}]}, \"table_id3\": {\"name\": \"hospital_staff\", \"location\": \"C:/data/hospital_staff.csv\", \"sheet_name\": null, \"label\": \"Hospital Staff\", \"description\": \"Contains information about hospital staff members.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"staff_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each staff member.\"}, {\"column\": \"column_2_id\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the staff member.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/patient_data.csv\", \"type\": \"csv\"}, \"output\": \"patient_data\", \"comment\": \"Loading patient data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/medical_records.xlsx\", \"type\": \"xlsx\"}, \"output\": \"medical_records\", \"comment\": \"Loading medical records from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"patient_data\", \"query\": \"age > 50\"}, \"output\": \"filtered_patient_data\", \"comment\": \"Filtering patient data to include only patients above the age of 50.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_patient_data\", \"medical_records\"], \"joinOn\": [\"patient_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered patient data with medical records on patient_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"BMI\", \"formula\": \"weight / (height/100)**2\"}, \"output\": \"joined_data_with_bmi\", \"comment\": \"Calculating BMI for each patient.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"joined_data_with_bmi\", \"query\": \"BMI > 25\"}, \"output\": \"overweight_patients\", \"comment\": \"Filtering to include only patients with a BMI greater than 25.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"overweight_patients\", \"groupBy\": \"gender\", \"aggregations\": [{\"column\": \"BMI\", \"name\": \"average_BMI\", \"function\": \"mean\"}]}, \"output\": \"average_bmi_by_gender\", \"comment\": \"Aggregating data by gender to find the average BMI.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"average_bmi_by_gender\", \"sortBy\": \"average_BMI\", \"order\": \"desc\"}, \"output\": \"sorted_average_bmi_by_gender\", \"comment\": \"Sorting the aggregated data by average BMI in descending order.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sorted_average_bmi_by_gender\", \"transformations\": [{\"columns\": [\"average_BMI\"], \"operation\": \"log_transform\"}]}, \"output\": \"transformed_average_bmi_by_gender\", \"comment\": \"Applying a log transformation to the average BMI column.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/output/average_bmi_by_gender.csv\", \"type\": \"csv\"}, \"output\": \"exported_data\", \"comment\": \"Exporting the final table to a CSV file.\"}]"}]}
{"id":"9e5d4100-e02c-49a6-aa37-137940cfd4b3","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\", \"Load the weather data from the Excel file located at C:/data/weather_data.xlsx.\", \"Filter the energy consumption data to include only records from the year 2023.\", \"Join the filtered energy consumption data with the weather data on the date column.\", \"Calculate a new column in the joined table to determine the energy efficiency by dividing energy usage by temperature.\", \"Aggregate the data by month to find the total energy consumption and average temperature.\", \"Sort the aggregated data by total energy consumption in descending order.\", \"Forecast the energy consumption for the next 30 days using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/energy_consumption.csv\", \"sheet_name\": null, \"label\": \"Energy Consumption Data\", \"description\": \"This table contains energy consumption data with columns for date, usage, and other relevant metrics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed on the given date.\"}]}, \"table_id2\": {\"name\": \"weather_data\", \"location\": \"C:/data/weather_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Weather Data\", \"description\": \"This table contains weather data with columns for date, temperature, and other weather metrics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the weather record.\"}, {\"column\": \"col_2\", \"column_name\": \"temperature\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The temperature recorded on the given date.\"}]}, \"table_id3\": {\"name\": \"historical_energy_prices\", \"location\": \"C:/data/historical_energy_prices.csv\", \"sheet_name\": null, \"label\": \"Historical Energy Prices\", \"description\": \"This table contains historical energy prices data, which is not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the price record.\"}, {\"column\": \"col_2\", \"column_name\": \"price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The energy price on the given date.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/energy_consumption.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the energy consumption data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/weather_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"weather_data\", \"comment\": \"Loading the weather data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"`date`.dt.year == 2023\"}, \"output\": \"filtered_energy_consumption\", \"comment\": \"Filtering energy consumption data to include only records from the year 2023.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_energy_consumption\", \"weather_data\"], \"joinOn\": [\"date\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered energy consumption data with weather data on the date column.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"energy_efficiency\", \"formula\": \"`usage` / `temperature`\"}, \"output\": \"data_with_efficiency\", \"comment\": \"Calculating energy efficiency by dividing energy usage by temperature.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"data_with_efficiency\", \"groupBy\": \"date.dt.to_period('M')\", \"aggregations\": [{\"column\": \"usage\", \"name\": \"total_energy_consumption\", \"function\": \"sum\"}, {\"column\": \"temperature\", \"name\": \"average_temperature\", \"function\": \"mean\"}]}, \"output\": \"monthly_aggregated_data\", \"comment\": \"Aggregating data by month to find total energy consumption and average temperature.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"monthly_aggregated_data\", \"sortBy\": \"total_energy_consumption\", \"order\": \"desc\"}, \"output\": \"sorted_aggregated_data\", \"comment\": \"Sorting aggregated data by total energy consumption in descending order.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"energy_consumption\", \"columns\": [\"usage\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"confidenceInterval\": 95}}, \"output\": \"energy_forecast\", \"comment\": \"Forecasting energy consumption for the next 30 days using the Holt-Winters method.\"}]"}]}
{"id":"8159ff38-4318-471d-acb7-353b84b26d95","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'case_details' which contains information about legal cases.\", \"Filter the 'case_details' table to only include cases from the year 2023.\", \"Load the table 'lawyer_info' which contains details about lawyers.\", \"Join the filtered 'case_details' table with the 'lawyer_info' table on the 'lawyer_id' column.\", \"Aggregate the joined table to find the total number of cases handled by each lawyer.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"case_details\", \"location\": \"C:/legal_data/case_details.csv\", \"sheet_name\": null, \"label\": \"Legal Case Details\", \"description\": \"A table containing detailed information about various legal cases, including case ID, lawyer ID, and case date.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"case_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each legal case.\"}, {\"column\": \"col_2\", \"column_name\": \"lawyer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the lawyer handling the case.\"}, {\"column\": \"col_3\", \"column_name\": \"case_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the case was filed.\"}]}, \"table_id2\": {\"name\": \"lawyer_info\", \"location\": \"C:/legal_data/lawyer_info.csv\", \"sheet_name\": null, \"label\": \"Lawyer Information\", \"description\": \"A table containing information about lawyers, including lawyer ID and name.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"lawyer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each lawyer.\"}, {\"column\": \"col_2\", \"column_name\": \"lawyer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the lawyer.\"}]}, \"table_id3\": {\"name\": \"court_schedule\", \"location\": \"C:/legal_data/court_schedule.xlsx\", \"sheet_name\": \"Schedule\", \"label\": \"Court Schedule\", \"description\": \"A table containing the schedule of court hearings, not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"hearing_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each court hearing.\"}, {\"column\": \"col_2\", \"column_name\": \"hearing_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the court hearing.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/legal_data/case_details.csv\", \"type\": \"csv\"}, \"output\": \"case_details\", \"comment\": \"Loading the table containing details of legal cases.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"case_details\", \"query\": \"`case_date`.year == 2023\"}, \"output\": \"filtered_case_details\", \"comment\": \"Filtering cases to include only those from the year 2023.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/legal_data/lawyer_info.csv\", \"type\": \"csv\"}, \"output\": \"lawyer_info\", \"comment\": \"Loading the table containing information about lawyers.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_case_details\", \"lawyer_info\"], \"joinOn\": [\"lawyer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_table\", \"comment\": \"Joining filtered case details with lawyer information on lawyer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_table\", \"groupBy\": \"lawyer_id\", \"aggregations\": [{\"column\": \"case_id\", \"name\": \"total_cases\", \"function\": \"count\"}]}, \"output\": \"lawyer_case_count\", \"comment\": \"Aggregating to find the total number of cases handled by each lawyer.\"}]"}]}
{"id":"4722da5c-fd2a-4adb-84d9-871a95e66340","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the Excel file located at C:/data/sales_data.xlsx.\", \"Filter the sales data to include only records from the year 2023.\", \"Apply a rolling mean transformation with a window size of 7 days to smooth the sales data.\", \"Forecast the next 30 days of sales using the Holt-Winters method with a daily frequency and a seasonal period of 7 days.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.xlsx\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains historical sales data including date, sales amount, and other relevant metrics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the sales record.\"}, {\"column\": \"col_2\", \"column_name\": \"sales_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The total sales amount for the given date.\"}, {\"column\": \"col_3\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"The identifier for the product sold.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"Contains customer information including customer ID, name, and contact details.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"contact_number\", \"column_type\": \"xsd:string\", \"column_description\": \"The contact number of the customer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"}, \"output\": \"sales_data_2023\", \"comment\": \"Filtering sales data to include only records from the year 2023.\"}, {\"action\": \"applyTimeSeriesTransformations\", \"parameters\": {\"table\": \"sales_data_2023\", \"transformations\": [{\"columns\": [\"sales_amount\"], \"operation\": \"rolling_mean\", \"parameters\": {\"window\": 7}}]}, \"output\": \"smoothed_sales_data\", \"comment\": \"Applying a rolling mean transformation with a window size of 7 days to smooth the sales data.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"smoothed_sales_data\", \"columns\": [\"sales_amount\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7}}, \"output\": \"sales_forecast\", \"comment\": \"Forecasting the next 30 days of sales using the Holt-Winters method with a daily frequency and a seasonal period of 7 days.\"}]"}]}
{"id":"49b53404-d4bc-4124-976a-2fae20cf6470","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\", \"Load the customer data from the Excel file located at C:/data/customers.xlsx.\", \"Filter the sales data to include only transactions with a total revenue greater than 1000.\", \"Join the filtered sales data with the customer data on the customer_id column using an inner join.\", \"Aggregate the joined data by region to calculate the total sales revenue.\", \"Sort the aggregated data by total sales in descending order.\", \"Calculate a new column for profit margin by subtracting total cost from total revenue.\", \"Apply a logarithmic transformation to the profit margin column to stabilize variance.\", \"Forecast the future sales revenue for the next 30 days using the Holt-Winters method with a seasonal period of 7 days.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_2023.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains sales transactions including total revenue, total cost, and customer IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer involved in the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"total_revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total revenue generated from the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"total_cost\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total cost incurred for the transaction.\"}, {\"column\": \"col_5\", \"column_name\": \"transaction_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the transaction.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customers.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Data\", \"description\": \"This table contains customer information including customer ID and region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the customer is located.\"}]}, \"table_id3\": {\"name\": \"inventory_data\", \"location\": \"C:/data/inventory.csv\", \"sheet_name\": null, \"label\": \"Inventory Data\", \"description\": \"This table contains inventory details including product IDs and stock levels.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"stock_level\", \"column_type\": \"xsd:integer\", \"column_description\": \"Current stock level of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_2023.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customers.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_data\", \"comment\": \"Loading the customer data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"`total_revenue` > 1000\"}, \"output\": \"filtered_sales\", \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales\", \"customer_data\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"sales_with_customers\", \"comment\": \"Joining filtered sales data with customer data on customer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_with_customers\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_revenue\", \"name\": \"total_sales\", \"function\": \"sum\"}]}, \"output\": \"regional_sales\", \"comment\": \"Aggregating sales data by region to calculate total sales revenue.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"regional_sales\", \"sortBy\": \"total_sales\", \"order\": \"desc\"}, \"output\": \"sorted_regional_sales\", \"comment\": \"Sorting aggregated data by total sales in descending order.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sales_with_customers\", \"columnName\": \"profit_margin\", \"formula\": \"`total_revenue` - `total_cost`\"}, \"output\": \"sales_with_profit\", \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sales_with_profit\", \"transformations\": [{\"columns\": [\"profit_margin\"], \"operation\": \"log_transform\"}]}, \"output\": \"transformed_sales\", \"comment\": \"Applying a logarithmic transformation to the profit margin column.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"transformed_sales\", \"columns\": [\"total_revenue\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"transaction_date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7}}, \"output\": \"sales_forecast\", \"comment\": \"Forecasting future sales revenue for the next 30 days using Holt-Winters method.\"}]"}]}
{"id":"65070387-085b-492d-86d5-ee30486f81c9","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the legal cases table from the CSV file.\", \"Filter the cases to include only those from the year 2023.\", \"Join the filtered cases with the lawyer information table on lawyer_id.\", \"Aggregate the joined data to count the number of cases per lawyer.\", \"Sort the aggregated data by the number of cases in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"legal_cases\", \"location\": \"C:/data/legal_cases.csv\", \"sheet_name\": null, \"label\": \"Legal Cases\", \"description\": \"A table containing information about legal cases, including case ID, lawyer ID, and case year.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"case_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each legal case.\"}, {\"column\": \"column_2_id\", \"column_name\": \"lawyer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the lawyer handling the case.\"}, {\"column\": \"column_3_id\", \"column_name\": \"case_year\", \"column_type\": \"xsd:integer\", \"column_description\": \"The year the case was filed.\"}]}, \"table_id2\": {\"name\": \"lawyer_info\", \"location\": \"C:/data/lawyer_info.csv\", \"sheet_name\": null, \"label\": \"Lawyer Information\", \"description\": \"A table containing information about lawyers, including lawyer ID and name.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"lawyer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each lawyer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"lawyer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the lawyer.\"}]}, \"table_id3\": {\"name\": \"court_schedule\", \"location\": \"C:/data/court_schedule.xlsx\", \"sheet_name\": \"Schedule\", \"label\": \"Court Schedule\", \"description\": \"A table containing the schedule of court hearings.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"hearing_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each court hearing.\"}, {\"column\": \"column_2_id\", \"column_name\": \"hearing_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the court hearing.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/legal_cases.csv\", \"type\": \"csv\"}, \"output\": \"legal_cases\", \"comment\": \"Loading the legal cases dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"legal_cases\", \"query\": \"case_year == 2023\"}, \"output\": \"cases_2023\", \"comment\": \"Filtering cases to include only those from the year 2023.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/lawyer_info.csv\", \"type\": \"csv\"}, \"output\": \"lawyer_info\", \"comment\": \"Loading the lawyer information dataset from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"cases_2023\", \"lawyer_info\"], \"joinOn\": [\"lawyer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_cases_lawyers\", \"comment\": \"Joining filtered cases with lawyer information on lawyer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_cases_lawyers\", \"groupBy\": \"lawyer_id\", \"aggregations\": [{\"column\": \"case_id\", \"name\": \"case_count\", \"function\": \"count\"}]}, \"output\": \"cases_per_lawyer\", \"comment\": \"Aggregating data to count the number of cases per lawyer.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"cases_per_lawyer\", \"sortBy\": \"case_count\", \"order\": \"desc\"}, \"output\": \"sorted_cases_per_lawyer\", \"comment\": \"Sorting the aggregated data by the number of cases in descending order.\"}]"}]}
{"id":"17f1d552-ce6f-4ea0-9baf-975334908a28","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\", \"Filter the data to include only records from the year 2023.\", \"Calculate the daily average energy consumption.\", \"Identify peak consumption days by filtering days with energy usage above 1000 kWh.\", \"Join the peak consumption data with weather data to analyze the impact of weather on energy usage.\", \"Aggregate the joined data to find the average temperature on peak consumption days.\", \"Sort the aggregated data by average temperature in descending order.\", \"Forecast future energy consumption for the next 30 days using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/energy_consumption.csv\", \"sheet_name\": null, \"label\": \"Energy Consumption Data\", \"description\": \"This table contains daily energy consumption data with timestamps and usage in kWh.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:dateTime\", \"column_description\": \"The date and time of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kWh.\"}]}, \"table_2\": {\"name\": \"weather_data\", \"location\": \"C:/data/weather_data.csv\", \"sheet_name\": null, \"label\": \"Weather Data\", \"description\": \"This table contains daily weather data including temperature and humidity.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the weather record.\"}, {\"column\": \"col_2\", \"column_name\": \"temperature\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The average temperature of the day in Celsius.\"}, {\"column\": \"col_3\", \"column_name\": \"humidity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The average humidity of the day in percentage.\"}]}, \"table_3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"This table contains data not relevant to the energy management workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column for this workflow.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/energy_consumption.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"`timestamp`.dt.year == 2023\"}, \"output\": \"energy_2023\", \"comment\": \"Filtering the energy consumption data to include only records from the year 2023.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"energy_2023\", \"groupBy\": \"timestamp.dt.date\", \"aggregations\": [{\"column\": \"usage\", \"name\": \"daily_avg_usage\", \"function\": \"mean\"}]}, \"output\": \"daily_avg_energy\", \"comment\": \"Calculating the daily average energy consumption.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"daily_avg_energy\", \"query\": \"daily_avg_usage > 1000\"}, \"output\": \"peak_days\", \"comment\": \"Identifying peak consumption days with energy usage above 1000 kWh.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/weather_data.csv\", \"type\": \"csv\"}, \"output\": \"weather_data\", \"comment\": \"Loading the weather data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"peak_days\", \"weather_data\"], \"joinOn\": [\"timestamp.dt.date\", \"date\"], \"joinType\": \"inner\"}, \"output\": \"peak_weather\", \"comment\": \"Joining peak consumption data with weather data to analyze the impact of weather on energy usage.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"peak_weather\", \"groupBy\": null, \"aggregations\": [{\"column\": \"temperature\", \"name\": \"avg_temp_on_peak_days\", \"function\": \"mean\"}]}, \"output\": \"avg_temp_peak_days\", \"comment\": \"Aggregating the joined data to find the average temperature on peak consumption days.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"avg_temp_peak_days\", \"sortBy\": \"avg_temp_on_peak_days\", \"order\": \"desc\"}, \"output\": \"sorted_avg_temp\", \"comment\": \"Sorting the aggregated data by average temperature in descending order.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"energy_2023\", \"columns\": [\"usage\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"timestamp\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"confidenceInterval\": 95}}, \"output\": \"energy_forecast\", \"comment\": \"Forecasting future energy consumption for the next 30 days using the Holt-Winters method.\"}]"}]}
{"id":"2e190cb3-5d8f-476f-b86d-382b7d9b527e","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the patient records table from the provided CSV file.\", \"Filter the patient records to include only those with a BMI greater than 25.\", \"Aggregate the filtered data to find the average age of patients with a BMI greater than 25.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"patient_records\", \"location\": \"C:/data/healthcare/patient_records.csv\", \"sheet_name\": null, \"label\": \"Patient Records\", \"description\": \"A table containing patient information including age, BMI, and other health metrics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"Patient_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_2\", \"column_name\": \"Age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the patient.\"}, {\"column\": \"col_3\", \"column_name\": \"BMI\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Body Mass Index of the patient.\"}]}, \"table_id2\": {\"name\": \"hospital_visits\", \"location\": \"C:/data/healthcare/hospital_visits.xlsx\", \"sheet_name\": \"Visits\", \"label\": \"Hospital Visits\", \"description\": \"A table detailing hospital visit records including patient ID and visit dates.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"Visit_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each hospital visit.\"}, {\"column\": \"col_2\", \"column_name\": \"Patient_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_3\", \"column_name\": \"Visit_Date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the hospital visit.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/healthcare/patient_records.csv\", \"type\": \"csv\"}, \"output\": \"patient_records\", \"comment\": \"Loading the patient records dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"patient_records\", \"query\": \"BMI > 25\"}, \"output\": \"filtered_patients\", \"comment\": \"Filtering patient records to include only those with a BMI greater than 25.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_patients\", \"groupBy\": null, \"aggregations\": [{\"column\": \"Age\", \"name\": \"average_age\", \"function\": \"mean\"}]}, \"output\": \"average_age_bmi_over_25\", \"comment\": \"Calculating the average age of patients with a BMI greater than 25.\"}]"}]}
{"id":"386655a6-3573-4da3-a53a-0f549b396a4d","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the financial transactions table from the CSV file.\", \"Filter the transactions to include only those with an amount greater than 1000.\", \"Join the filtered transactions with the customer details table on the customer ID.\", \"Aggregate the joined data to calculate the total transaction amount per customer.\"]\n\n                ### Context:\n                {\"table_1\": {\"name\": \"financial_transactions\", \"location\": \"C:/data/finance/transactions.csv\", \"sheet_name\": null, \"label\": \"Financial Transactions\", \"description\": \"A table containing financial transactions with columns for transaction ID, customer ID, amount, and date.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer involved in the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of money involved in the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the transaction occurred.\"}]}, \"table_2\": {\"name\": \"customer_details\", \"location\": \"C:/data/finance/customers.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Details\", \"description\": \"A table containing customer details with columns for customer ID, name, and contact information.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"contact_info\", \"column_type\": \"xsd:string\", \"column_description\": \"Contact information for the customer.\"}]}, \"table_3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/finance/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"A table with irrelevant data not used in the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/finance/transactions.csv\", \"type\": \"csv\"}, \"output\": \"financial_transactions\", \"comment\": \"Loading the financial transactions table from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"financial_transactions\", \"query\": \"amount > 1000\"}, \"output\": \"filtered_transactions\", \"comment\": \"Filtering transactions to include only those with an amount greater than 1000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/finance/customers.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_details\", \"comment\": \"Loading the customer details table from an Excel file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_transactions\", \"customer_details\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered transactions with customer details on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"amount\", \"name\": \"total_transaction_amount\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating joined data to calculate the total transaction amount per customer.\"}]"}]}
{"id":"0177b5e1-669a-4124-b2c8-bb12b6a809e4","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'court_cases' table from the CSV file located at C:/data/legal/court_cases.csv.\", \"Filter the 'court_cases' table to include only cases where the verdict is 'Guilty'.\", \"Join the filtered 'court_cases' table with the 'lawyers' table on the 'lawyer_id' column.\", \"Aggregate the joined table to find the total number of 'Guilty' verdicts per lawyer.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"court_cases\", \"location\": \"C:/data/legal/court_cases.csv\", \"sheet_name\": null, \"label\": \"Court Cases\", \"description\": \"A table containing details of various court cases, including case ID, verdict, and associated lawyer ID.\", \"columns\": [{\"column\": \"case_id\", \"column_name\": \"CASE_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each court case.\"}, {\"column\": \"verdict\", \"column_name\": \"VERDICT\", \"column_type\": \"xsd:string\", \"column_description\": \"The verdict of the court case, e.g., 'Guilty' or 'Not Guilty'.\"}, {\"column\": \"lawyer_id\", \"column_name\": \"LAWYER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the lawyer associated with the case.\"}]}, \"table_id2\": {\"name\": \"lawyers\", \"location\": \"C:/data/legal/lawyers.csv\", \"sheet_name\": null, \"label\": \"Lawyers\", \"description\": \"A table containing information about lawyers, including their ID and name.\", \"columns\": [{\"column\": \"lawyer_id\", \"column_name\": \"LAWYER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each lawyer.\"}, {\"column\": \"lawyer_name\", \"column_name\": \"LAWYER_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the lawyer.\"}]}, \"table_id3\": {\"name\": \"legal_firms\", \"location\": \"C:/data/legal/legal_firms.csv\", \"sheet_name\": null, \"label\": \"Legal Firms\", \"description\": \"A table containing information about legal firms, including firm ID and name.\", \"columns\": [{\"column\": \"firm_id\", \"column_name\": \"FIRM_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each legal firm.\"}, {\"column\": \"firm_name\", \"column_name\": \"FIRM_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the legal firm.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/legal/court_cases.csv\", \"type\": \"csv\"}, \"output\": \"court_cases\", \"comment\": \"Loading the court cases dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"court_cases\", \"query\": \"VERDICT == 'Guilty'\"}, \"output\": \"guilty_cases\", \"comment\": \"Filtering court cases to include only those with a 'Guilty' verdict.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/legal/lawyers.csv\", \"type\": \"csv\"}, \"output\": \"lawyers\", \"comment\": \"Loading the lawyers dataset from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"guilty_cases\", \"lawyers\"], \"joinOn\": [\"LAWYER_ID\"], \"joinType\": \"inner\"}, \"output\": \"guilty_cases_with_lawyers\", \"comment\": \"Joining the filtered court cases with the lawyers table on lawyer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"guilty_cases_with_lawyers\", \"groupBy\": \"LAWYER_NAME\", \"aggregations\": [{\"column\": \"CASE_ID\", \"name\": \"total_guilty_verdicts\", \"function\": \"count\"}]}, \"output\": \"guilty_verdicts_per_lawyer\", \"comment\": \"Aggregating the joined table to find the total number of 'Guilty' verdicts per lawyer.\"}]"}]}
{"id":"f96591bd-96c4-4994-8118-dde7a802a7fe","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\", \"Load the product information from the Excel file located at C:/data/product_info.xlsx.\", \"Filter the sales data to include only transactions from the year 2023.\", \"Join the filtered sales data with the product information on the product_id column.\", \"Aggregate the joined data to calculate the total sales per product category.\", \"Sort the aggregated data by total sales in descending order.\", \"Apply a log transformation to the total sales column to stabilize variance.\", \"Forecast future sales for the next 30 days using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains sales transactions including product IDs, sales amounts, and transaction dates.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"column_2_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the product sold.\"}, {\"column\": \"column_3_id\", \"column_name\": \"sales_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of sales in dollars.\"}, {\"column\": \"column_4_id\", \"column_name\": \"transaction_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the transaction.\"}]}, \"table_id2\": {\"name\": \"product_info\", \"location\": \"C:/data/product_info.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Product Information\", \"description\": \"This table contains information about products including product IDs and categories.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"The category to which the product belongs.\"}]}, \"table_id3\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"This table contains customer information including customer IDs and demographics.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_info.xlsx\", \"type\": \"xlsx\"}, \"output\": \"product_info\", \"comment\": \"Loading the product information from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"`transaction_date`.dt.year == 2023\"}, \"output\": \"filtered_sales_data\", \"comment\": \"Filtering sales data to include only transactions from the year 2023.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales_data\", \"product_info\"], \"joinOn\": [\"product_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered sales data with product information on product_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"category\", \"aggregations\": [{\"column\": \"sales_amount\", \"name\": \"total_sales\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating the joined data to calculate total sales per product category.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_sales\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting the aggregated data by total sales in descending order.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sorted_data\", \"transformations\": [{\"columns\": [\"total_sales\"], \"operation\": \"log_transform\"}]}, \"output\": \"transformed_data\", \"comment\": \"Applying a log transformation to the total sales column to stabilize variance.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"transformed_data\", \"columns\": [\"total_sales\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"dateColumn\": \"transaction_date\"}}, \"output\": \"forecast_results\", \"comment\": \"Forecasting future sales for the next 30 days using the Holt-Winters method.\"}]"}]}
{"id":"5bd257a7-e4d5-40af-94c4-d19be429c0ad","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the patient records table from the hospital database.\", \"Load the medication prescriptions table from the hospital database.\", \"Filter the patient records to include only those admitted in the last 30 days.\", \"Join the filtered patient records with the medication prescriptions on patient ID.\", \"Calculate the total medication cost for each patient.\", \"Aggregate the data to find the average medication cost per department.\", \"Sort the aggregated data by average medication cost in descending order.\", \"Identify any outliers in the medication costs using a z-score threshold.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"patient_records\", \"location\": \"C:/hospital_data/patient_records.csv\", \"sheet_name\": null, \"label\": \"Patient Records\", \"description\": \"Contains detailed records of patients including admission dates and departments.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_2\", \"column_name\": \"admission_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date when the patient was admitted.\"}, {\"column\": \"col_3\", \"column_name\": \"department\", \"column_type\": \"xsd:string\", \"column_description\": \"Department where the patient is admitted.\"}]}, \"table_id2\": {\"name\": \"medication_prescriptions\", \"location\": \"C:/hospital_data/medication_prescriptions.csv\", \"sheet_name\": null, \"label\": \"Medication Prescriptions\", \"description\": \"Contains records of medication prescribed to patients including costs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_2\", \"column_name\": \"medication_cost\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Cost of the prescribed medication.\"}]}, \"table_id3\": {\"name\": \"staff_records\", \"location\": \"C:/hospital_data/staff_records.csv\", \"sheet_name\": null, \"label\": \"Staff Records\", \"description\": \"Contains records of hospital staff including roles and departments.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"staff_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each staff member.\"}, {\"column\": \"col_2\", \"column_name\": \"role\", \"column_type\": \"xsd:string\", \"column_description\": \"Role of the staff member in the hospital.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/hospital_data/patient_records.csv\", \"type\": \"csv\"}, \"output\": \"patient_records\", \"comment\": \"Loading patient records from the hospital database.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/hospital_data/medication_prescriptions.csv\", \"type\": \"csv\"}, \"output\": \"medication_prescriptions\", \"comment\": \"Loading medication prescriptions from the hospital database.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"patient_records\", \"query\": \"`admission_date` >= '2023-09-01'\"}, \"output\": \"recent_patients\", \"comment\": \"Filtering patient records to include only those admitted in the last 30 days.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"recent_patients\", \"medication_prescriptions\"], \"joinOn\": [\"patient_id\"], \"joinType\": \"inner\"}, \"output\": \"patients_with_medications\", \"comment\": \"Joining filtered patient records with medication prescriptions on patient ID.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"patients_with_medications\", \"columnName\": \"total_medication_cost\", \"formula\": \"`medication_cost`\"}, \"output\": \"patients_with_total_cost\", \"comment\": \"Calculating the total medication cost for each patient.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"patients_with_total_cost\", \"groupBy\": \"department\", \"aggregations\": [{\"column\": \"total_medication_cost\", \"name\": \"average_medication_cost\", \"function\": \"mean\"}]}, \"output\": \"average_cost_per_department\", \"comment\": \"Aggregating data to find the average medication cost per department.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"average_cost_per_department\", \"sortBy\": \"average_medication_cost\", \"order\": \"desc\"}, \"output\": \"sorted_average_cost\", \"comment\": \"Sorting the aggregated data by average medication cost in descending order.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sorted_average_cost\", \"transformations\": [{\"columns\": [\"average_medication_cost\"], \"operation\": \"normalize\"}]}, \"output\": \"outliers_identified\", \"comment\": \"Identifying any outliers in the medication costs using a z-score threshold.\"}]"}]}
{"id":"f3292fef-35b7-4037-b730-2ff3aec00dcd","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the financial transactions table from the CSV file.\", \"Filter the transactions to include only those with an amount greater than 1000.\", \"Calculate the total amount for each category.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"financial_transactions\", \"location\": \"C:/data/finance/transactions.csv\", \"sheet_name\": null, \"label\": \"Financial Transactions\", \"description\": \"A table containing all financial transactions including date, amount, and category.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of money involved in the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"The category of the transaction, such as groceries, utilities, etc.\"}]}, \"table_id2\": {\"name\": \"budget_plan\", \"location\": \"C:/data/finance/budget_plan.xlsx\", \"sheet_name\": \"Plan\", \"label\": \"Budget Plan\", \"description\": \"A table outlining the planned budget for different categories.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"The category for which the budget is planned.\"}, {\"column\": \"col_2\", \"column_name\": \"planned_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The planned budget amount for the category.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/finance/transactions.csv\", \"type\": \"csv\"}, \"output\": \"financial_transactions\", \"comment\": \"Loading the financial transactions dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"financial_transactions\", \"query\": \"`amount` > 1000\"}, \"output\": \"filtered_transactions\", \"comment\": \"Filtering transactions to include only those with an amount greater than 1000.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_transactions\", \"groupBy\": \"category\", \"aggregations\": [{\"column\": \"amount\", \"name\": \"total_amount\", \"function\": \"sum\"}]}, \"output\": \"category_totals\", \"comment\": \"Calculating the total amount for each category.\"}]"}]}
{"id":"7a630c2d-cb72-4981-a6d7-b69c7f388702","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the production schedule table from the CSV file.\", \"Filter the production schedule to include only high-priority orders.\", \"Join the filtered production schedule with the inventory table to check stock availability.\", \"Aggregate the joined data to calculate the total quantity needed for each product.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.csv\", \"sheet_name\": null, \"label\": \"Production Schedule\", \"description\": \"This table contains the production schedule with order details and priority levels.\", \"columns\": [{\"column\": \"order_id\", \"column_name\": \"ORDER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each order.\"}, {\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the product to be manufactured.\"}, {\"column\": \"priority\", \"column_name\": \"PRIORITY\", \"column_type\": \"xsd:string\", \"column_description\": \"Priority level of the order (e.g., high, medium, low).\"}, {\"column\": \"quantity\", \"column_name\": \"QUANTITY\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Quantity of the product to be manufactured.\"}]}, \"table_id2\": {\"name\": \"inventory\", \"location\": \"C:/data/manufacturing/inventory.csv\", \"sheet_name\": null, \"label\": \"Inventory\", \"description\": \"This table contains current stock levels for each product.\", \"columns\": [{\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the product in stock.\"}, {\"column\": \"stock_level\", \"column_name\": \"STOCK_LEVEL\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Current stock level of the product.\"}]}, \"table_id3\": {\"name\": \"employee_records\", \"location\": \"C:/data/manufacturing/employee_records.csv\", \"sheet_name\": null, \"label\": \"Employee Records\", \"description\": \"This table contains records of employees working in the manufacturing plant.\", \"columns\": [{\"column\": \"employee_id\", \"column_name\": \"EMPLOYEE_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"name\", \"column_name\": \"NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the employee.\"}, {\"column\": \"department\", \"column_name\": \"DEPARTMENT\", \"column_type\": \"xsd:string\", \"column_description\": \"Department where the employee works.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.csv\", \"type\": \"csv\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the production schedule from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"production_schedule\", \"query\": \"PRIORITY == 'high'\"}, \"output\": \"high_priority_orders\", \"comment\": \"Filtering the production schedule to include only high-priority orders.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/inventory.csv\", \"type\": \"csv\"}, \"output\": \"inventory\", \"comment\": \"Loading the inventory data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"high_priority_orders\", \"inventory\"], \"joinOn\": [\"PRODUCT_ID\"], \"joinType\": \"inner\"}, \"output\": \"orders_with_stock\", \"comment\": \"Joining high-priority orders with inventory to check stock availability.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"orders_with_stock\", \"groupBy\": \"PRODUCT_ID\", \"aggregations\": [{\"column\": \"QUANTITY\", \"name\": \"total_quantity_needed\", \"function\": \"sum\"}]}, \"output\": \"total_quantity_per_product\", \"comment\": \"Aggregating the data to calculate the total quantity needed for each product.\"}]"}]}
{"id":"f9f91ac6-360f-4158-8a9e-3511fa049651","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'raw_materials' which contains information about the available raw materials.\", \"Load the table 'production_schedule' which contains the planned production activities.\", \"Filter the 'raw_materials' table to include only materials with a quantity greater than 100.\", \"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\", \"Calculate a new column 'material_cost' in the joined table by multiplying 'quantity' by 'unit_price'.\", \"Aggregate the joined table to calculate the total cost of materials for each production activity.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"raw_materials\", \"location\": \"C:/data/manufacturing/raw_materials.csv\", \"sheet_name\": null, \"label\": \"Raw Materials Inventory\", \"description\": \"This table contains information about the raw materials available for manufacturing, including their quantities and unit prices.\", \"columns\": [{\"column\": \"material_id\", \"column_name\": \"MATERIAL_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each raw material.\"}, {\"column\": \"quantity\", \"column_name\": \"QUANTITY\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The quantity of the raw material available.\"}, {\"column\": \"unit_price\", \"column_name\": \"UNIT_PRICE\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The price per unit of the raw material.\"}]}, \"table_id2\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.xlsx\", \"sheet_name\": \"Schedule\", \"label\": \"Production Schedule\", \"description\": \"This table contains the schedule of production activities, including the materials required for each activity.\", \"columns\": [{\"column\": \"activity_id\", \"column_name\": \"ACTIVITY_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each production activity.\"}, {\"column\": \"material_id\", \"column_name\": \"MATERIAL_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the raw material required for the activity.\"}, {\"column\": \"activity_date\", \"column_name\": \"ACTIVITY_DATE\", \"column_type\": \"xsd:date\", \"column_description\": \"The date on which the production activity is scheduled.\"}]}, \"table_id3\": {\"name\": \"employee_records\", \"location\": \"C:/data/manufacturing/employee_records.csv\", \"sheet_name\": null, \"label\": \"Employee Records\", \"description\": \"This table contains records of employees, including their roles and departments.\", \"columns\": [{\"column\": \"employee_id\", \"column_name\": \"EMPLOYEE_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"name\", \"column_name\": \"NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the employee.\"}, {\"column\": \"role\", \"column_name\": \"ROLE\", \"column_type\": \"xsd:string\", \"column_description\": \"The role of the employee within the company.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/raw_materials.csv\", \"type\": \"csv\"}, \"output\": \"raw_materials\", \"comment\": \"Loading the raw materials inventory from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.xlsx\", \"type\": \"xlsx\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the production schedule from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"raw_materials\", \"query\": \"`QUANTITY` > 100\"}, \"output\": \"filtered_raw_materials\", \"comment\": \"Filtering raw materials to include only those with a quantity greater than 100.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_raw_materials\", \"production_schedule\"], \"joinOn\": [\"MATERIAL_ID\"], \"joinType\": \"inner\"}, \"output\": \"joined_table\", \"comment\": \"Joining filtered raw materials with the production schedule on material_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_table\", \"columnName\": \"material_cost\", \"formula\": \"`QUANTITY` * `UNIT_PRICE`\"}, \"output\": \"joined_table_with_cost\", \"comment\": \"Calculating material cost by multiplying quantity by unit price.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_table_with_cost\", \"groupBy\": \"ACTIVITY_ID\", \"aggregations\": [{\"column\": \"material_cost\", \"name\": \"total_material_cost\", \"function\": \"sum\"}]}, \"output\": \"activity_material_costs\", \"comment\": \"Aggregating to calculate the total cost of materials for each production activity.\"}]"}]}
{"id":"7102d152-3eae-4591-a0cf-de8faf683046","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer demographics table from the CSV file.\", \"Filter the customers who are aged between 25 and 40.\", \"Aggregate the filtered data to find the average income by city.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_demographics\", \"location\": \"C:/data/marketing/customer_demographics.csv\", \"sheet_name\": null, \"label\": \"Customer Demographics\", \"description\": \"This table contains information about customer demographics including age, income, and city.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"income\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Annual income of the customer.\"}, {\"column\": \"col_4\", \"column_name\": \"city\", \"column_type\": \"xsd:string\", \"column_description\": \"City where the customer resides.\"}]}, \"table_id2\": {\"name\": \"product_sales\", \"location\": \"C:/data/marketing/product_sales.csv\", \"sheet_name\": null, \"label\": \"Product Sales\", \"description\": \"This table contains sales data for different products.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"sales_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total sales amount for the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing/customer_demographics.csv\", \"type\": \"csv\"}, \"output\": \"customer_demographics\", \"comment\": \"Loading the customer demographics table from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_demographics\", \"query\": \"`age` >= 25 and `age` <= 40\"}, \"output\": \"filtered_customers\", \"comment\": \"Filtering customers who are aged between 25 and 40.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_customers\", \"groupBy\": \"city\", \"aggregations\": [{\"column\": \"income\", \"name\": \"average_income\", \"function\": \"mean\"}]}, \"output\": \"average_income_by_city\", \"comment\": \"Aggregating the filtered data to find the average income by city.\"}]"}]}
{"id":"644fe062-1933-48e3-868b-22c74f2c2db5","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\", \"Load the table 'product_data' which contains details about products and their associated suppliers.\", \"Filter the supplier data to include only those with a sustainability rating above 80.\", \"Join the filtered supplier data with the product data on the supplier ID.\", \"Aggregate the joined data to calculate the total number of products supplied by each sustainable supplier.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Information\", \"description\": \"Contains information about suppliers including their sustainability ratings.\", \"columns\": [{\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"sustainability_rating\", \"column_name\": \"SUSTAINABILITY_RATING\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Rating of the supplier's sustainability practices.\"}, {\"column\": \"supplier_name\", \"column_name\": \"SUPPLIER_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the supplier.\"}]}, \"table_id2\": {\"name\": \"product_data\", \"location\": \"C:/data/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Information\", \"description\": \"Contains details about products and their associated suppliers.\", \"columns\": [{\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier of the product.\"}, {\"column\": \"product_name\", \"column_name\": \"PRODUCT_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}]}, \"table_id3\": {\"name\": \"logistics_data\", \"location\": \"C:/data/logistics_data.csv\", \"sheet_name\": null, \"label\": \"Logistics Information\", \"description\": \"Contains logistics data including transportation modes and emissions.\", \"columns\": [{\"column\": \"logistics_id\", \"column_name\": \"LOGISTICS_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each logistics entry.\"}, {\"column\": \"emissions\", \"column_name\": \"EMISSIONS\", \"column_type\": \"xsd:decimal\", \"column_description\": \"CO2 emissions associated with the logistics entry.\"}, {\"column\": \"transport_mode\", \"column_name\": \"TRANSPORT_MODE\", \"column_type\": \"xsd:string\", \"column_description\": \"Mode of transportation used.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading supplier data containing sustainability ratings.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_data.csv\", \"type\": \"csv\"}, \"output\": \"product_data\", \"comment\": \"Loading product data with associated suppliers.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"SUSTAINABILITY_RATING > 80\"}, \"output\": \"sustainable_suppliers\", \"comment\": \"Filtering suppliers to include only those with a sustainability rating above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"sustainable_suppliers\", \"product_data\"], \"joinOn\": [\"SUPPLIER_ID\"], \"joinType\": \"inner\"}, \"output\": \"sustainable_supplier_products\", \"comment\": \"Joining filtered supplier data with product data on supplier ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sustainable_supplier_products\", \"groupBy\": \"SUPPLIER_ID\", \"aggregations\": [{\"column\": \"PRODUCT_ID\", \"name\": \"total_products\", \"function\": \"count\"}]}, \"output\": \"sustainable_supplier_aggregation\", \"comment\": \"Aggregating data to calculate the total number of products supplied by each sustainable supplier.\"}]"}]}
{"id":"f9f44a31-f838-4354-a22c-4cdc457f8d61","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'supply_chain_data' table from the CSV file.\", \"Load the 'emissions_data' table from the Excel file.\", \"Filter the 'supply_chain_data' to include only records from 2023.\", \"Join 'supply_chain_data' with 'emissions_data' on the 'supplier_id' column.\", \"Calculate the total emissions for each supplier by summing the 'emissions' column.\", \"Sort the resulting table by 'total_emissions' in descending order.\", \"Filter the sorted table to include only the top 10 suppliers with the highest emissions.\", \"Calculate the average emissions per product for these top suppliers.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supply_chain_data\", \"location\": \"C:/data/supply_chain_data.csv\", \"sheet_name\": null, \"label\": \"Supply Chain Data\", \"description\": \"Contains information about suppliers, products, and transactions in the supply chain.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"column_2_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_3_id\", \"column_name\": \"transaction_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the transaction.\"}]}, \"table_id2\": {\"name\": \"emissions_data\", \"location\": \"C:/data/emissions_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Emissions Data\", \"description\": \"Contains emissions data for each supplier.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"column_2_id\", \"column_name\": \"emissions\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total emissions reported by the supplier.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"unrelated_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An unrelated column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supply_chain_data.csv\", \"type\": \"csv\"}, \"output\": \"supply_chain_data\", \"comment\": \"Loading the supply chain data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/emissions_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"emissions_data\", \"comment\": \"Loading the emissions data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supply_chain_data\", \"query\": \"`transaction_date`.dt.year == 2023\"}, \"output\": \"filtered_supply_chain_data\", \"comment\": \"Filtering supply chain data to include only records from 2023.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_supply_chain_data\", \"emissions_data\"], \"joinOn\": [\"supplier_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining supply chain data with emissions data on supplier_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"supplier_id\", \"aggregations\": [{\"column\": \"emissions\", \"name\": \"total_emissions\", \"function\": \"sum\"}]}, \"output\": \"supplier_emissions\", \"comment\": \"Calculating total emissions for each supplier.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"supplier_emissions\", \"sortBy\": \"total_emissions\", \"order\": \"desc\"}, \"output\": \"sorted_emissions\", \"comment\": \"Sorting suppliers by total emissions in descending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sorted_emissions\", \"query\": \"index < 10\"}, \"output\": \"top_suppliers\", \"comment\": \"Filtering to include only the top 10 suppliers with the highest emissions.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"top_suppliers\", \"groupBy\": [], \"aggregations\": [{\"column\": \"total_emissions\", \"name\": \"average_emissions_per_product\", \"function\": \"mean\"}]}, \"output\": \"top_suppliers_with_avg_emissions\", \"comment\": \"Calculating average emissions per product for top suppliers.\"}]"}]}
{"id":"d6aab96f-9f49-431e-b40f-57782c3d1f9c","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the patient records table from the healthcare database.\", \"Filter the patient records to include only those with a diagnosis of diabetes.\", \"Join the filtered patient records with the medication table to get prescribed medications.\", \"Aggregate the data to find the average age of diabetic patients for each medication.\", \"Sort the aggregated data by average age in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"patient_records\", \"location\": \"C:/healthcare_data/patient_records.csv\", \"sheet_name\": null, \"label\": \"Patient Records\", \"description\": \"Contains detailed records of patients including their diagnoses and personal information.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the patient.\"}, {\"column\": \"col_3\", \"column_name\": \"diagnosis\", \"column_type\": \"xsd:string\", \"column_description\": \"Medical diagnosis of the patient.\"}]}, \"table_id2\": {\"name\": \"medication\", \"location\": \"C:/healthcare_data/medication.csv\", \"sheet_name\": null, \"label\": \"Medication Records\", \"description\": \"Contains information about medications prescribed to patients.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_2\", \"column_name\": \"medication_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the prescribed medication.\"}]}, \"table_id3\": {\"name\": \"hospital_visits\", \"location\": \"C:/healthcare_data/hospital_visits.csv\", \"sheet_name\": null, \"label\": \"Hospital Visits\", \"description\": \"Records of hospital visits by patients, including visit dates and reasons.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"visit_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each hospital visit.\"}, {\"column\": \"col_2\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_3\", \"column_name\": \"visit_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the hospital visit.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/healthcare_data/patient_records.csv\", \"type\": \"csv\"}, \"output\": \"patient_records\", \"comment\": \"Loading the patient records dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"patient_records\", \"query\": \"diagnosis == 'diabetes'\"}, \"output\": \"diabetic_patients\", \"comment\": \"Filtering patient records to include only those diagnosed with diabetes.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/healthcare_data/medication.csv\", \"type\": \"csv\"}, \"output\": \"medication\", \"comment\": \"Loading the medication records dataset from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"diabetic_patients\", \"medication\"], \"joinOn\": [\"patient_id\"], \"joinType\": \"inner\"}, \"output\": \"diabetic_patients_medication\", \"comment\": \"Joining diabetic patient records with medication records based on patient_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"diabetic_patients_medication\", \"groupBy\": \"medication_name\", \"aggregations\": [{\"column\": \"age\", \"name\": \"average_age\", \"function\": \"mean\"}]}, \"output\": \"average_age_by_medication\", \"comment\": \"Aggregating data to find the average age of diabetic patients for each medication.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"average_age_by_medication\", \"sortBy\": \"average_age\", \"order\": \"desc\"}, \"output\": \"sorted_average_age_by_medication\", \"comment\": \"Sorting the aggregated data by average age in descending order.\"}]"}]}
{"id":"7c2bb50a-3f4b-4eae-a857-fa8e8c7e300a","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\", \"Filter the sales data to include only records where the total revenue is greater than 1000.\", \"Join the filtered sales data with the customer information table on the customer_id column.\", \"Aggregate the joined data by product category to calculate the total sales revenue for each category.\", \"Sort the aggregated data by total sales revenue in descending order.\", \"Apply a rolling mean transformation with a window size of 7 days to smooth the sales data.\", \"Forecast the next 30 days of sales using the Holt-Winters method with a daily frequency.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"Contains sales records including transaction details and revenue.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer involved in the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"total_revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total revenue generated from the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"product_category\", \"column_type\": \"xsd:string\", \"column_description\": \"Category of the product sold.\"}, {\"column\": \"col_5\", \"column_name\": \"transaction_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date when the transaction occurred.\"}]}, \"table_id2\": {\"name\": \"customer_info\", \"location\": \"C:/data/customer_info.csv\", \"sheet_name\": null, \"label\": \"Customer Information\", \"description\": \"Contains customer details including customer_id and demographic information.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"customer_age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"Contains data that is not relevant to the sales forecasting task.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"total_revenue > 1000\"}, \"output\": \"filtered_sales_data\", \"comment\": \"Filtering sales data to include only records with total revenue greater than 1000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_info.csv\", \"type\": \"csv\"}, \"output\": \"customer_info\", \"comment\": \"Loading customer information from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales_data\", \"customer_info\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_sales_data\", \"comment\": \"Joining filtered sales data with customer information on customer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_sales_data\", \"groupBy\": \"product_category\", \"aggregations\": [{\"column\": \"total_revenue\", \"name\": \"total_sales_revenue\", \"function\": \"sum\"}]}, \"output\": \"aggregated_sales_data\", \"comment\": \"Aggregating sales data by product category to calculate total sales revenue.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_sales_data\", \"sortBy\": \"total_sales_revenue\", \"order\": \"desc\"}, \"output\": \"sorted_sales_data\", \"comment\": \"Sorting aggregated data by total sales revenue in descending order.\"}, {\"action\": \"applyTimeSeriesTransformations\", \"parameters\": {\"table\": \"sales_data\", \"transformations\": [{\"columns\": [\"total_revenue\"], \"operation\": \"rolling_mean\", \"parameters\": {\"window\": 7}}]}, \"output\": \"smoothed_sales_data\", \"comment\": \"Applying a rolling mean transformation with a window size of 7 days to smooth the sales data.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"smoothed_sales_data\", \"columns\": [\"total_revenue\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"transaction_date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"confidenceInterval\": 95}}, \"output\": \"sales_forecast\", \"comment\": \"Forecasting the next 30 days of sales using the Holt-Winters method with a daily frequency.\"}]"}]}
{"id":"28d68f86-cedb-4285-a8e8-5c19d86f0647","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\", \"Load the table 'shipment_data' which includes details about shipments and their carbon emissions.\", \"Filter the 'supplier_data' table to include only suppliers with a sustainability rating above 80.\", \"Join the filtered supplier data with the 'shipment_data' table on the supplier ID.\", \"Calculate the total carbon emissions for each supplier by summing the emissions from all their shipments.\", \"Aggregate the data to find the average carbon emissions per shipment for each supplier.\", \"Sort the aggregated data by average carbon emissions in ascending order.\", \"Identify suppliers with average carbon emissions below a threshold of 50.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Information\", \"description\": \"Contains information about suppliers including their sustainability ratings.\", \"columns\": [{\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"sustainability_rating\", \"column_name\": \"SUSTAINABILITY_RATING\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Sustainability rating of the supplier.\"}, {\"column\": \"supplier_name\", \"column_name\": \"SUPPLIER_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the supplier.\"}]}, \"table_id2\": {\"name\": \"shipment_data\", \"location\": \"C:/data/shipment_data.csv\", \"sheet_name\": null, \"label\": \"Shipment Details\", \"description\": \"Includes details about shipments and their carbon emissions.\", \"columns\": [{\"column\": \"shipment_id\", \"column_name\": \"SHIPMENT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier associated with the shipment.\"}, {\"column\": \"carbon_emissions\", \"column_name\": \"CARBON_EMISSIONS\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Carbon emissions for the shipment.\"}]}, \"table_id3\": {\"name\": \"inventory_data\", \"location\": \"C:/data/inventory_data.csv\", \"sheet_name\": null, \"label\": \"Inventory Information\", \"description\": \"Contains information about inventory levels and stock details.\", \"columns\": [{\"column\": \"inventory_id\", \"column_name\": \"INVENTORY_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each inventory item.\"}, {\"column\": \"stock_level\", \"column_name\": \"STOCK_LEVEL\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Current stock level of the inventory item.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading supplier data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/shipment_data.csv\", \"type\": \"csv\"}, \"output\": \"shipment_data\", \"comment\": \"Loading shipment data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"SUSTAINABILITY_RATING > 80\"}, \"output\": \"filtered_supplier_data\", \"comment\": \"Filtering suppliers with a sustainability rating above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_supplier_data\", \"shipment_data\"], \"joinOn\": [\"SUPPLIER_ID\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered supplier data with shipment data on supplier ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"SUPPLIER_ID\", \"aggregations\": [{\"column\": \"CARBON_EMISSIONS\", \"name\": \"total_emissions\", \"function\": \"sum\"}]}, \"output\": \"total_emissions_per_supplier\", \"comment\": \"Calculating total carbon emissions for each supplier.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"SUPPLIER_ID\", \"aggregations\": [{\"column\": \"CARBON_EMISSIONS\", \"name\": \"average_emissions\", \"function\": \"mean\"}]}, \"output\": \"average_emissions_per_supplier\", \"comment\": \"Calculating average carbon emissions per shipment for each supplier.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"average_emissions_per_supplier\", \"sortBy\": \"average_emissions\", \"order\": \"asc\"}, \"output\": \"sorted_suppliers\", \"comment\": \"Sorting suppliers by average carbon emissions in ascending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sorted_suppliers\", \"query\": \"average_emissions < 50\"}, \"output\": \"sustainable_suppliers\", \"comment\": \"Identifying suppliers with average carbon emissions below 50.\"}]"}]}
{"id":"abb0fe08-1a4f-44e2-b07a-11c05160032d","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'case_details' table from the CSV file located at C:/data/legal/case_details.csv.\", \"Load the 'lawyer_info' table from the Excel file located at C:/data/legal/lawyer_info.xlsx.\", \"Filter the 'case_details' table to include only cases with a status of 'open'.\", \"Join the filtered 'case_details' table with the 'lawyer_info' table on the 'lawyer_id' column.\", \"Aggregate the joined table to count the number of open cases per lawyer.\", \"Sort the aggregated data by the number of open cases in descending order.\", \"Calculate a new column 'case_duration' in the 'case_details' table as the difference between 'end_date' and 'start_date'.\", \"Filter the 'case_details' table to include only cases with a 'case_duration' greater than 30 days.\", \"Join the filtered 'case_details' with the 'lawyer_info' table again to get lawyer details for long-duration cases.\", \"Aggregate the final joined table to find the average 'case_duration' per lawyer.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"case_details\", \"location\": \"C:/data/legal/case_details.csv\", \"sheet_name\": null, \"label\": \"Case Details\", \"description\": \"Contains details of legal cases including start and end dates, status, and associated lawyer.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"case_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each case.\"}, {\"column\": \"col_2\", \"column_name\": \"lawyer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the lawyer handling the case.\"}, {\"column\": \"col_3\", \"column_name\": \"start_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the case started.\"}, {\"column\": \"col_4\", \"column_name\": \"end_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the case ended.\"}, {\"column\": \"col_5\", \"column_name\": \"status\", \"column_type\": \"xsd:string\", \"column_description\": \"Current status of the case.\"}]}, \"table_id2\": {\"name\": \"lawyer_info\", \"location\": \"C:/data/legal/lawyer_info.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Lawyer Information\", \"description\": \"Contains information about lawyers including their ID and name.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"lawyer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each lawyer.\"}, {\"column\": \"col_2\", \"column_name\": \"lawyer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the lawyer.\"}]}, \"table_id3\": {\"name\": \"court_schedule\", \"location\": \"C:/data/legal/court_schedule.xlsx\", \"sheet_name\": \"Schedule\", \"label\": \"Court Schedule\", \"description\": \"Contains the schedule of court hearings, not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"hearing_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each court hearing.\"}, {\"column\": \"col_2\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the court hearing.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/legal/case_details.csv\", \"type\": \"csv\"}, \"output\": \"case_details\", \"comment\": \"Loading the case details dataset from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/legal/lawyer_info.xlsx\", \"type\": \"xlsx\"}, \"output\": \"lawyer_info\", \"comment\": \"Loading the lawyer information dataset from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"case_details\", \"query\": \"status == 'open'\"}, \"output\": \"open_cases\", \"comment\": \"Filtering the case details to include only open cases.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"open_cases\", \"lawyer_info\"], \"joinOn\": [\"lawyer_id\"], \"joinType\": \"inner\"}, \"output\": \"open_cases_with_lawyers\", \"comment\": \"Joining open cases with lawyer information on lawyer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"open_cases_with_lawyers\", \"groupBy\": \"lawyer_id\", \"aggregations\": [{\"column\": \"case_id\", \"name\": \"open_case_count\", \"function\": \"count\"}]}, \"output\": \"open_cases_per_lawyer\", \"comment\": \"Aggregating to count the number of open cases per lawyer.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"open_cases_per_lawyer\", \"sortBy\": \"open_case_count\", \"order\": \"desc\"}, \"output\": \"sorted_open_cases_per_lawyer\", \"comment\": \"Sorting the aggregated data by the number of open cases in descending order.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"case_details\", \"columnName\": \"case_duration\", \"formula\": \"`end_date` - `start_date`\"}, \"output\": \"case_details_with_duration\", \"comment\": \"Calculating the case duration as the difference between end_date and start_date.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"case_details_with_duration\", \"query\": \"case_duration > 30\"}, \"output\": \"long_duration_cases\", \"comment\": \"Filtering cases to include only those with a duration greater than 30 days.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"long_duration_cases\", \"lawyer_info\"], \"joinOn\": [\"lawyer_id\"], \"joinType\": \"inner\"}, \"output\": \"long_duration_cases_with_lawyers\", \"comment\": \"Joining long-duration cases with lawyer information to get lawyer details.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"long_duration_cases_with_lawyers\", \"groupBy\": \"lawyer_id\", \"aggregations\": [{\"column\": \"case_duration\", \"name\": \"average_case_duration\", \"function\": \"mean\"}]}, \"output\": \"average_case_duration_per_lawyer\", \"comment\": \"Aggregating to find the average case duration per lawyer.\"}]"}]}
{"id":"1c495fef-3e3c-40cb-bb4b-a9c1243cac8a","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer demographics table from the CSV file.\", \"Load the marketing campaign results from the Excel file.\", \"Filter the marketing campaign results to include only successful campaigns.\", \"Join the filtered campaign results with customer demographics on customer ID.\", \"Aggregate the joined data to find the average age of customers for each campaign.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_demographics\", \"location\": \"C:/data/customer_demographics.csv\", \"sheet_name\": null, \"label\": \"Customer Demographics\", \"description\": \"Contains demographic information of customers including age, gender, and location.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"gender\", \"column_type\": \"xsd:string\", \"column_description\": \"Gender of the customer.\"}]}, \"table_id2\": {\"name\": \"marketing_campaign_results\", \"location\": \"C:/data/marketing_campaign_results.xlsx\", \"sheet_name\": \"Results\", \"label\": \"Marketing Campaign Results\", \"description\": \"Contains results of various marketing campaigns including success status and customer IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"campaign_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each marketing campaign.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_3\", \"column_name\": \"success\", \"column_type\": \"xsd:boolean\", \"column_description\": \"Indicates whether the campaign was successful.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Data\", \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"data_point\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant data point.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_demographics.csv\", \"type\": \"csv\"}, \"output\": \"customer_demographics\", \"comment\": \"Loading customer demographics data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing_campaign_results.xlsx\", \"type\": \"xlsx\"}, \"output\": \"marketing_campaign_results\", \"comment\": \"Loading marketing campaign results from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"marketing_campaign_results\", \"query\": \"success == True\"}, \"output\": \"successful_campaigns\", \"comment\": \"Filtering marketing campaign results to include only successful campaigns.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"successful_campaigns\", \"customer_demographics\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining successful campaign results with customer demographics on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"campaign_id\", \"aggregations\": [{\"column\": \"age\", \"name\": \"average_age\", \"function\": \"mean\"}]}, \"output\": \"campaign_age_analysis\", \"comment\": \"Aggregating joined data to find the average age of customers for each campaign.\"}]"}]}
{"id":"653ada03-a22e-4a15-9f2f-34f832dba7e1","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\", \"Filter the data to include only records where the energy usage is greater than 100 kWh.\", \"Aggregate the filtered data by date to calculate the total energy usage per day.\", \"Sort the aggregated data by total energy usage in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/energy_consumption.csv\", \"sheet_name\": null, \"label\": \"Energy Consumption Data\", \"description\": \"This table contains records of energy consumption with columns for date, time, and energy usage in kWh.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"time\", \"column_type\": \"xsd:time\", \"column_description\": \"The time of the energy consumption record.\"}, {\"column\": \"col_3\", \"column_name\": \"energy_usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kWh.\"}]}, \"table_id2\": {\"name\": \"weather_data\", \"location\": \"C:/data/weather_data.csv\", \"sheet_name\": null, \"label\": \"Weather Data\", \"description\": \"This table contains weather information with columns for date, temperature, and humidity.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the weather record.\"}, {\"column\": \"col_2\", \"column_name\": \"temperature\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The temperature recorded on the given date.\"}, {\"column\": \"col_3\", \"column_name\": \"humidity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The humidity level recorded on the given date.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/energy_consumption.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"energy_usage > 100\"}, \"output\": \"filtered_energy\", \"comment\": \"Filtering the data to include only records where energy usage is greater than 100 kWh.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_energy\", \"groupBy\": \"date\", \"aggregations\": [{\"column\": \"energy_usage\", \"name\": \"total_energy_usage\", \"function\": \"sum\"}]}, \"output\": \"daily_energy_usage\", \"comment\": \"Aggregating the filtered data by date to calculate the total energy usage per day.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"daily_energy_usage\", \"sortBy\": \"total_energy_usage\", \"order\": \"desc\"}, \"output\": \"sorted_energy_usage\", \"comment\": \"Sorting the aggregated data by total energy usage in descending order.\"}]"}]}
{"id":"11d4ccb1-6ab5-4bdb-8965-64dd681b75c6","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'supplier_data' which contains information about suppliers and their sustainability scores.\", \"Load the table 'product_data' which includes product details and associated supplier IDs.\", \"Filter the 'supplier_data' to include only suppliers with a sustainability score above 80.\", \"Join the filtered supplier data with the 'product_data' on the supplier ID.\", \"Aggregate the joined data to calculate the total number of products supplied by each sustainable supplier.\", \"Load the table 'transport_data' which contains transportation modes and their carbon emissions.\", \"Join the aggregated supplier-product data with 'transport_data' on the transportation mode.\", \"Calculate a new column 'total_emissions' by multiplying the number of products by carbon emissions per product.\", \"Sort the data by 'total_emissions' in ascending order to identify the most sustainable supply chain routes.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Information\", \"description\": \"Contains supplier IDs, names, and sustainability scores.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"col_2\", \"column_name\": \"supplier_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the supplier.\"}, {\"column\": \"col_3\", \"column_name\": \"sustainability_score\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Sustainability score of the supplier.\"}]}, \"table_id2\": {\"name\": \"product_data\", \"location\": \"C:/data/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Details\", \"description\": \"Includes product IDs, names, and associated supplier IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"ID of the supplier providing the product.\"}]}, \"table_id3\": {\"name\": \"transport_data\", \"location\": \"C:/data/transport_data.csv\", \"sheet_name\": null, \"label\": \"Transportation Emissions\", \"description\": \"Contains transportation modes and their carbon emissions per product.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transport_mode\", \"column_type\": \"xsd:string\", \"column_description\": \"Mode of transportation.\"}, {\"column\": \"col_2\", \"column_name\": \"carbon_emissions\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Carbon emissions per product for the transportation mode.\"}]}, \"table_id4\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the supply chain sustainability analysis.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading supplier data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_data.csv\", \"type\": \"csv\"}, \"output\": \"product_data\", \"comment\": \"Loading product data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"sustainability_score > 80\"}, \"output\": \"filtered_supplier_data\", \"comment\": \"Filtering suppliers with a sustainability score above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_supplier_data\", \"product_data\"], \"joinOn\": [\"supplier_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_supplier_product_data\", \"comment\": \"Joining filtered supplier data with product data on supplier ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_supplier_product_data\", \"groupBy\": \"supplier_id\", \"aggregations\": [{\"column\": \"product_id\", \"name\": \"total_products\", \"function\": \"count\"}]}, \"output\": \"aggregated_supplier_product_data\", \"comment\": \"Aggregating data to count total products supplied by each sustainable supplier.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/transport_data.csv\", \"type\": \"csv\"}, \"output\": \"transport_data\", \"comment\": \"Loading transportation data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"aggregated_supplier_product_data\", \"transport_data\"], \"joinOn\": [\"transport_mode\"], \"joinType\": \"inner\"}, \"output\": \"joined_transport_data\", \"comment\": \"Joining aggregated supplier-product data with transport data on transport mode.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_transport_data\", \"columnName\": \"total_emissions\", \"formula\": \"total_products * carbon_emissions\"}, \"output\": \"emissions_data\", \"comment\": \"Calculating total emissions by multiplying total products by carbon emissions per product.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"emissions_data\", \"sortBy\": \"total_emissions\", \"order\": \"asc\"}, \"output\": \"sorted_emissions_data\", \"comment\": \"Sorting data by total emissions in ascending order.\"}]"}]}
{"id":"07cdfc71-2040-4131-8270-f7a425dfe80c","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the energy consumption data from the CSV file.\", \"Filter the data to include only records from the year 2023.\", \"Aggregate the data to calculate the total energy consumption per month.\", \"Join the aggregated data with the weather data to analyze the impact of weather on energy consumption.\", \"Calculate a new column for the average daily energy consumption per month.\", \"Sort the data by total energy consumption in descending order.\", \"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/energy_consumption.csv\", \"sheet_name\": null, \"label\": \"Energy Consumption Data\", \"description\": \"This table contains daily energy consumption data with timestamps and consumption values.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:dateTime\", \"column_description\": \"The date and time of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"consumption\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kilowatt-hours.\"}]}, \"table_id2\": {\"name\": \"weather_data\", \"location\": \"C:/data/weather_data.csv\", \"sheet_name\": null, \"label\": \"Weather Data\", \"description\": \"This table contains daily weather data including temperature and humidity.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the weather record.\"}, {\"column\": \"col_2\", \"column_name\": \"temperature\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The average temperature of the day in Celsius.\"}, {\"column\": \"col_3\", \"column_name\": \"humidity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The average humidity of the day in percentage.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"This table contains unrelated data not used in the workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"unrelated_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An unrelated column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/energy_consumption.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"`timestamp`.dt.year == 2023\"}, \"output\": \"filtered_energy_2023\", \"comment\": \"Filtering the energy consumption data to include only records from the year 2023.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_energy_2023\", \"groupBy\": \"timestamp.dt.month\", \"aggregations\": [{\"column\": \"consumption\", \"name\": \"total_monthly_consumption\", \"function\": \"sum\"}]}, \"output\": \"monthly_energy_consumption\", \"comment\": \"Aggregating the data to calculate the total energy consumption per month.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/weather_data.csv\", \"type\": \"csv\"}, \"output\": \"weather_data\", \"comment\": \"Loading the weather data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"monthly_energy_consumption\", \"weather_data\"], \"joinOn\": [\"timestamp\", \"date\"], \"joinType\": \"inner\"}, \"output\": \"energy_weather_analysis\", \"comment\": \"Joining the aggregated energy data with weather data to analyze the impact of weather on energy consumption.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"energy_weather_analysis\", \"columnName\": \"average_daily_consumption\", \"formula\": \"total_monthly_consumption / 30\"}, \"output\": \"energy_with_avg_daily\", \"comment\": \"Calculating a new column for the average daily energy consumption per month.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"energy_with_avg_daily\", \"sortBy\": \"total_monthly_consumption\", \"order\": \"desc\"}, \"output\": \"sorted_energy_data\", \"comment\": \"Sorting the data by total energy consumption in descending order.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"sorted_energy_data\", \"columns\": [\"total_monthly_consumption\"], \"forecastParameters\": {\"forecastPeriod\": 6, \"frequency\": \"MS\", \"dateColumn\": \"timestamp\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 12, \"confidenceInterval\": 95}}, \"output\": \"energy_forecast\", \"comment\": \"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]"}]}
{"id":"34014733-8c94-46ca-aab2-ea4fcdf9ae63","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the energy consumption data from the CSV file.\", \"Filter the data to include only records from the year 2023.\", \"Calculate the total energy consumption for each month.\", \"Identify peak energy usage days by filtering days with consumption above a certain threshold.\", \"Forecast the energy consumption for the next 30 days using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/energy_consumption_2023.csv\", \"sheet_name\": null, \"label\": \"Energy Consumption Data\", \"description\": \"This table contains daily energy consumption data for the year 2023.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"consumption\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kilowatt-hours.\"}]}, \"table_id2\": {\"name\": \"energy_prices\", \"location\": \"C:/data/energy_prices_2023.csv\", \"sheet_name\": null, \"label\": \"Energy Prices Data\", \"description\": \"This table contains daily energy prices for the year 2023.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy price record.\"}, {\"column\": \"col_2\", \"column_name\": \"price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The price of energy per kilowatt-hour.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/energy_consumption_2023.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the energy consumption data for 2023 from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"`date`.dt.year == 2023\"}, \"output\": \"filtered_energy_2023\", \"comment\": \"Filtering the data to include only records from the year 2023.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_energy_2023\", \"groupBy\": \"date.dt.month\", \"aggregations\": [{\"column\": \"consumption\", \"name\": \"total_monthly_consumption\", \"function\": \"sum\"}]}, \"output\": \"monthly_energy_consumption\", \"comment\": \"Calculating the total energy consumption for each month.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"filtered_energy_2023\", \"query\": \"consumption > 1000\"}, \"output\": \"peak_usage_days\", \"comment\": \"Identifying peak energy usage days by filtering days with consumption above 1000 kWh.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"filtered_energy_2023\", \"columns\": [\"consumption\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"confidenceInterval\": 95}}, \"output\": \"energy_forecast\", \"comment\": \"Forecasting the energy consumption for the next 30 days using the Holt-Winters method.\"}]"}]}
{"id":"fad7802f-42f5-40fc-8a7f-8a10b9fda767","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the product specifications table from the CSV file.\", \"Filter the product specifications to include only those with a status of 'approved'.\", \"Join the approved product specifications with the supplier information table on the supplier ID.\", \"Aggregate the joined data to calculate the total cost per supplier.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"product_specifications\", \"location\": \"C:/data/product_specifications.csv\", \"sheet_name\": null, \"label\": \"Product Specifications\", \"description\": \"A table containing detailed specifications of products, including status and supplier ID.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"status\", \"column_type\": \"xsd:string\", \"column_description\": \"Approval status of the product.\"}, {\"column\": \"col_4\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier of the product.\"}, {\"column\": \"col_5\", \"column_name\": \"cost\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Cost of the product.\"}]}, \"table_id2\": {\"name\": \"supplier_information\", \"location\": \"C:/data/supplier_information.csv\", \"sheet_name\": null, \"label\": \"Supplier Information\", \"description\": \"A table containing information about suppliers, including supplier ID and name.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"col_2\", \"column_name\": \"supplier_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the supplier.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"A table containing data not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant data column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_specifications.csv\", \"type\": \"csv\"}, \"output\": \"product_specifications\", \"comment\": \"Loading the product specifications from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"product_specifications\", \"query\": \"status == 'approved'\"}, \"output\": \"approved_products\", \"comment\": \"Filtering the product specifications to include only approved products.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_information.csv\", \"type\": \"csv\"}, \"output\": \"supplier_information\", \"comment\": \"Loading the supplier information from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"approved_products\", \"supplier_information\"], \"joinOn\": [\"supplier_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining approved product specifications with supplier information on supplier ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"supplier_id\", \"aggregations\": [{\"column\": \"cost\", \"name\": \"total_cost\", \"function\": \"sum\"}]}, \"output\": \"total_cost_per_supplier\", \"comment\": \"Aggregating the joined data to calculate the total cost per supplier.\"}]"}]}
{"id":"6fa0bcdb-ec13-4dda-8002-1302093431b3","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback table from the CSV file.\", \"Filter the feedback to include only those with a rating of 4 or 5.\", \"Aggregate the filtered feedback by product category to calculate the average rating.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback Data\", \"description\": \"This table contains customer feedback including ratings and comments for various products.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"product_category\", \"column_type\": \"xsd:string\", \"column_description\": \"Category of the product being reviewed.\"}, {\"column\": \"col_3\", \"column_name\": \"rating\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Customer rating for the product, ranging from 1 to 5.\"}, {\"column\": \"col_4\", \"column_name\": \"feedback\", \"column_type\": \"xsd:string\", \"column_description\": \"Textual feedback provided by the customer.\"}]}, \"table_id2\": {\"name\": \"product_catalog\", \"location\": \"C:/data/product_catalog.xlsx\", \"sheet_name\": \"Products\", \"label\": \"Product Catalog\", \"description\": \"A catalog of products with details like product ID, name, and category.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"product_category\", \"column_type\": \"xsd:string\", \"column_description\": \"Category to which the product belongs.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading the customer feedback data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"rating >= 4\"}, \"output\": \"positive_feedback\", \"comment\": \"Filtering feedback to include only those with a rating of 4 or 5.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"positive_feedback\", \"groupBy\": \"product_category\", \"aggregations\": [{\"column\": \"rating\", \"name\": \"average_rating\", \"function\": \"mean\"}]}, \"output\": \"average_rating_by_category\", \"comment\": \"Aggregating feedback by product category to calculate the average rating.\"}]"}]}
{"id":"8b7cf17b-493c-4455-9864-8aa6d64f5da2","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'shipment_data' containing details of shipments including dates and destinations.\", \"Filter the shipment data to include only shipments destined for 'New York'.\", \"Join the filtered shipment data with the 'carrier_info' table on the 'carrier_id' column.\", \"Calculate the total weight of shipments for each carrier.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"shipment_data\", \"location\": \"C:/data/logistics/shipment_data.csv\", \"sheet_name\": null, \"label\": \"Shipment Data\", \"description\": \"This table contains details of shipments including shipment dates, destinations, weights, and carrier IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"col_2\", \"column_name\": \"destination\", \"column_type\": \"xsd:string\", \"column_description\": \"Destination city for the shipment.\"}, {\"column\": \"col_3\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the shipment in kilograms.\"}, {\"column\": \"col_4\", \"column_name\": \"carrier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the carrier handling the shipment.\"}]}, \"table_id2\": {\"name\": \"carrier_info\", \"location\": \"C:/data/logistics/carrier_info.csv\", \"sheet_name\": null, \"label\": \"Carrier Information\", \"description\": \"This table contains information about carriers including their IDs and names.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"carrier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each carrier.\"}, {\"column\": \"col_2\", \"column_name\": \"carrier_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the carrier.\"}]}, \"table_id3\": {\"name\": \"warehouse_data\", \"location\": \"C:/data/logistics/warehouse_data.csv\", \"sheet_name\": null, \"label\": \"Warehouse Data\", \"description\": \"This table contains information about warehouses including their locations and capacities.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"warehouse_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each warehouse.\"}, {\"column\": \"col_2\", \"column_name\": \"location\", \"column_type\": \"xsd:string\", \"column_description\": \"Location of the warehouse.\"}, {\"column\": \"col_3\", \"column_name\": \"capacity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Capacity of the warehouse in cubic meters.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/shipment_data.csv\", \"type\": \"csv\"}, \"output\": \"shipment_data\", \"comment\": \"Loading the shipment data containing details of shipments.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"shipment_data\", \"query\": \"destination == 'New York'\"}, \"output\": \"ny_shipments\", \"comment\": \"Filtering shipments to include only those destined for New York.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/carrier_info.csv\", \"type\": \"csv\"}, \"output\": \"carrier_info\", \"comment\": \"Loading the carrier information table.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"ny_shipments\", \"carrier_info\"], \"joinOn\": [\"carrier_id\"], \"joinType\": \"inner\"}, \"output\": \"ny_shipments_with_carriers\", \"comment\": \"Joining filtered shipment data with carrier information on carrier_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"ny_shipments_with_carriers\", \"groupBy\": \"carrier_id\", \"aggregations\": [{\"column\": \"weight\", \"name\": \"total_weight\", \"function\": \"sum\"}]}, \"output\": \"total_weight_per_carrier\", \"comment\": \"Calculating the total weight of shipments for each carrier.\"}]"}]}
{"id":"e02dffc9-d334-40d0-a879-617ec622e86b","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'supplier_data' table from the CSV file located at C:/data/supplier_data.csv.\", \"Load the 'emissions_data' table from the Excel file located at C:/data/emissions_data.xlsx.\", \"Filter the 'supplier_data' table to include only suppliers with a sustainability rating above 80.\", \"Join the filtered 'supplier_data' table with the 'emissions_data' table on the 'supplier_id' column using an inner join.\", \"Calculate a new column 'emission_per_unit' in the joined table by dividing 'total_emissions' by 'units_supplied'.\", \"Aggregate the joined table by 'region' to calculate the total emissions and average sustainability rating.\", \"Sort the aggregated data by 'total_emissions' in descending order.\", \"Apply a log transformation to the 'total_emissions' column to stabilize variance.\", \"Filter the transformed data to include only regions with a log-transformed total emissions value below 10.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Data\", \"description\": \"Contains information about suppliers including their sustainability ratings.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"column_2_id\", \"column_name\": \"sustainability_rating\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Sustainability rating of the supplier.\"}, {\"column\": \"column_3_id\", \"column_name\": \"units_supplied\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Number of units supplied by the supplier.\"}]}, \"table_id2\": {\"name\": \"emissions_data\", \"location\": \"C:/data/emissions_data.xlsx\", \"sheet_name\": null, \"label\": \"Emissions Data\", \"description\": \"Contains emissions data for each supplier.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"column_2_id\", \"column_name\": \"total_emissions\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total emissions reported by the supplier.\"}, {\"column\": \"column_3_id\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the supplier operates.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant data column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading supplier data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/emissions_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"emissions_data\", \"comment\": \"Loading emissions data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"sustainability_rating > 80\"}, \"output\": \"filtered_supplier_data\", \"comment\": \"Filtering suppliers with a sustainability rating above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_supplier_data\", \"emissions_data\"], \"joinOn\": [\"supplier_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered supplier data with emissions data on supplier_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"emission_per_unit\", \"formula\": \"total_emissions / units_supplied\"}, \"output\": \"joined_data_with_emission_per_unit\", \"comment\": \"Calculating emissions per unit supplied.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data_with_emission_per_unit\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_emissions\", \"name\": \"total_emissions\", \"function\": \"sum\"}, {\"column\": \"sustainability_rating\", \"name\": \"avg_sustainability_rating\", \"function\": \"mean\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data by region to calculate total emissions and average sustainability rating.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_emissions\", \"order\": \"desc\"}, \"output\": \"sorted_aggregated_data\", \"comment\": \"Sorting aggregated data by total emissions in descending order.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sorted_aggregated_data\", \"transformations\": [{\"columns\": [\"total_emissions\"], \"operation\": \"log_transform\"}]}, \"output\": \"transformed_data\", \"comment\": \"Applying log transformation to total emissions.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"transformed_data\", \"query\": \"total_emissions < 10\"}, \"output\": \"final_filtered_data\", \"comment\": \"Filtering regions with log-transformed total emissions below 10.\"}]"}]}
{"id":"07a75bab-1d87-45c4-a8bc-58855254b56a","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'financial_transactions' table from the CSV file located at 'C:/data/financial_transactions.csv'.\", \"Filter the 'financial_transactions' table to include only transactions where the 'amount' is greater than 1000.\", \"Calculate a new column 'net_amount' in the filtered table by subtracting 'fees' from 'amount'.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"financial_transactions\", \"location\": \"C:/data/financial_transactions.csv\", \"sheet_name\": null, \"label\": \"Financial Transactions\", \"description\": \"A table containing financial transaction data including transaction amounts, fees, and other related information.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The total amount of the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"fees\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The fees associated with the transaction.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Data\", \"description\": \"A table containing customer information such as names, addresses, and contact details.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the customer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/financial_transactions.csv\", \"type\": \"csv\"}, \"output\": \"financial_transactions\", \"comment\": \"Loading the financial transactions dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"financial_transactions\", \"query\": \"amount > 1000\"}, \"output\": \"filtered_transactions\", \"comment\": \"Filtering transactions to include only those with an amount greater than 1000.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"filtered_transactions\", \"columnName\": \"net_amount\", \"formula\": \"amount - fees\"}, \"output\": \"transactions_with_net_amount\", \"comment\": \"Calculating net amount by subtracting fees from the transaction amount.\"}]"}]}
{"id":"ab9bc9c1-00d3-4434-843c-86ee834bc5ac","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'energy_usage' which contains daily energy consumption data for the building.\", \"Filter the data to include only records where energy consumption exceeds 1000 kWh.\", \"Aggregate the filtered data to calculate the total energy consumption for each month.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_usage\", \"location\": \"C:/data/building_management/energy_usage.csv\", \"sheet_name\": null, \"label\": \"Energy Usage Data\", \"description\": \"This table contains daily energy consumption data for the building, including date and energy consumption in kWh.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"energy_consumption\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kWh.\"}]}, \"table_id2\": {\"name\": \"maintenance_schedule\", \"location\": \"C:/data/building_management/maintenance_schedule.xlsx\", \"sheet_name\": \"Schedule\", \"label\": \"Maintenance Schedule\", \"description\": \"This table contains the maintenance schedule for building equipment.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"equipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"The unique identifier for each piece of equipment.\"}, {\"column\": \"col_2\", \"column_name\": \"maintenance_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The scheduled date for maintenance.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/building_management/energy_usage.csv\", \"type\": \"csv\"}, \"output\": \"energy_usage\", \"comment\": \"Loading the energy usage data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_usage\", \"query\": \"energy_consumption > 1000\"}, \"output\": \"high_energy_usage\", \"comment\": \"Filtering the data to include only records where energy consumption exceeds 1000 kWh.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"high_energy_usage\", \"groupBy\": \"date\", \"aggregations\": [{\"column\": \"energy_consumption\", \"name\": \"monthly_total_energy\", \"function\": \"sum\"}]}, \"output\": \"monthly_energy_consumption\", \"comment\": \"Aggregating the filtered data to calculate the total energy consumption for each month.\"}]"}]}
{"id":"8f1f3eec-5e98-45f5-ae59-27ad5620a029","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'raw_materials' which contains information about the materials used in manufacturing.\", \"Load the table 'production_schedule' which contains the schedule for manufacturing processes.\", \"Filter the 'raw_materials' table to include only materials with a stock greater than 100 units.\", \"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\", \"Calculate a new column 'required_stock' in the joined table by multiplying 'daily_usage' by 'days_scheduled'.\", \"Aggregate the joined table by 'material_type' to find the total 'required_stock' for each type.\", \"Sort the aggregated data by 'total_required_stock' in descending order to prioritize materials with the highest demand.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"raw_materials\", \"location\": \"C:/data/manufacturing/raw_materials.csv\", \"sheet_name\": null, \"label\": \"Raw Materials Inventory\", \"description\": \"This table contains information about the raw materials used in the manufacturing process, including material ID, type, and stock levels.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"material_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each material.\"}, {\"column\": \"column_2_id\", \"column_name\": \"material_type\", \"column_type\": \"xsd:string\", \"column_description\": \"Type of material, such as metal, plastic, etc.\"}, {\"column\": \"column_3_id\", \"column_name\": \"stock\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Current stock level of the material.\"}]}, \"table_id2\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.xlsx\", \"sheet_name\": \"Schedule\", \"label\": \"Production Schedule\", \"description\": \"This table contains the production schedule, detailing the processes and materials required for manufacturing.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"material_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each material.\"}, {\"column\": \"col_2\", \"column_name\": \"daily_usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Daily usage of the material in the production process.\"}, {\"column\": \"col_3\", \"column_name\": \"days_scheduled\", \"column_type\": \"xsd:integer\", \"column_description\": \"Number of days the material is scheduled for use.\"}]}, \"table_id3\": {\"name\": \"maintenance_logs\", \"location\": \"C:/data/manufacturing/maintenance_logs.csv\", \"sheet_name\": null, \"label\": \"Maintenance Logs\", \"description\": \"Logs of maintenance activities performed on manufacturing equipment.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"equipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the equipment.\"}, {\"column\": \"col_B\", \"column_name\": \"maintenance_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date when maintenance was performed.\"}, {\"column\": \"col_C\", \"column_name\": \"maintenance_type\", \"column_type\": \"xsd:string\", \"column_description\": \"Type of maintenance performed.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/raw_materials.csv\", \"type\": \"csv\"}, \"output\": \"raw_materials\", \"comment\": \"Loading the raw materials inventory from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.xlsx\", \"type\": \"xlsx\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the production schedule from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"raw_materials\", \"query\": \"stock > 100\"}, \"output\": \"filtered_materials\", \"comment\": \"Filtering raw materials to include only those with stock greater than 100 units.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_materials\", \"production_schedule\"], \"joinOn\": [\"material_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered materials with production schedule on material_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"required_stock\", \"formula\": \"daily_usage * days_scheduled\"}, \"output\": \"joined_data_with_required_stock\", \"comment\": \"Calculating required stock by multiplying daily usage by days scheduled.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data_with_required_stock\", \"groupBy\": \"material_type\", \"aggregations\": [{\"column\": \"required_stock\", \"name\": \"total_required_stock\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data by material type to find total required stock.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_required_stock\", \"order\": \"desc\"}, \"output\": \"sorted_aggregated_data\", \"comment\": \"Sorting aggregated data by total required stock in descending order.\"}]"}]}
{"id":"f141c725-8946-4d92-aeb1-fecb5898cb75","messages":[{"role":"system","content":"# Tamarind Workflow Generation\n\n## Role & Objective\n\nYou are an expert data engineer specializing in Tamarind Workflow. Your task is to convert natural language instructions into a deterministic, structured Tamarind Workflow encoded in JSON.\n\n * Follow the official Tamarind Workflow schema\n * Strictly validate actions & parameters\n * Ensure explicit dependencies & correct execution order\n * Never assume missing details\u2014return an error instead\n * Each action must include a \"comment\" explaining its purpose and parameters\n\n## 1. Context\n\nYou will be provided with:\n\n * **List of Instructions**: The user\u2019s request in natural language.\n * **Metadata**: Information about available datasets, tables, and attributes.\n\nYour output must be a valid JSON object containing either:\n\n * A workflow ( `\"workflow\"` )  if all required details are present.\n * An error message ( `\"error\"` ) if valid workflow generation is not possible.\n\n## 2. Expected Output Format\n\nA valid Tamarind Workflow consists of a JSON array where each action follows this schema:\n\n```json\n{\n    \"workflow\": [\n        { \n            \"action\": \"action_name\",\n            \"parameters\": { ... },\n            \"output\": \"output_table\",\n            \"comment\": \"Detailed explanation of why this step was generated and why these parameters were chosen.\"\n        },\n        { \n            \"action\": \"another_action\",\n            \"parameters\": { ... },\n            \"output\": \"another_output_table\",\n            \"comment\": \"Explanation for this action.\"\n        }\n    ]\n}\n```\n\nIf workflow generation is not possible, return:\n\n```json\n{\n    \"workflow\": [],\n    \"error\":  \"Detailed explanation of why the workflow couldn't be generated.\"\n}\n```\n\n**Important:**\n\n * The \"workflow\" key must always contain a list (empty if an error occurs).\n * The \"error\" key must be structured and provide both a summary (\"message\") and details (\"details\").\n * No assumptions\u2014if the user input lacks critical details, return an error instead of an incorrect workflow.\n\n## 3. Tamarind Workflow Generation Rules\n\n### 3.1. Deterministic Mapping\n\n * Each step in the input must map explicitly to one or more Tamarind Workflow actions.\n * The workflow must not include extra, inferred, or assumed actions.\n\n### 3.2. Strict Action Definitions\n\n * Only use predefined actions in the Tamarind Workflow specification.\n * Do not introduce new actions or modify existing ones.\n\n### 3.3. Parameter Accuracy\n\n * Ensure all required parameters are provided for each action.\n * Validate enumerated values (e.g., predefined options) before including them.\n * Reject incomplete or incorrect parameters and return an error.\n\n### 3.4. Expression Correctness\n\n * When referencing columns or table names, enclose names in **backticks (\\ `) if they contain special characters** (e.g., -, ., (, )` , etc.).\n * Ensure expressions conform to the Tamarind Workflow syntax.\n\n### 3.5. Data Dependencies\n\n * Tables must be created before being referenced.\n * Each step should logically follow the previous and respect table dependencies.\n\n### 3.6. Incremental Fixes\n\n * If a validation error occurs within a single step, only adjust that step.\n * Do not regenerate the entire workflow\u2014preserve correctly generated actions.\n\n### 3.7. Error Reporting\n\n * If a step cannot be mapped to a valid action, return a structured error.\n * Example error format: \n\n```json\n    {\n\n        \"workflow\": [],\n        \"error\": \"The instruction requested filtering, but no table was provided. Ensure the input specifies a valid table.\"\n    }\n```\n\n### 3.8. Tamarind Workflow Action Schema\n\n * Each action must strictly adhere to the Tamarind Workflow Specification, which is provided below.\n * Each action must include the following fields:\n  + `\"action\"` (*string*): The name of the action.\n  + `\"parameters\"` (*object*): A dictionary of required parameters.\n  + `\"output\"` (*string*): Name of the resulting table.\n  + `\"comment\"` (*string*): A detailed explanation of why this step was generated and why these parameters were used.\n\n * All fields are mandatory\u2014if any required field is missing or incorrect, the workflow generation must return an error instead of producing an invalid workflow.\n\n#### 3.8.1 Strict Adherence to the Full Specification\n\n * The full Tamarind Workflow Specification is provided below, listing all valid actions along with their permitted parameters and value types.\n * Only the defined actions in the specification may be used\u2014custom or inferred actions are not allowed.\n * For actions involving column expressions, the specification explicitly defines:\n   * Valid operations that can be performed on columns.\n   * Allowed data transformations and how they should be structured.\n   * Correct syntax and formatting (e.g., enclosing column names with special characters in backticks `column_name` ).\n\n#### 3.8.2 Enforcement of the Specification\n\n * The generated workflow must strictly adhere to the specification.\n * If a request requires an action or parameter not defined in the specification, return an error instead of approximating a solution.\n * If an expression needs to be applied to a column, ensure it follows the exact format and constraints defined in the specification.\n * Reject any missing or incorrect parameters rather than making assumptions.\n * Any violation of these rules must trigger a structured error response instead of generating an invalid workflow.\n\n### 3.9. JSON Comments Policy\n\n * The generated JSON must not contain JavaScript-style comments (// or /* */).\n * Any necessary explanations must go inside the \"comment\" field of each action.\n\n### 3.10. No Hallucination Rule\n\n * Never generate an action unless explicitly required or inferable from metadata.\n * If an instruction is ambiguous or lacks key details, return an error instead of making assumptions.\n\n### 3.11. Strict Parameter Validation\n\n * Check all parameters against the official schema.\n * If an invalid parameter is requested, return an error instead of auto-correcting.\n\n### 3.12. Explicit Dependencies\n\n * Actions must be sequenced correctly\u2014an output table must exist before it is referenced in subsequent steps.\n * Ensure that no table is referenced before its creation.\n\n### 3.13. Reject Undefined Actions\n\n * If a requested action does not exist in the Tamarind Workflow specification, return an error instead of attempting to approximate it.\n\n## 4. Example Workflow\n\nInput Instruction: `\"Load a CSV file from /data/source.csv and filter rows where engagement_score > 50.\"`\n\nGenerated Workflow Output:\n\n```json\n{\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"location\": \"/data/source.csv\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_1\",\n            \"comment\": \"Loading data from the specified CSV file. This is the first step to ensure the table is available for processing.\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"table_1\",\n                \"query\": \"engagement_score > 50\"\n            },\n            \"output\": \"filtered_table\",\n            \"comment\": \"Filtering records where engagement_score is greater than 50 to retain only high-engagement rows.\"\n        }\n    ]\n}\n```\n\n## 5. Example Error Case\n\nInput Instruction: `\"Filter records where engagement_score > 50 without specifying the table.\"`\n\nGenerated Error Response:\n\n```json\n{\n    \"workflow\": [],\n    \"error\": \"Missing required parameter: 'table' in action 'filterData'.\"\n}\n```\n\n\n# Tamarind Workflow Language Specification\n\n## 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n## 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n * A **name** identifying the operation to be performed.\n * A **list of parameters**, including input tables and transformation options.\n * An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n## 3. Actions and Their Definitions\n\n### 3.1 `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n * `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n * `type` (*enum: `csv` , `xlsx` *): Format of the table.\n\n**Behavior**\n\n * If the table does not exist, an error is raised.\n * If the format is unsupported, an error is raised.\n * If the table is already loaded, it is overwritten unless explicitly prevented.\n * The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n * Identifying a Table\n\n```json\n{{ \"name\": \"identifyTable\",\n  \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n    \"type\": \"csv\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n * Unsupported Data Type\n\n```json\n{{\n  \"name\": \"identifyTable\",\n  \"parameters\": {{\n    \"location\": \"https://example.com/data.xml\",\n    \"type\": \"xml\",\n    \"table\": \"sales_data\"\n  }},\n  \"output\": \"sales_data\"\n}}\n```\n\n**Expected Error:** Unsupported file type.\n\n### 3.2 `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table to filter.\n * `query` (*string*): A query expression applied to the columns and resulting in a boolean value to filter rows. The expressions language is specified below\n * `caseInsensitiveColumns` (*list[string]*, default= `[]` ): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n * Columns referenced in the query must exist; otherwise, an error is raised.\n * If the table does not exist, an error is raised.\n * If the query is invalid, an error is raised.\n\n**Example**\n\n * Filtering Data\n\n```json\n{{\n  \"name\": \"filterData\",\n  \"parameters\": {{\n    \"table\": \"sales_data\",\n    \"query\": \"revenue > 1000\"\n  }},\n  \"output\": \"filtered_sales\"\n}}\n```\n\n### 3.3 `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n * `tables` (*list[string]*): Names of tables to join.\n * `joinOn` (*list[string]*): Columns used for joining.\n * `joinType` (*enum: `inner` , `outer` , `left` , `right` *): Join type.\n\n**Behavior:**\n\n * If join columns do not exist in both tables, an error is raised.\n * Duplicate column names are automatically renamed, the prefix is the table name followed by `_` .\n * Null values are permitted in join keys.\n\n### 3.4 `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n * `table` (*string*): The name of the input table to be aggregated.\n * `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n * `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n   + `column` : (*string*) The name of the column on which the aggregation function will be applied.\n   + `name` : (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set, `name` is recommended to avoid column name conflicts.\n   + `function` (*enum: `sum` , `mean` , `median` , `min` , `max` , `count` , `std` , `var` *): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n * If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n * Non-numeric columns in aggregation are ignored.\n * Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n### 3.5 `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `sortBy` (*list[string] | string*): Columns to sort by.\n * `order` (*list[string] | *string, enum: `asc` , `desc` *): Sorting direction.\n\n**Behavior:**\n\n * If `sortBy` columns do not exist, sorting is skipped.\n * Null values are sorted based on the underlying sorting algorithm.\n\n### 3.6 `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `columnName` (*string*): Name of the new column.\n * `formula` (*string*): An expression using columns and functions that, when evaluated, produces a valid value assigned to the new column.\n\n**Behavior:**\n\n * Missing columns in the formula are treated as `null` .\n * Division by zero results in `null` .\n * Only a safe subset of operations is allowed.\n * If a formula is invalid, an error is raised.\n * Safe functions: `abs` , `round` , `min` , `max` , `sqrt` , `log` , `exp` , `sin` , `cos` , `tan` , `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff`\n * Safe operators: `+` , `-` , `*` , `/` , `**` , `%` , `(` , `)`\n\n### 3.7 `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed general transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n### 3.8 `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n * `table` (*string*): Name of the table.\n * `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n   + `columns` (*list[string]*): Column names involved in the operation.\n   + `operation` (*enum*): One of the allowed time series transformations.\n   + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n * The transformations are applied sequentially in the order they appear in the `transformations` list.\n * If an operation requires a column that does not exist, an **error is raised**.\n * If a transformation fails (e.g., invalid parameters), an **error is raised**.\n * No filtering operations are allowed.\n * The transformed table is stored with the name specified in `output` .\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description**                                |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n### 3.9 `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n * **`table`** (*string*): Name of the table containing the time series data.\n * **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n * **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n   + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n   + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n   + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n   + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n   + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n   + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n   + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n   + **`confidenceInterval`** (*float*, optional, default= `95` ): Confidence interval percentage for the forecast (must be between 0 and 100).\n * **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n * **\ud83d\udd39 Date Column Handling**\n   + If `dateColumn` **is not specified**, the system:\n\n     - **Infers it** if exactly **one datetime column** exists.\n     - **Raises an error** if multiple or no datetime columns are found.\n\n   + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n   + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n * **\ud83d\udd39 Target Column Handling**\n   + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n   + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n   + If interpolation **fails**, an **error is raised**.\n   + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n * **\ud83d\udd39 Forecast Period Validation**\n   + `forecastPeriod` **must be an integer \u2265 1**.\n   + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n * **\ud83d\udd39 Time Series Continuity**\n   + If **gaps exist**, they are **forward-filled**.\n   + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n * **\ud83d\udd39 Forecast Output**\n   + The `output` table contains:\n\n     - **For forecasting models ( `holt_winters` , `xgboost` )**:\n       - A ** `dateColumn` with future timestamps**.\n       - A ** `forecast` column** with predicted values.\n       - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n     - **For anomaly detection models ( `isolation_forest` , `local_outlier_factor` , `one_class_svm` , `gmm` , `z-score` )**:\n       - **Does not generate future timestamps**.\n       - Contains an ** `outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  }},\n  \"output\": \"forecast_results\"\n}}\n```\n\n```json\n{{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {{\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  }},\n  \"output\": \"energy_forecast\"\n}}\n```\n\n## 4. Specification for Valid Expressions\n\nThe following language specifies how expressions must be constructed for filtering rows, aggregating values or determining values for new columns\n\n### 4.1. Syntax and Operations\n\nThe following syntax rules and operations are supported:\n\n#### 4.1.1 Arithmetic Operations\n\n * Basic arithmetic: `+` , `-` , `*` , `/` , `**` , `%`\n\n * Parentheses for precedence: `(a + b) * c`\n\n * Negative values: `-a`\n\n * Floating-point and integer operations: `df[\"col1\"] + 2.5, df[\"col2\"] / 3`\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2 - 5\ncol1 * (col2 + col3) / col4\n```\n\n#### 4.1.2 Comparison Operations\n\n * Basic comparisons: `==` , `!=` , `<` , `>` , `<=` , `>=`\n\n * Chained comparisons: `a < b < c`\n\n * Element-wise operations on Table columns\n\n**Example Valid Expressions:**\n\n```python\ncol1 > col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n#### 4.1.3 Boolean Operations\n\n * Logical operations: `and` , `or` , `not`\n\n * Element-wise logical operations on Table columns\n \n**Example Valid Expressions:**\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n```\n\nFor eval():\n\n```python\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n#### 4.1.4 List and Tuple Literals\n\n * Lists and tuples can be used in expressions\n * Example: `df[\"col1\"].isin([1, 2, 3])`\n\n**Example Valid Expressions:**\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n#### 4.1.5 Attribute Access\n\n * Column access via dot notation: `df.a`\n\n * Only works if column names are valid Python identifiers\n\n**Example Valid Expressions:**\n\n```python\na > 10\na + b\n```\n\n#### 4.1.6 Subscript Expressions\n\n * Accessing a column via dictionary-style indexing\n\n**Example Valid Expressions:**\n\n```python\ndf['col1'] + df['col2']\n```\n\n#### 4.1.7 Math Functions\n\n * Supported functions: \n   + Trigonometric functions: `sin` , `cos` , `tan` , `arcsin` , `arccos` , `arctan` , `arctan2`\n\n   + Hyperbolic functions: `sinh` , `cosh` , `tanh` , `arcsinh` , `arccosh` , `arctanh`\n\n   + Exponential and logarithmic functions: `exp` , `expm1` , `log` , `log10` , `log1p`\n\n   + Power and root functions: `sqrt` , \n   + Statistical functions: `sum` , `mean` , `std` , `var` , `median` , `cumsum` , `cumprod` , `diff` , \n   + General functions: `abs` , `round` , `min` , `max`\n\n * Applied element-wise to Table columns\n\n**Example Valid Expressions:**\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\n```\n\n#### 4.1.8 Column Naming Rules\n\n* If a column name is a valid Python identifier, it can be used as is:\n* If a column name contains spaces, special characters, or starts with a number, it must be enclosed in backticks (`):\n\n**Example Valid Expressions:**\n\n```python\ncol1 + col2\n`a column name` + col2\n`123_invalid` - `another column`\n```\n\n**Example Inalid Expressions:**\n\n```python\ncolumn name + col2     # Invalid: missing back tick\n\"a column name\" + col2 # Invalid: double quotes instead of backticks\n'a column name' + col2 # Invalid: single quotes instead of backticks\n```\n\n### 4.2. Disallowed Syntax\n\nThe following constructs are not allowed in `eval()` and `query()` :\n\n#### 4.2.1 Function Calls (Other than Math Functions)\n\n```python\n# Invalid\nlen(col1)\nstr(col1)\n```\n\n#### 4.2.2 `is` and `is not` Operators\n\n```python\n# Invalid\ncol1 is None\ncol1 is not col2\n```\n\n#### 4.2.3 If Expressions\n\n```python\n# Invalid\ncol1 if col2 > 10 else col3\n```\n\n#### 4.2.4 Lambda Expressions\n\n```python\n# Invalid\nlambda x: x + 1\n```\n\n#### 4.2.5 Comprehensions\n\n```python\n# Invalid\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n#### 4.2.6 Dictionary and Set Literals\n\n```python\n# Invalid\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n#### 4.2.7 Yield Expressions & Generators\n\n```python\n# Invalid\n(x for x in col1)\n```\n\n#### 4.2.8 Boolean Expressions with Scalars\n\n```python\n# Invalid\nTrue and False\n```\n\n#### 4.2.9 Statements (Loops, Conditionals)\n\n```python\n# Invalid\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n### 4.3. Special Cases and Restrictions\n\n#### 4.3.1 Multiline Expressions\n\nAllowed only if all lines have item assignments.\n\n```python\n# Invalid\na = b + 1\na + 2  # Raises ValueError\n```\n\n#### 4.3.2 Inplace Operations\n\n`inplace=True` requires item assignment.\n\n```python\n# Invalid\ncol1 + col2, inplace=True  # Raises ValueError\n```\n\n#### 4.3.3 Target Not Supporting `.copy()`\n\nSome targets may not allow assignment when `inplace=False` .\n\n### 4.4. Examples \n\n#### 4.4.1 Valid Expressions\n\n##### 4.4.1.1 Arithmetic Operations\n\n```python\ncol1 + col2\ncol1 - col2 * 3\n(col1 + col2) / col3\ncol1 ** 2 + col2 ** 0.5\ncol1 % 5\ncol1 * (col2 + col3) / col4\n- col1 + col2\n```\n\n##### 4.4.1.2 Comparison Operations\n\n```python\ncol1 > col2\ncol1 < col2\ncol1 == col2\ncol1 != col2\ncol1 >= col2\ncol1 <= col2\n10 < col1 <= 50\ncol1 != col2 and col3 > 100\n```\n\n##### 4.4.1.3 Valid Boolean Operations\n\n```python\ncol1 > 10 and col2 < 5\ncol1 == 20 or col2 == 30\nnot col1 < 5\n(col1 > 10) & (col2 < 5)\n(col1 == 20) | (col2 == 30)\n~(col1 < 5)\n```\n\n##### 4.4.1.4 List and Tuple Literals\n\n```python\ncol1 in [1, 2, 3]\ncol1 not in (4, 5, 6)\n```\n\n##### 4.4.1.5 Attribute Access\n\n```python\na > 10\na + b\n```\n\n##### 4.4.1.6 Subscript Expressions\n\n```python\ncol1[0] + col2[1]\n```\n\n##### 4.4.1.7 Math Functions\n\n```python\nsin(col1) + cos(col2)\nlog(col1) + sqrt(col2)\nabs(col3) > 10\nexp(col1) - log10(col2)\ntanh(col1) * sinh(col2)\narcsin(col1) + arccos(col2)\narctan2(col1, col2)\n```\n\n#### 4.4.2 Invalid Expressions\n\n##### 4.4.2.1 Function Calls (Other than Math Functions)\n\n```python\nlen(col1)\nstr(col1)\n```\n\n##### 4.4.2.2 `is` and `is not` Operators\n\n```python\ncol1 is None\ncol1 is not col2\n```\n\n##### 4.4.2.3 If Expressions\n\n```python\ncol1 if col2 > 10 else col3\n```\n\n##### 4.4.2.4 Lambda Expressions\n\n```python\nlambda x: x + 1\n```\n\n##### 4.4.2.5 Comprehensions\n\n```python\n[x for x in col1]\n{x: x**2 for x in col1}\n```\n\n##### 4.4.2.6 Dictionary and Set Literals\n\n```python\n{1, 2, 3}\n{'a': 1, 'b': 2}\n```\n\n##### 4.4.2.7 Yield Expressions & Generators\n\n```python\n(x for x in col1)\n```\n\n##### 4.4.2.8 Boolean Expressions with Scalars\n\n```python\nTrue and False\n```\n\n##### 4.4.2.9 Statements (Loops, Conditionals)\n\n```python\nfor i in range(10): i + 1\nwhile col1 < 10: col1 + 1\nif col1 > 10: col1 + 1\n```\n\n##### 4.4.2.10 Multiline Expressions Without Assignment\n\n```python\na = b + 1\na + 2  # Invalid because the last line lacks assignment\n```\n\n##### 4.4.2.11 Inplace Operations Without Assignment\n\n```python\ncol1 + col2  # Invalid if used with inplace=True\n```\n\n##### 4.4.2.12 Incorrect Column Naming\n\n```python\n\"a column name\" + col2   # Invalid: double quotes instead of backticks\n'a column name' + col2   # Invalid: single quotes instead of backticks\na column name + col2     # Invalid: missing backticks around column name\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located in the data directory.\", \"Filter the sales data to include only transactions with a total revenue greater than 5000.\", \"Join the filtered sales data with the customer information table on the customer ID.\", \"Aggregate the joined data to calculate the total sales revenue per region.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains sales transactions including customer IDs, product IDs, and total revenue.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer involved in the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"total_revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total revenue generated from the transaction.\"}]}, \"table_id2\": {\"name\": \"customer_info\", \"location\": \"C:/data/customer_info.csv\", \"sheet_name\": null, \"label\": \"Customer Information\", \"description\": \"This table contains information about customers including their ID and region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the customer is located.\"}]}, \"table_id3\": {\"name\": \"product_catalog\", \"location\": \"C:/data/product_catalog.csv\", \"sheet_name\": null, \"label\": \"Product Catalog\", \"description\": \"This table contains information about products including product IDs and descriptions.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"product_description\", \"column_type\": \"xsd:string\", \"column_description\": \"Description of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"`total_revenue` > 5000\"}, \"output\": \"filtered_sales\", \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 5000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_info.csv\", \"type\": \"csv\"}, \"output\": \"customer_info\", \"comment\": \"Loading the customer information from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales\", \"customer_info\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"sales_with_customers\", \"comment\": \"Joining filtered sales data with customer information based on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_with_customers\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_revenue\", \"name\": \"total_sales\", \"function\": \"sum\"}]}, \"output\": \"regional_sales\", \"comment\": \"Aggregating sales data by region to compute total revenue.\"}]"}]}
{"id":"ab37133c5c80016271b5a73f7c069a1f4355329aa7c63504bd17b3432f720197","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"What parameters are required for the 'applyTransformations' action?"},{"role":"assistant","content":"'applyTransformations' requires a 'table' (name of the table) and 'transformations' (a list of transformations, each specifying 'columns', 'operation', and optional 'parameters')."}]}
{"id":"313f2f2317a3264e480a627765c90848dbff71350d4a025fb39c4b115c64e928","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"What forecast frequencies are supported in 'forecastData'?"},{"role":"assistant","content":"Supported forecast frequencies include 'B' (business day), 'C' (custom business day), 'D' (calendar day), 'W' (weekly), 'ME' (month end), 'BME' (business month end), 'MS' (month start), 'BMS' (business month start), 'QE' (quarter end), 'QS' (quarter start), 'YE' (year end), 'YS' (year start), 'h' (hourly), 'bh' (business hour), 'min' (minutely), 's' (secondly), and more."}]}
{"id":"739035eff4bd0d5dd8f894da11b33744c02e9f33d288830e95d53cea6f9cbd12","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"When should the 'log_transform' operation be used?"},{"role":"assistant","content":"Use 'log_transform' when you need to stabilize variance and reduce skewness in data. This is often used in financial and economic datasets where values span multiple orders of magnitude."}]}
{"id":"6d8bfa352c7d0a0e04262872a254252de45e519355ad8536d1411e8b874491bd","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"What is the purpose of the 'applyTimeSeriesTransformations' action?"},{"role":"assistant","content":"The 'applyTimeSeriesTransformations' action applies a sequence of transformations to a time series dataset, such as resampling, rolling calculations, lagging, and trend decomposition."}]}
{"id":"7fa7e89d7e0e346e1f7d88e9e223c469e5c0d5913a6b5e110f6d570c55b47ba9","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of an invalid 'forecastData' action with an incorrect 'nu' value for 'one_class_svm'."},{"role":"assistant","content":"{ \"name\": \"forecastData\", \"parameters\": { \"table\": \"sensor_readings\", \"columns\": [\"temperature\"], \"forecastParameters\": { \"algorithm\": \"one_class_svm\", \"nu\": 1.5 }, \"output\": \"anomaly_detection\" } } // Error: 'nu' must be between 0 and 1."}]}
{"id":"7693bb29a470783f1dfa6c421d4e485430b4ee6f7e7fadf71ee1d27231d1e047","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of an invalid 'forecastData' action with a missing required parameter."},{"role":"assistant","content":"{ \"name\": \"forecastData\", \"parameters\": { \"table\": \"sales_data\", \"forecastParameters\": { \"forecastPeriod\": 30, \"frequency\": \"D\", \"algorithm\": \"xgboost\" }, \"output\": \"forecast_results\" } } // Error: 'columns' parameter is missing."}]}
{"id":"77d102cc1f8d750cbd729b3db5452c7c4c33132139d36bc9fbcbbc39592f1762","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"What is the purpose of the 'filterData' action?"},{"role":"assistant","content":"The 'filterData' action filters a table based on a query condition, keeping only the rows that match the condition."}]}
{"id":"8f24dff4b03e52a2ae96680fef5b551ff65ca926d53d99a4c2b1896891307936","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"When should the 'convert_timezone' operation be used?"},{"role":"assistant","content":"Use 'convert_timezone' when you need to change the time zone of a datetime column, such as converting UTC timestamps to local time."}]}
{"id":"2bfb7501e9d12309923c09100daee8d90e3b0b74eb12a49b1c567c1b3ce91242","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"What is the purpose of the 'sortData' action?"},{"role":"assistant","content":"The 'sortData' action sorts a table by one or more specified columns in either ascending or descending order."}]}
{"id":"356974af5e2efdc4d99895dfa5ee86f162fbe217087d281c717b68f9bdd372f5","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"What parameters are required for the 'sortData' action?"},{"role":"assistant","content":"'sortData' requires a 'table' (name of the table to sort), 'sortBy' (columns to sort by), and 'order' (sorting direction, either 'asc' or 'desc')."}]}
{"id":"e16cda151bb2341fdb78f7387471832bee5e6061203c1a629558e6d5ab8687be","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of an invalid 'aggregateData' action with a missing required parameter."},{"role":"assistant","content":"{ \"name\": \"aggregateData\", \"parameters\": { \"table\": \"sales\", \"aggregations\": [ { \"column\": \"revenue\", \"name\": \"total_revenue\", \"function\": \"sum\" } ] }, \"output\": \"sales_summary\" } // Error: 'groupBy' parameter is missing."}]}
{"id":"42f6d28d51b05b9cf744aa8486839593135b7426e7a116e9517a22169a378fd8","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of an invalid subscript expression."},{"role":"assistant","content":"'df[col1] + df[col2]' // Error: Column names must be enclosed in quotes."}]}
{"id":"d2cbb9f173e323e17cc744a585f2f6df61e481661e3c1168b6b88d2780e8cb2d","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"What happens if the table specified in 'identifyTable' does not exist?"},{"role":"assistant","content":"If the table does not exist, an error is raised."}]}
{"id":"6a1fff7a4e6b0682f761f3f706cc8fc7f4cfc31aca00be021f84c65f89e11826","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"How can columns be accessed using attribute notation?"},{"role":"assistant","content":"Columns can be accessed using dot notation if they are valid Python identifiers, such as 'df.a'."}]}
{"id":"5be73e6004175306b1434e9c6944b69a2732df4311d38386c20dc96accaf8ed8","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of an invalid list comprehension."},{"role":"assistant","content":"'[x for x in col1]' // Error: List comprehensions are not allowed in Tamarind."}]}
{"id":"67f47440ffe16378cc0f85689d2ba26345de1b32a8d22eb3a97ec9c8b7aeac40","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of a valid arithmetic expression using basic operations."},{"role":"assistant","content":"'col1 + col2 - 5'"}]}
{"id":"c1b679a8d126a1d6c8acb8c86abc3f53803d26d372d89e8eb0ae27b55ab8c114","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of a valid subscript expression using indexing."},{"role":"assistant","content":"'col1[0] + col2[1]'"}]}
{"id":"64903f38fdffde4ab21355a997bb1065977ccceb825e4d422c7829a9cbb5890d","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of a valid math function using trigonometry."},{"role":"assistant","content":"'sin(col1) + cos(col2)'"}]}
{"id":"5106c45cae4099ab8b62b9953ea21c7a2d84ea8e25c66c73219628d88b3367f8","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of a valid attribute access expression."},{"role":"assistant","content":"'a > 10'"}]}
{"id":"27561cf93e212a9b51a0c7875b1f095ebbd04fe32eb7c4d94cdbe52f7ea33a7d","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of a valid comparison expression using greater than or equal to."},{"role":"assistant","content":"'col1 >= col2'"}]}
{"id":"10c6a11bde362852ba0d4099d5cecf20b4137b5c8be3b242c170d68907c5a238","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"What is the 'z-score' algorithm used for?"},{"role":"assistant","content":"Use 'z-score' for anomaly detection by identifying outliers using a standard deviation threshold."}]}
{"id":"aa0f41bc3c9aed61f832811b28c9c2a61ec60958225e425d304669d4dd6d7177","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of an invalid dictionary comprehension."},{"role":"assistant","content":"'{x: x**2 for x in col1}' // Error: Dictionary comprehensions are not allowed in Tamarind."}]}
{"id":"7c074e802b3440a0605224404a033717d064068f311d0c18bfefeeefaf7d173f","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of a valid arithmetic expression using subtraction and multiplication."},{"role":"assistant","content":"'col1 - col2 * 3'"}]}
{"id":"636b9a3cf652d5dbd9c126bdf55506f5693dc87f170bda538d6d8f7c4e23353c","messages":[{"role":"system","content":"You are an expert assistant trained to understand, validate, and generate workflows in the Tamarind Workflow Language (TWL)."},{"role":"user","content":"Provide an example of a valid 'calculateDerivedColumn' action using a safe function."},{"role":"assistant","content":"{ \"name\": \"calculateDerivedColumn\", \"parameters\": { \"table\": \"sales\", \"columnName\": \"log_revenue\", \"formula\": \"log(revenue)\" }, \"output\": \"sales_with_log\" }"}]}
